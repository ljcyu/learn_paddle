{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c517a6b-4556-4521-b0dd-27915561940b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-09T08:12:20.817177Z",
     "iopub.status.busy": "2022-11-09T08:12:20.816525Z",
     "iopub.status.idle": "2022-11-09T08:12:20.827222Z",
     "shell.execute_reply": "2022-11-09T08:12:20.825953Z",
     "shell.execute_reply.started": "2022-11-09T08:12:20.817135Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "input=paddle.randn((3,10))\n",
    "labels=paddle.to_tensor([1,0,1])\n",
    "net=nn.Linear(10,2)\n",
    "out=net(input)\n",
    "loss=F.cross_entropy(out,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "160f1f33-fda0-410f-a9ac-65ffa163b078",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-09T08:12:42.690344Z",
     "iopub.status.busy": "2022-11-09T08:12:42.689170Z",
     "iopub.status.idle": "2022-11-09T08:12:42.702847Z",
     "shell.execute_reply": "2022-11-09T08:12:42.702099Z",
     "shell.execute_reply.started": "2022-11-09T08:12:42.690287Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      " Layer (type)       Input Shape          Output Shape         Param #    \n",
      "===========================================================================\n",
      "   Linear-6          [[1, 10]]              [1, 2]              22       \n",
      "===========================================================================\n",
      "Total params: 22\n",
      "Trainable params: 22\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "---------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_params': 22, 'trainable_params': 22}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=paddle.Model(net)\n",
    "model.summary((-1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d5f7083-c339-4d9d-a82c-b7bcaf6e5231",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-09T08:17:04.223279Z",
     "iopub.status.busy": "2022-11-09T08:17:04.222575Z",
     "iopub.status.idle": "2022-11-09T08:17:04.475846Z",
     "shell.execute_reply": "2022-11-09T08:17:04.474971Z",
     "shell.execute_reply.started": "2022-11-09T08:17:04.223231Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: CosineAnnealingWithWarmupDecay set learning rate to 0.001.\n",
      "avg_loss:0.2362695187330246 acc:0.0\n",
      "Epoch 2: CosineAnnealingWithWarmupDecay set learning rate to 0.002.\n",
      "avg_loss:0.23253676295280457 acc:1.0\n",
      "Epoch 3: CosineAnnealingWithWarmupDecay set learning rate to 0.003.\n",
      "avg_loss:0.225230872631073 acc:1.0\n",
      "Epoch 4: CosineAnnealingWithWarmupDecay set learning rate to 0.004.\n",
      "avg_loss:0.21466848254203796 acc:1.0\n",
      "Epoch 5: CosineAnnealingWithWarmupDecay set learning rate to 0.005.\n",
      "avg_loss:0.20131079852581024 acc:0.0\n",
      "Epoch 6: CosineAnnealingWithWarmupDecay set learning rate to 0.004946461621797824.\n",
      "avg_loss:0.18573983013629913 acc:1.0\n",
      "Epoch 7: CosineAnnealingWithWarmupDecay set learning rate to 0.004788186371224372.\n",
      "avg_loss:0.17151932418346405 acc:0.0\n",
      "Epoch 8: CosineAnnealingWithWarmupDecay set learning rate to 0.004532091636218621.\n",
      "avg_loss:0.15882524847984314 acc:1.0\n",
      "Epoch 9: CosineAnnealingWithWarmupDecay set learning rate to 0.0041893699855792025.\n",
      "avg_loss:0.147733673453331 acc:1.0\n",
      "Epoch 10: CosineAnnealingWithWarmupDecay set learning rate to 0.0037749999999999997.\n",
      "avg_loss:0.13824090361595154 acc:1.0\n",
      "Epoch 11: CosineAnnealingWithWarmupDecay set learning rate to 0.003307091636218621.\n",
      "avg_loss:0.13028396666049957 acc:0.0\n",
      "Epoch 12: CosineAnnealingWithWarmupDecay set learning rate to 0.002806094735005751.\n",
      "avg_loss:0.12376046180725098 acc:0.0\n",
      "Epoch 13: CosineAnnealingWithWarmupDecay set learning rate to 0.0022939052649942494.\n",
      "avg_loss:0.11854325234889984 acc:0.0\n",
      "Epoch 14: CosineAnnealingWithWarmupDecay set learning rate to 0.0017929083637813791.\n",
      "avg_loss:0.11449232697486877 acc:1.0\n",
      "Epoch 15: CosineAnnealingWithWarmupDecay set learning rate to 0.0013250000000000007.\n",
      "avg_loss:0.11146079003810883 acc:1.0\n",
      "Epoch 16: CosineAnnealingWithWarmupDecay set learning rate to 0.0009106300144207982.\n",
      "avg_loss:0.10929886251688004 acc:0.0\n",
      "Epoch 17: CosineAnnealingWithWarmupDecay set learning rate to 0.000567908363781379.\n",
      "avg_loss:0.10785447806119919 acc:1.0\n",
      "Epoch 18: CosineAnnealingWithWarmupDecay set learning rate to 0.0003118136287756276.\n",
      "avg_loss:0.10697298496961594 acc:0.0\n",
      "Epoch 19: CosineAnnealingWithWarmupDecay set learning rate to 0.00015353837820217606.\n",
      "avg_loss:0.10649685561656952 acc:1.0\n",
      "Epoch 20: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.10626513510942459 acc:1.0\n",
      "Epoch 21: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.10611569881439209 acc:0.0\n",
      "Epoch 22: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.1059672012925148 acc:1.0\n",
      "Epoch 23: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.10581986606121063 acc:1.0\n",
      "Epoch 24: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.10567326098680496 acc:1.0\n",
      "Epoch 25: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.10552727431058884 acc:0.0\n",
      "Epoch 26: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.10538201779127121 acc:0.0\n",
      "Epoch 27: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.10523717105388641 acc:0.0\n",
      "Epoch 28: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.10509273409843445 acc:1.0\n",
      "Epoch 29: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.10494859516620636 acc:1.0\n",
      "Epoch 30: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.1048048660159111 acc:1.0\n",
      "Epoch 31: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.10466133803129196 acc:1.0\n",
      "Epoch 32: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.10451788455247879 acc:1.0\n",
      "Epoch 33: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.10437474399805069 acc:0.0\n",
      "Epoch 34: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.1042315736413002 acc:1.0\n",
      "Epoch 35: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.10408860445022583 acc:0.0\n",
      "Epoch 36: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.10394561290740967 acc:1.0\n",
      "Epoch 37: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.10380270332098007 acc:1.0\n",
      "Epoch 38: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.10365977883338928 acc:1.0\n",
      "Epoch 39: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.10351693630218506 acc:1.0\n",
      "Epoch 40: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.10337407886981964 acc:1.0\n",
      "Epoch 41: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.10323119908571243 acc:1.0\n",
      "Epoch 42: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.10308819264173508 acc:1.0\n",
      "Epoch 43: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.10294537991285324 acc:1.0\n",
      "Epoch 44: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.1028023287653923 acc:1.0\n",
      "Epoch 45: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.10265947133302689 acc:1.0\n",
      "Epoch 46: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.10251638293266296 acc:1.0\n",
      "Epoch 47: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.1023733839392662 acc:1.0\n",
      "Epoch 48: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.10223036259412766 acc:0.0\n",
      "Epoch 49: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.10208720713853836 acc:1.0\n",
      "Epoch 50: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n",
      "avg_loss:0.10194414108991623 acc:1.0\n",
      "Epoch 51: CosineAnnealingWithWarmupDecay set learning rate to 0.0001.\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import *\n",
    "#model.train()\n",
    "epochs =50   \n",
    "\n",
    "scheduler=CosineAnnealingWithWarmupDecay(0.005,0.0001,5,20,verbose=True)\n",
    "optim = paddle.optimizer.Adam(learning_rate=scheduler, parameters=model.parameters())\n",
    "# 模型训练的两层循环\n",
    "for epoch in range(epochs):\n",
    "    for batch_id in range(3):\n",
    "        x_data = input[batch_id]\n",
    "        y_data = labels[batch_id]\n",
    "        # print(\"x_data: \", x_data[0][0][0][0]) # 打印神经网络的输入：批数据中的第一个数据的第一个元素\n",
    "        predicts = net(x_data)\n",
    "        #print(\"predicts: \", predicts[0].numpy()) # 打印神经网络的输出：批数据中的第一个数据的第一个元素           \n",
    "        loss = F.cross_entropy(predicts, y_data)        \n",
    "        acc = paddle.metric.accuracy(predicts, paddle.unsqueeze(y_data,1))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.clear_grad()\n",
    "        avg_loss=paddle.mean(loss)            \n",
    "        print(f'avg_loss:{avg_loss.numpy().item()} acc:{acc.numpy().item()}')\n",
    "        break\n",
    "    scheduler.step()    \n",
    "    #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340aae64-978a-4b5b-aa87-e0090cc52609",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
