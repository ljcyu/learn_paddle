{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "allclose(a, b, rtol=1e-05, atol=1e-08, equal_nan=False)\n",
    "    Returns True if two arrays are element-wise equal within a tolerance.\n",
    "   >The tolerance values are positive, typically very small numbers.  The relative difference (`rtol` * abs(`b`)) and the absolute difference\n",
    "    `atol` are added together to compare against the absolute difference between `a` and `b`.\n",
    "    >If either array contains one or more NaNs, False is returned.Infs are treated as equal if they are in the same place and of the same\n",
    "    sign in both arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "paddle.optimizer中需要scheduler和optimizer，scheduler.step就可以更新学习率，可以一批或一epoch更新学习率， 自己决定\n",
    "fluid.optimizer中的optimizer需要optimizer.minimize(loss)，有这个学习率才变化，不然不变\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.fluid as fluid"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 "
     ]
    }
   ],
   "source": [
    "\n",
    "#不指定学习率退化方法的时候，学习率都一样，哪adam有啥用\n",
    "# example1: LearningRateDecay is not used, return value is all the same\n",
    "with fluid.dygraph.guard():\n",
    "    emb = fluid.dygraph.Embedding([10, 10])\n",
    "    adam = fluid.optimizer.Adam(0.001, parameter_list = emb.parameters())\n",
    "    for i in range(10):\n",
    "        lr = adam.current_step_lr()\n",
    "        print(lr,end=' ') # 0.001\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0 lr:0.2\n",
      "step:1 lr:0.2\n",
      "step:2 lr:0.4\n",
      "step:3 lr:0.4\n",
      "step:4 lr:0.6\n",
      "step:5 lr:0.6\n",
      "step:6 lr:0.8\n",
      "step:7 lr:0.8\n",
      "step:8 lr:1.0\n",
      "step:9 lr:1.0\n",
      "step:10 lr:1.0\n",
      "step:11 lr:1.0\n"
     ]
    }
   ],
   "source": [
    "from paddle.optimizer.lr import PiecewiseDecay\n",
    "\n",
    "# example2: PiecewiseDecay is used, return the step learning rate\n",
    "#with fluid.dygraph.guard():\n",
    "inp = np.random.uniform(-0.1, 0.1, [10, 10]).astype(\"float32\")\n",
    "linear = paddle.nn.Linear(10, 10)\n",
    "inp = paddle.to_tensor(inp)\n",
    "out = linear(inp)\n",
    "loss = paddle.mean(out)\n",
    "\n",
    "bd = [2, 4, 6, 8]\n",
    "value = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "# epoch:上一轮的轮数，重启训练时设置为上一轮的epoch数。默认值为 -1，则为初始学习率 。\n",
    "scheduler=PiecewiseDecay(bd,value,last_epoch=-1)\n",
    "adam = paddle.optimizer.Adam(scheduler,parameters=linear.parameters())\n",
    "\n",
    "# first step: learning rate is 0.2\n",
    "np.allclose(adam.get_lr(), 0.2, rtol=1e-06, atol=0.0) # True\n",
    "\n",
    "# learning rate for different steps\n",
    "ret = [0.2, 0.2, 0.4, 0.4, 0.6, 0.6, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0]\n",
    "for i in range(12):\n",
    "    out=linear(inp)\n",
    "    loss=paddle.mean(out)\n",
    "    lr = adam.get_lr()\n",
    "    loss.backward()\n",
    "    adam.step()\n",
    "    scheduler.step() #没有这个学习率不会变化\n",
    "    adam.clear_grad()\n",
    "\n",
    "    print(f'step:{i} lr:{lr}')\n",
    "    np.allclose(lr, ret[i], rtol=1e-06, atol=0.0) # True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0,lr:0.010000\n",
      "step:1,lr:0.010000\n",
      "step:2,lr:0.005000\n",
      "step:3,lr:0.005000\n",
      "step:4,lr:0.002500\n",
      "step:5,lr:0.002500\n",
      "step:6,lr:0.001250\n",
      "step:7,lr:0.001250\n",
      "step:8,lr:0.000625\n",
      "step:9,lr:0.000625\n",
      "step:10,lr:0.000312\n",
      "step:11,lr:0.000312\n"
     ]
    }
   ],
   "source": [
    "from paddle.fluid.dygraph import ExponentialDecay\n",
    "\n",
    "with fluid.dygraph.guard():\n",
    "    inp = np.random.uniform(-0.1, 0.1, [10, 10]).astype(\"float32\")\n",
    "    linear = paddle.nn.Linear(10, 10)\n",
    "    inp = paddle.to_tensor(inp)\n",
    "    out = linear(inp)\n",
    "    loss = fluid.layers.reduce_mean(out)\n",
    "    \n",
    "    adam = fluid.optimizer.Adam(learning_rate=ExponentialDecay(\n",
    "              learning_rate=0.01,\n",
    "              decay_steps=2,\n",
    "              decay_rate=0.5,\n",
    "              staircase=True), parameter_list = linear.parameters())\n",
    "    for i in range(12):\n",
    "        adam.minimize(loss)#有这个学习率才变化，不然不变\n",
    "        lr = adam.current_step_lr()\n",
    "\n",
    "        print('step:{},lr:{:.6f}'.format(i,lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "1a0b9ba60814a67372f1fa9b4eb66be82019f2c219957cd164c3147c8b4fc0ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
