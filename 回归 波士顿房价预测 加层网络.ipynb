{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和波士顿房价预测相比，完成以下作业\n",
    "2. 挑战题：用代码实现两层的神经网络的梯度传播，中间层的尺寸为13【房价预测案例】（教案当前的版本为一层的神经网络），如 **图14** 所示。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/580f2553aa4643809006f5a8d3deb2aa8dd4e1aa69d94cf6a35ead5fe7cf469e\" width=\"300\"  ></center>\n",
    "<center><br>图14：两层的神经网络</br></center>\n",
    "<br></br>\n",
    "\n",
    "看这里就行[main](#main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入需要用到的package\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入训练数据\n",
    "datafile = './work/housing.data'\n",
    "data = np.fromfile(datafile, sep=' ')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.options.display.max_rows=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入数据 pandas\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "# 读入之后的数据被转化成1维array，其中array的第0-13项是第一条数据，第14-27项是第二条数据，以此类推.... \n",
    "# 这里对原始数据做reshape，变成N x 14的形式\n",
    "feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE','DIS', \n",
    "                 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\n",
    "#原来的数据不是用\\t，一个或多个空格\n",
    "old_df=pd.read_table('./work/housing.txt',names=feature_names)\n",
    "old_df.head(3)\n",
    "old_df[['DIS','TAX']]\n",
    "old_df.max()\n",
    "old_df.min()\n",
    "df=(old_df-old_df.min())/(old_df.max()-old_df.min())\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas划分训练和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.fluid as fluid\n",
    "from sklearn.model_selection import train_test_split\n",
    "import paddle.fluid.dygraph as dygraph\n",
    "from paddle.fluid.dygraph import Linear\n",
    "train_data,eval_data=train_test_split(df,test_size=0.1,random_state=42)\n",
    "BATCH_SIZE = 16\n",
    "def reader_creator2(data):\n",
    "    def reader():\n",
    "        for row in data.values:           \n",
    "            yield [row[:-1], row[-1:]]          #返回Iterable对象\n",
    "    return reader\n",
    "train_reader = paddle.batch(reader_creator2(train_data),\n",
    "                            batch_size=BATCH_SIZE)\n",
    "test_reader = paddle.batch(reader_creator2(eval_data), \n",
    "                           batch_size=BATCH_SIZE)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(3)\n",
    "eval_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 封装成load data函数\n",
    "\n",
    "将上述几个数据处理操作封装成`load data`函数，以便下一步模型的调用，实现方法如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # 从文件导入数据\n",
    "    datafile = './work/housing.data'\n",
    "    data = np.fromfile(datafile, sep=' ')\n",
    "    data\n",
    "\n",
    "    # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数\n",
    "    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \\\n",
    "                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\n",
    "    feature_num = len(feature_names)\n",
    "\n",
    "    # 将原始数据进行Reshape，变成[N, 14]这样的形状\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\n",
    "\n",
    "    # 将原数据集拆分成训练集和测试集\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\n",
    "    # 测试集和训练集必须是没有交集的\n",
    "    ratio = 0.8\n",
    "    offset = int(data.shape[0] * ratio)\n",
    "    training_data = data[:offset]\n",
    "\n",
    "    # 计算训练集的最大值，最小值，平均值    \n",
    "    maximums, minimums, avgs = data.max(axis=0), data.min(axis=0), \\\n",
    "                                 data.sum(axis=0) / data.shape[0]\n",
    "\n",
    "    # 对数据进行归一化处理\n",
    "    for i in range(feature_num):\n",
    "        print(feature_names[i],maximums[i], minimums[i], avgs[i])\n",
    "        data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])\n",
    "\n",
    "    # 训练集和测试集的划分比例\n",
    "    training_data = data[:offset]\n",
    "    test_data = data[offset:]\n",
    "    return training_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取数据\n",
    "training_data, test_data = load_data()\n",
    "x = training_data[:, :-1]\n",
    "y = training_data[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看数据\n",
    "print(x[0])\n",
    "print(y[0])\n",
    "print(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，\n",
    "        # 此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于Network类的定义，模型的计算过程如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(13)\n",
    "x1 = x[0]\n",
    "y1 = y[0]\n",
    "z = net.forward(x1)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练配置\n",
    "\n",
    "模型设计完成后，需要通过训练配置寻找模型的最优值，即通过损失函数来衡量模型的好坏。训练配置也是深度学习模型关键要素之一。\n",
    "\n",
    "通过模型计算$x_1$表示的影响因素所对应的房价应该是$z$, 但实际数据告诉我们房价是$y$。这时我们需要有某种指标来衡量预测值$z$跟真实值$y$之间的差距。对于回归问题，最常采用的衡量方法是使用均方误差作为评价模型好坏的指标，具体定义如下：\n",
    "\n",
    "$$Loss = (y - z)^2$$\n",
    "\n",
    "上式中的$Loss$（简记为: $L$）通常也被称作损失函数，它是衡量模型好坏的指标。在回归问题中，均方误差是一种比较常见的形式，分类问题中通常会采用交叉熵作为损失函数，在后续的章节中会更详细的介绍。对一个样本计算损失函数值的实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss = (y1 - z)*(y1 - z)\n",
    "print(Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为计算损失函数时需要把每个样本的损失函数值都考虑到，所以我们需要对单个样本的损失函数进行求和，并除以样本总数$N$。\n",
    "$$Loss= \\frac{1}{N}\\sum_{i=1}^N{(y_i - z_i)^2}$$\n",
    "在Network类下面添加损失函数的计算过程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial{L}}{\\partial{w_0}} = ({x_1^{0}\\cdot w_0} + {x_1^{1}\\cdot w_1} + ...  + {x_1^{12}\\cdot w_{12}} + b - y_1)\\cdot x_1^{0}=({z_1} - {y_1})\\cdot x_1^{0}$$\n",
    "\n",
    "$w_0$只和$x^{0}$有关这点很重要\n",
    "\n",
    "$$dw=np.dot(x^{T},z-y)$$\n",
    "\n",
    "$$x^{T}= \\left\\{\\begin{matrix} x_1^{0} & x_2^{0} & x_3^{0}\\\\ x_1^{1} & x_2^{1} & x_3^{1} \\\\ \\vdots & \\vdots & \\vdots \\\\ x_1^{12} & x_2^{12} & x_3^{12} \\end{matrix} \\right\\}$$\n",
    "$$z-y= \\left\\{\\begin{matrix} z_1-y_1\\\\ z_2-y_2 \\\\ z_3-y_3 \\end{matrix} \\right\\}$$\n",
    "$$\n",
    "x^{T}\\cdot(z-y)=\\left\\{\\begin{matrix} x_1^{0}\\cdot(z_1-y_1)+ x_2^{0}\\cdot(z_2-y_2) +x_3^{0}\\cdot(z_3-y_3)\\\\ x_1^{1}\\cdot(z_1-y_1)+ x_2^{2}\\cdot(z_2-y_2) +x_3^{3}\\cdot(z_3-y_3) \\\\ \\vdots  \\\\ x_1^{12}\\cdot(z_1-y_1)+ x_2^{12}\\cdot(z_2-y_2) +x_3^{12}\\cdot(z_3-y_3) \\end{matrix} \\right\\}\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{w_j}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)\\frac{\\partial{z_i}}{\\partial{w_j}}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)x_i^{j}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network2(object):\n",
    "    def __init__(self,num_of_weights):\n",
    "        np.random.seed(100)\n",
    "        self.w=np.random.randn(num_of_weights,1)\n",
    "        self.b=0\n",
    "    def forward(self,x):\n",
    "        z=np.dot(x,self.w)+self.b\n",
    "        return z\n",
    "    def loss(self,z,y):\n",
    "        error=z-y\n",
    "        cost=error**2\n",
    "        cost=np.mean(cost)\n",
    "        return cost        \n",
    "    def update(self,x,y,z):\n",
    "        lr=0.01\n",
    "        m=len(z)      \n",
    "        dw=np.dot(x.T,z-y)/m\n",
    "        #print(dw)\n",
    "        self.w=self.w-lr*dw\n",
    "        db=np.sum(z-y,axis=0)/m\n",
    "        self.b=self.b-lr*db\n",
    "    def train(self,x,y):\n",
    "        z=self.forward(x)\n",
    "        loss_=self.loss(z,y)\n",
    "        self.update(x,y,z) \n",
    "        return loss_       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=Network2(13)\n",
    "x_=x[:3]\n",
    "y_=y[:3]\n",
    "z=net.forward(x_)\n",
    "cost=net.loss(y_,z)\n",
    "\n",
    "z.shape\n",
    "x_.shape\n",
    "cost\n",
    "net.update(x_,y_,z)\n",
    "z=net.forward(x_)\n",
    "cost=net.loss(y_,z)\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自己写的训练\n",
    "from visualdl import LogWriter\n",
    "writer=LogWriter('log/network2')\n",
    "net=Network2(13)\n",
    "losses=[]\n",
    "for i in range(1000):\n",
    "    loss_=net.train(x,y)\n",
    "    if i%5==0 or i==999:print(loss_)\n",
    "\n",
    "    writer.add_scalar('loss',step=i,value=loss_)    \n",
    "    losses.append(loss_)\n",
    "\n",
    "    if loss_<1e-4:break    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自己写的绘图\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "t=np.arange(0,1000,1)\n",
    "fig,ax=plt.subplots()\n",
    "\n",
    "ax.plot(t,losses)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自己写的训练，加上batch\n",
    "from visualdl import LogWriter\n",
    "writer=LogWriter('log/network2-batch')\n",
    "net=Network2(13)\n",
    "losses=[]\n",
    "batch_=32\n",
    "np.random.shuffle(training_data)\n",
    "batches_=[training_data[k:k+batch_] for k in range(0,len(training_data),batch_)]\n",
    "\n",
    "print(batches_[0].shape)\n",
    "print(batches_[-1].shape)\n",
    "all_iter=0\n",
    "for i in range(500):\n",
    "    avg_loss=0\n",
    "    for batch_id,mini_batch in enumerate(batches_):        \n",
    "        x_=mini_batch[:,:-1]\n",
    "        y_=mini_batch[:,-1:]\n",
    "        loss_=net.train(x_,y_)\n",
    "        avg_loss=avg_loss+loss_\n",
    "        all_iter=all_iter+1\n",
    "        if i%5==0 :print(loss_)\n",
    "        writer.add_scalar('loss-32-e500',step=all_iter,value=loss_)    \n",
    "        losses.append(loss_)\n",
    "    avg_loss=avg_loss/len(training_data)\n",
    "    print(avg_loss)\n",
    "    if avg_loss<1e-4:break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_process(iters,data,title='loss',color='green',label='loss'):\n",
    "    plt.title(title, fontsize=24)\n",
    "    plt.xlabel(\"iter\", fontsize=20)\n",
    "    plt.ylabel(label, fontsize=20)\n",
    "    plt.plot(iters, data,color=color,label=label) \n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"main\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x->h1->z$，和真实值$y$\n",
    "$$h1=w_1x+b_1,z=w_2h1+b_2$$\n",
    "$$\\frac{\\partial{L}}{\\partial{w_2}} = \\frac{\\partial{L}}{\\partial{z}}\\cdot \\frac{\\partial{z}}{\\partial{w_2}}=\\frac{\\partial{L}}{\\partial{z}}\\cdot h1$$\n",
    "$$\\frac{\\partial{L}}{\\partial{b_2}} = \\frac{\\partial{L}}{\\partial{z}}\\cdot \\frac{\\partial{z}}{\\partial{b_2}}=\\frac{\\partial{L}}{\\partial{z}}\\cdot 1$$\n",
    "$$\\frac{\\partial{z}}{\\partial{h1}} = w_2$$\n",
    "$$\\frac{\\partial{L}}{\\partial{w_1}} = \\frac{\\partial{L}}{\\partial{z}}\\cdot \\frac{\\partial{z}}{\\partial{h1}} \\cdot \\frac{\\partial{h1}}{\\partial{w_1}}=\\frac{\\partial{L}}{\\partial{z}}\\cdot w_2 \\cdot x$$\n",
    "$$\\frac{\\partial{L}}{\\partial{b_1}} = \\frac{\\partial{L}}{\\partial{z}}\\cdot \\frac{\\partial{z}}{\\partial{h1}} \\cdot \\frac{\\partial{h1}}{\\partial{b_1}}=\\frac{\\partial{L}}{\\partial{z}}\\cdot w_2 \\cdot 1$$\n",
    "$$\\frac{\\partial{L}}{\\partial{w_1}} = \\frac{\\partial{L}}{\\partial{z}}\\cdot \\frac{\\partial{z}}{\\partial{h1}} \\cdot \\frac{\\partial{h1}}{\\partial{w_1}}$$\n",
    "$$\\frac{\\partial{L}}{\\partial{z}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加个中间13层的网，13->13->1\n",
    "import numpy as np\n",
    "class Network13_13(object):\n",
    "    def __init__(self,num_of_weights):\n",
    "        np.random.seed(100)\n",
    "        self.w1=np.random.randn(num_of_weights,num_of_weights)\n",
    "        self.b1=np.random.randn(1,num_of_weights)\n",
    "        self.w2=np.random.randn(num_of_weights,1)\n",
    "        self.b2=0\n",
    "    def forward(self,x):            \n",
    "        h1=np.dot(x,self.w1)+self.b1\n",
    "        z=np.dot(h1,self.w2)+self.b2\n",
    "        #print(h1,z)\n",
    "        return h1,z\n",
    "    def loss(self,z,y):\n",
    "        error=z-y\n",
    "        cost=error**2\n",
    "        cost=np.mean(cost)\n",
    "        return cost                 \n",
    "    def update(self,x,h1,z,y):#x->h1->z，实际值为y\n",
    "        lr=0.0005\n",
    "        m=len(y)      \n",
    "        db2=np.sum(z-y,axis=0)/m\n",
    "        dw2=np.dot(h1.T,z-y)/m\n",
    "        dh1=np.dot(z-y,self.w2.T)/m\n",
    "        db1=np.sum(dh1,axis=0)\n",
    "        #db1=np.sum(self.w2,axis=0)        \n",
    "        dw1=np.dot(x.T,dh1)\n",
    "        \"\"\"\n",
    "        print('x.shape {},h1.shape {},z.shape {},y.shape{}'.format(x.shape,h1.shape,z.shape,y.shape))\n",
    "        print('dw2',dw2.shape)\n",
    "        print('db2',db2.shape)\n",
    "        print('dw1',dw1.shape)        \n",
    "        print('db1',db1.shape)\n",
    "        \"\"\"\n",
    "        self.w1=self.w1-lr*dw1\n",
    "        self.b2=self.b2-lr*db2\n",
    "        self.b1=self.b1-lr*db1\n",
    "        self.w2=self.w2-lr*dw2\n",
    "    def train(self,x,y):\n",
    "        h1,z=self.forward(x)    \n",
    "        loss_=self.loss(z,y)\n",
    "        self.update(x,h1,z,y)\n",
    "        return loss_\n",
    "\n",
    "\n",
    "\n",
    "net=Network13_13(13)\n",
    "\n",
    "batch_=32\n",
    "np.random.shuffle(training_data)\n",
    "batches_=[training_data[k:k+batch_] for k in range(0,len(training_data),batch_)]\n",
    "\n",
    "print(batches_[0].shape)\n",
    "print(batches_[-1].shape)\n",
    "losses=[]\n",
    "all_iter=0\n",
    "all_iters=[]\n",
    "for i in range(50):\n",
    "    avg_loss=0\n",
    "    for batch_id,mini_batch in enumerate(batches_):        \n",
    "        x_=mini_batch[:,:-1]\n",
    "        y_=mini_batch[:,-1:]\n",
    "        loss_=net.train(x_,y_)\n",
    "        avg_loss=avg_loss+loss_\n",
    "        all_iter=all_iter+1\n",
    "        all_iters.append(all_iter)\n",
    "        if i%5==0 :print(loss_)        \n",
    "        losses.append(loss_)\n",
    "    avg_loss=avg_loss/len(training_data)\n",
    "    print('avg_loss',avg_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "draw_process(all_iters,losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
