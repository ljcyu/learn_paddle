{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-1, 3, 10) (-1, 1) (-1, 7) name: \"cross_entropy2_0.tmp_0\"\n",
      "type {\n",
      "  type: LOD_TENSOR\n",
      "  lod_tensor {\n",
      "    tensor {\n",
      "      data_type: FP32\n",
      "      dims: -1\n",
      "      dims: 1\n",
      "    }\n",
      "    lod_level: 0\n",
      "  }\n",
      "}\n",
      "persistable: false\n",
      "\n",
      "name: \"x\"\n",
      "type {\n",
      "  type: LOD_TENSOR\n",
      "  lod_tensor {\n",
      "    tensor {\n",
      "      data_type: FP32\n",
      "      dims: -1\n",
      "      dims: 3\n",
      "      dims: 10\n",
      "    }\n",
      "    lod_level: 0\n",
      "  }\n",
      "}\n",
      "persistable: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import paddle.fluid as fluid\n",
    "class_num = 7\n",
    "x = fluid.layers.data(name='x', shape=[3, 10], dtype='float32') # -1 3 10\n",
    "label = fluid.layers.data(name='label', shape=[1], dtype='int64') #-1 1\n",
    "predict = fluid.layers.fc(input=x, size=class_num, act='softmax')#-1 7\n",
    "cost = fluid.layers.cross_entropy(input=predict, label=label)\n",
    "print(x.shape,label.shape,predict.shape,cost)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: [0.35952604]\n",
      "step: 1, loss: [0.31887382]\n",
      "step: 2, loss: [0.2995552]\n",
      "step: 3, loss: [0.28412756]\n",
      "step: 4, loss: [0.26553214]\n",
      "step: 5, loss: [0.24091512]\n",
      "step: 6, loss: [0.21059522]\n",
      "step: 7, loss: [0.17760354]\n",
      "step: 8, loss: [0.14681032]\n",
      "step: 9, loss: [0.12299061]\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "\n",
    "inputs = paddle.rand((256, 64))\n",
    "\n",
    "linear = paddle.nn.Linear(64, 8, bias_attr=False)\n",
    "loss_fn = paddle.nn.MSELoss()\n",
    "optimizer = paddle.optimizer.Adam(0.01, parameters=linear.parameters())\n",
    "\n",
    "for i in range(10):\n",
    "    hidden = linear(inputs)\n",
    "    # weight from input to hidden is shared with the linear mapping from hidden to output\n",
    "    outputs = paddle.matmul(hidden, linear.weight, transpose_y=True)\n",
    "    loss = loss_fn(outputs, inputs)\n",
    "    loss.backward()\n",
    "    print(\"step: {}, loss: {}\".format(i, loss.numpy()))\n",
    "    optimizer.step()\n",
    "    optimizer.clear_grad()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-01 13:30:17.745 | INFO     | __main__:<module>:8 - fc.weight:Parameter containing:\n",
      "Tensor(shape=[8, 5], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
      "       [[-0.26917499,  0.21250401, -0.64733118, -0.55289793,  0.64664483],\n",
      "        [ 0.03267508, -0.23102362, -0.62312615, -0.66298652,  0.56355345],\n",
      "        [ 0.10465726,  0.14252797, -0.65502608,  0.30007410, -0.56954384],\n",
      "        [ 0.14217915,  0.17066082, -0.02539300, -0.16849701, -0.14745426],\n",
      "        [ 0.63527560,  0.15381630, -0.60549498, -0.15355670, -0.44909406],\n",
      "        [ 0.35809821, -0.03604871,  0.42882413, -0.52067983,  0.09776447],\n",
      "        [ 0.30929220,  0.09259517, -0.57030648,  0.01338258,  0.59317863],\n",
      "        [-0.60106969,  0.13676916, -0.09769146,  0.49599123, -0.23148265]])\n",
      "2022-11-01 13:30:17.748 | INFO     | __main__:<module>:9 - fc.bias:Parameter containing:\n",
      "Tensor(shape=[5], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
      "       [0., 0., 0., 0., 0.])\n",
      "2022-11-01 13:30:17.753 | INFO     | __main__:<module>:16 - x:Tensor(shape=[2, 8], dtype=float32, place=CUDAPlace(0), stop_gradient=True,\n",
      "       [[0.78531247, 0.81736028, 0.79681695, 0.15102093, 0.93136758, 0.04332138, 0.16646625, 0.49245149],\n",
      "        [0.12513840, 0.55602980, 0.11535818, 0.45463714, 0.78989142, 0.84746808, 0.13083205, 0.39368597]])\n",
      "2022-11-01 13:30:17.757 | INFO     | __main__:<module>:19 - fc(x):Tensor(shape=[2, 5], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
      "       [[ 0.28286302,  0.34185854, -2.23185253, -0.68153381,  0.06306744],\n",
      "        [ 0.67030537,  0.14907347, -0.74252456, -0.84535497, -0.02387509]])\n",
      "2022-11-01 13:30:17.761 | INFO     | __main__:<module>:21 - predicts:Tensor(shape=[2, 5], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
      "       [[0.30070192, 0.31897572, 0.02432255, 0.11463151, 0.24136828],\n",
      "        [0.39117682, 0.23227635, 0.09523331, 0.08592711, 0.19538642]])\n",
      "2022-11-01 13:30:17.763 | INFO     | __main__:<module>:24 - loss:Tensor(shape=[1], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
      "       [1.46066070])\n",
      "2022-11-01 13:30:17.766 | INFO     | __main__:<module>:30 - 1.4606607\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "from loguru import logger\n",
    "\n",
    "logger.add('fc.log')\n",
    "\n",
    "fc = paddle.nn.Linear(8,5)\n",
    "logger.info('fc.weight:%s'%fc.weight)\n",
    "logger.info('fc.bias:%s'%fc.bias)\n",
    "# 设置优化器\n",
    "optim = paddle.optimizer.Adam(learning_rate=0.001, parameters=fc.parameters())\n",
    "x = paddle.rand([2,2,2,2])\n",
    "label = paddle.to_tensor([1,0],dtype='int64')\n",
    "\n",
    "x=paddle.reshape(x,[-1,8])\n",
    "logger.info('x:%s'%x)\n",
    "# 前向传播\n",
    "predicts = fc(x)\n",
    "logger.info('fc(x):%s'%predicts)\n",
    "#predicts=F.softmax(predicts) # cross_entroy会计算softmax，所以这里不要计算\n",
    "logger.info('predicts:%s'%predicts)\n",
    "# 计算损失\n",
    "loss = F.cross_entropy(predicts, label)\n",
    "logger.info('loss:%s'%loss)\n",
    "# 开始进行反向传播\n",
    "loss.backward()\n",
    "# 更新参数\n",
    "optim.step()\n",
    "loss.clear_grad()\n",
    "logger.info(loss.numpy()[0])\n",
    "'''\n",
    "# 计算在cross_entry前计算softmax的计算结果，当然不应该计算，这里是验证，才发现不应该计算\n",
    "2022-11-01 13:30:17.745 | INFO     | __main__:<module>:8 - fc.weight:Parameter containing:\n",
    "Tensor(shape=[8, 5], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
    "       [[-0.26917499,  0.21250401, -0.64733118, -0.55289793,  0.64664483],\n",
    "        [ 0.03267508, -0.23102362, -0.62312615, -0.66298652,  0.56355345],\n",
    "        [ 0.10465726,  0.14252797, -0.65502608,  0.30007410, -0.56954384],\n",
    "        [ 0.14217915,  0.17066082, -0.02539300, -0.16849701, -0.14745426],\n",
    "        [ 0.63527560,  0.15381630, -0.60549498, -0.15355670, -0.44909406],\n",
    "        [ 0.35809821, -0.03604871,  0.42882413, -0.52067983,  0.09776447],\n",
    "        [ 0.30929220,  0.09259517, -0.57030648,  0.01338258,  0.59317863],\n",
    "        [-0.60106969,  0.13676916, -0.09769146,  0.49599123, -0.23148265]])\n",
    "2022-11-01 13:30:17.748 | INFO     | __main__:<module>:9 - fc.bias:Parameter containing:\n",
    "Tensor(shape=[5], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
    "       [0., 0., 0., 0., 0.])\n",
    "2022-11-01 13:30:17.753 | INFO     | __main__:<module>:16 - x:Tensor(shape=[2, 8], dtype=float32, place=CUDAPlace(0), stop_gradient=True,\n",
    "       [[0.78531247, 0.81736028, 0.79681695, 0.15102093, 0.93136758, 0.04332138, 0.16646625, 0.49245149],\n",
    "        [0.12513840, 0.55602980, 0.11535818, 0.45463714, 0.78989142, 0.84746808, 0.13083205, 0.39368597]])\n",
    "2022-11-01 13:30:17.757 | INFO     | __main__:<module>:19 - fc(x):Tensor(shape=[2, 5], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
    "       [[ 0.28286302,  0.34185854, -2.23185253, -0.68153381,  0.06306744],\n",
    "        [ 0.67030537,  0.14907347, -0.74252456, -0.84535497, -0.02387509]])\n",
    "2022-11-01 13:30:17.761 | INFO     | __main__:<module>:21 - predicts:Tensor(shape=[2, 5], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
    "       [[0.30070192, 0.31897572, 0.02432255, 0.11463151, 0.24136828],\n",
    "        [0.39117682, 0.23227635, 0.09523331, 0.08592711, 0.19538642]])\n",
    "2022-11-01 13:30:17.763 | INFO     | __main__:<module>:24 - loss:Tensor(shape=[1], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
    "       [1.46066070])\n",
    "2022-11-01 13:30:17.766 | INFO     | __main__:<module>:30 - 1.4606607\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.30070193 0.31897573 0.02432255 0.1146315  0.24136829]\n",
      " [0.39117682 0.23227634 0.09523331 0.08592711 0.19538642]]\n",
      "[[-1.51503367 -1.49675987 -1.79141305 -1.7011041  -1.57436732]\n",
      " [-1.42456139 -1.58346187 -1.7205049  -1.72981111 -1.62035179]]\n",
      "1.4606606295728652\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "fc_w=np.array([[-0.26917499,  0.21250401, -0.64733118, -0.55289793,  0.64664483],\n",
    "               [ 0.03267508, -0.23102362, -0.62312615, -0.66298652,  0.56355345],\n",
    "               [ 0.10465726,  0.14252797, -0.65502608,  0.30007410, -0.56954384],\n",
    "               [ 0.14217915,  0.17066082, -0.02539300, -0.16849701, -0.14745426],\n",
    "               [ 0.63527560,  0.15381630, -0.60549498, -0.15355670, -0.44909406],\n",
    "               [ 0.35809821, -0.03604871,  0.42882413, -0.52067983,  0.09776447],\n",
    "               [ 0.30929220,  0.09259517, -0.57030648,  0.01338258,  0.59317863],\n",
    "               [-0.60106969,  0.13676916, -0.09769146,  0.49599123, -0.23148265]])\n",
    "fc_b=np.array([0., 0., 0., 0., 0.])\n",
    "x=np.array([[0.78531247, 0.81736028, 0.79681695, 0.15102093, 0.93136758, 0.04332138, 0.16646625, 0.49245149],\n",
    "            [0.12513840, 0.55602980, 0.11535818, 0.45463714, 0.78989142, 0.84746808, 0.13083205, 0.39368597]])\n",
    "y=np.array([[ 0.28286302,  0.34185854, -2.23185253, -0.68153381,  0.06306744],\n",
    "            [ 0.67030537,  0.14907347, -0.74252456, -0.84535497, -0.02387509]])\n",
    "predicts_softmax=np.array([[0.30070192, 0.31897572, 0.02432255, 0.11463151, 0.24136828],\n",
    "                   [0.39117682, 0.23227635, 0.09523331, 0.08592711, 0.19538642]])\n",
    "loss=1.4606607\n",
    "_y=np.matmul(x,fc_w)+fc_b\n",
    "_y=np.exp(y)/np.sum(np.exp(y),axis=1,keepdims=True)\n",
    "print(_y)\n",
    "# cross_entroy会先计算softmax，然后再计算交叉熵，所以不用自己计算softmax\n",
    "_y=np.exp(_y)/np.sum(np.exp(_y),axis=1,keepdims=True)\n",
    "_y=np.log(_y)\n",
    "print(_y)\n",
    "labels=np.array([1,0])\n",
    "#\n",
    "print((-_y[0][1]-_y[1][0])/2)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[2, 5], dtype=float64, place=CUDAPlace(0), stop_gradient=True,\n",
      "       [[ 0.06601013,  0.37503667, -0.60469677,  0.68248436,  0.20264421],\n",
      "        [-0.16129715,  0.02520340,  0.59349338,  0.51909989, -0.52911843]])\n",
      "Tensor(shape=[2], dtype=int64, place=CUDAPlace(0), stop_gradient=True,\n",
      "       [0, 1])\n",
      "Tensor(shape=[1], dtype=float64, place=CUDAPlace(0), stop_gradient=True,\n",
      "       [1.76475810])\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import numpy as np\n",
    "\n",
    "input_data = paddle.uniform([2, 5], dtype=\"float64\")\n",
    "label_data = np.random.randint(0, 2, size=(2)).astype(np.int64)\n",
    "\n",
    "\n",
    "input =  paddle.to_tensor(input_data)\n",
    "label =  paddle.to_tensor(label_data)\n",
    "print(input)\n",
    "print(label)\n",
    "\n",
    "ce_loss = paddle.nn.CrossEntropyLoss()\n",
    "output = ce_loss(input, label)\n",
    "print(output)\n",
    "'''\n",
    "Tensor(shape=[2, 5], dtype=float64, place=CUDAPlace(0), stop_gradient=True,\n",
    "       [[ 0.06601013,  0.37503667, -0.60469677,  0.68248436,  0.20264421],\n",
    "        [-0.16129715,  0.02520340,  0.59349338,  0.51909989, -0.52911843]])\n",
    "Tensor(shape=[2], dtype=int64, place=CUDAPlace(0), stop_gradient=True,\n",
    "       [0, 1])\n",
    "Tensor(shape=[1], dtype=float64, place=CUDAPlace(0), stop_gradient=True,\n",
    "       [1.76475810])\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自己计算softmax [[0.17029278 0.23195555 0.0870787  0.31544787 0.19522511]\n",
      " [0.14287564 0.17216876 0.30392019 0.28213104 0.09890436]]\n",
      "paddle计算softmax Tensor(shape=[2, 5], dtype=float64, place=CUDAPlace(0), stop_gradient=True,\n",
      "       [[0.17029278, 0.23195555, 0.08707870, 0.31544787, 0.19522511],\n",
      "        [0.14287564, 0.17216876, 0.30392019, 0.28213104, 0.09890436]])\n",
      "Tensor(shape=[2, 5], dtype=float64, place=CUDAPlace(0), stop_gradient=True,\n",
      "       [[-1.77023608, -1.46120954, -2.44094298, -1.15376185, -1.63360200],\n",
      "        [-1.94578066, -1.75928011, -1.19099013, -1.26538362, -2.31360194]])\n",
      "Tensor(shape=[1], dtype=float64, place=CUDAPlace(0), stop_gradient=True,\n",
      "       [1.76475810])\n"
     ]
    }
   ],
   "source": [
    "input2=np.array([[ 0.06601013,  0.37503667, -0.60469677,  0.68248436,  0.20264421],[-0.16129715,  0.02520340,  0.59349338,  0.51909989, -0.52911843]])\n",
    "labels2=np.array([0,1])\n",
    "_softmax=np.exp(input2)/(np.sum(np.exp(input2),axis=1,keepdims=True))\n",
    "print(f'自己计算softmax {_softmax}')\n",
    "#_softmax_res=np.array([[0.17029278,0.23195555,0.0870787,0.31544787,0.19522511],[0.14287564,0.17216876,0.30392019,0.28213104,0.09890436]])\n",
    "_softmax_paddle=F.softmax(paddle.to_tensor(input2))\n",
    "print(f'paddle计算softmax {_softmax_paddle}')\n",
    "log_res=paddle.log(_softmax_paddle)\n",
    "print(log_res)\n",
    "# softmax-log-NLLLoss\n",
    "# 先求softmax，再求log，然后求NLLLoss\n",
    "# NLLLoss的结果就是把上面的输出与Label对应的那个值拿出来，再去掉负号，再求均值。\n",
    "# [Pytorch详解NLLLoss和CrossEntropyLoss](https://blog.csdn.net/qq_22210253/article/details/85229988)\n",
    "print((-log_res[0][0]+-log_res[1][1])/2)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
