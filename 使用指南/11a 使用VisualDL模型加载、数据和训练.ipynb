{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[使用 VisualDL 可视化模型，数据和训练-使用文档-PaddlePaddle深度学习平台](https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/advanced/visualdl_usage_cn.html)\n",
    "\n",
    "运行该代码后，将会创建一个./runs/mnist_experiment 文件夹，用于存储写入到 VisualDL 的数据。\n",
    "可以在训练程序执行前、中、后任意一个阶段，启动 VisualDL 的可视化服务、读取数据、并进入浏览器查看。启动命令为：\n",
    "visualdl --logdir ./runs/mnist_experiment --model ./runs/mnist_experiment/model.pdmodel --host 0.0.0.0 --port 8040\n",
    "--logdir：与使用 LogWriter 时指定的参数相同。\n",
    "--model：（可选）为保存的网络模型结构文件。\n",
    "--host：指定服务的 IP 地址。\n",
    "--port：指定服务的端口地址。\n",
    "在命令行中输入上述命令启动服务后，可以在浏览器中输入 http://localhost:8040 (也可以查看 ip 地址，将 localhost 换成 ip)进行查看。\n",
    "如果是在AI Studio上训练程序，可以在模型训练结束后，参考如下界面设置日志文件所在目录和模型文件，启动 VisualDL 的可视化服务。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "# 加载飞桨相关库\n",
    "import paddle\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear\n",
    "import paddle.nn.functional as F\n",
    "# 从 visualdl 库中引入 LogWriter 类\n",
    "from visualdl import LogWriter\n",
    "# 创建 LogWriter 对象，指定 logdir 参数，如果指定路径不存在将会创建一个文件夹\n",
    "logwriter = LogWriter(logdir='./runs/mnist_experiment')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "# 数据载入\n",
    "class MNISTDataset(paddle.io.Dataset):\n",
    "    def __init__(self, mode='train'):\n",
    "        self.mnist_data = paddle.vision.datasets.MNIST(mode=mode)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.mnist_data[idx]\n",
    "        data = np.reshape(data, [1, 28, 28]).astype('float32') / 255\n",
    "        label = np.reshape(label, [1]).astype('int64')\n",
    "        return (data, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cache file C:\\Users\\ljc\\.cache\\paddle\\dataset\\mnist\\t10k-images-idx3-ubyte.gz not found, downloading https://dataset.bj.bcebos.com/mnist/t10k-images-idx3-ubyte.gz \n",
      "Begin to download\n",
      "\n",
      "Download finished\n",
      "Cache file C:\\Users\\ljc\\.cache\\paddle\\dataset\\mnist\\t10k-labels-idx1-ubyte.gz not found, downloading https://dataset.bj.bcebos.com/mnist/t10k-labels-idx1-ubyte.gz \n",
      "Begin to download\n",
      "..\n",
      "Download finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader = paddle.io.DataLoader(MNISTDataset(mode='train'),batch_size=16,shuffle=True)\n",
    "test_loader = paddle.io.DataLoader(MNISTDataset(mode='test'),batch_size=16,shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "# 定义 mnist 数据识别网络模型结构\n",
    "class MNIST(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(MNIST, self).__init__()\n",
    "\n",
    "        # 定义卷积层，输出特征通道 out_channels 设置为 20，卷积核的大小 kernel_size 为 5，卷积步长 stride=1，padding=2\n",
    "        self.conv1 = Conv2D(in_channels=1, out_channels=20, kernel_size=5, stride=1, padding=2)\n",
    "        # 定义池化层，池化核的大小 kernel_size 为 2，池化步长为 2\n",
    "        self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义卷积层，输出特征通道 out_channels 设置为 20，卷积核的大小 kernel_size 为 5，卷积步长 stride=1，padding=2\n",
    "        self.conv2 = Conv2D(in_channels=20, out_channels=20, kernel_size=5, stride=1, padding=2)\n",
    "        # 定义池化层，池化核的大小 kernel_size 为 2，池化步长为 2\n",
    "        self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义一层全连接层，输出维度是 10\n",
    "        self.fc = Linear(in_features=980, out_features=10)\n",
    "\n",
    "    # 定义网络前向计算过程，卷积后紧接着使用池化层，最后使用全连接层计算最终输出\n",
    "    # 卷积层激活函数使用 Relu，全连接层激活函数使用 softmax\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = paddle.reshape(x, [x.shape[0], -1])\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss is: [2.4059968]\n",
      "epoch: 0, batch: 200, loss is: [1.7189382]\n",
      "epoch: 0, batch: 400, loss is: [1.0198879]\n",
      "epoch: 0, batch: 600, loss is: [1.0962524]\n",
      "epoch: 0, batch: 800, loss is: [0.6769724]\n",
      "epoch: 0, batch: 1000, loss is: [0.65520746]\n",
      "epoch: 0, batch: 1200, loss is: [0.5241337]\n",
      "epoch: 0, batch: 1400, loss is: [0.4246518]\n",
      "epoch: 0, batch: 1600, loss is: [0.43745375]\n",
      "epoch: 0, batch: 1800, loss is: [0.22160484]\n",
      "epoch: 0, batch: 2000, loss is: [0.17028359]\n",
      "epoch: 0, batch: 2200, loss is: [0.70554364]\n",
      "epoch: 0, batch: 2400, loss is: [0.2472116]\n",
      "epoch: 0, batch: 2600, loss is: [0.41409796]\n",
      "epoch: 0, batch: 2800, loss is: [0.1965942]\n",
      "epoch: 0, batch: 3000, loss is: [0.47332242]\n",
      "epoch: 0, batch: 3200, loss is: [0.21878421]\n",
      "epoch: 0, batch: 3400, loss is: [0.16712691]\n",
      "epoch: 0, batch: 3600, loss is: [0.06845479]\n",
      "[validation]After epoch 0: accuracy/loss: 0.927299976348877/0.26732635498046875\n",
      "epoch: 1, batch: 0, loss is: [0.53019166]\n",
      "epoch: 1, batch: 200, loss is: [0.1959933]\n",
      "epoch: 1, batch: 400, loss is: [0.18516138]\n",
      "epoch: 1, batch: 600, loss is: [0.2573491]\n",
      "epoch: 1, batch: 800, loss is: [0.34747556]\n",
      "epoch: 1, batch: 1000, loss is: [0.42199644]\n",
      "epoch: 1, batch: 1200, loss is: [0.19626273]\n",
      "epoch: 1, batch: 1400, loss is: [0.0532621]\n",
      "epoch: 1, batch: 1600, loss is: [0.3892492]\n",
      "epoch: 1, batch: 1800, loss is: [0.17962532]\n",
      "epoch: 1, batch: 2000, loss is: [0.29207575]\n",
      "epoch: 1, batch: 2200, loss is: [0.07761687]\n",
      "epoch: 1, batch: 2400, loss is: [0.1699008]\n",
      "epoch: 1, batch: 2600, loss is: [0.3012043]\n",
      "epoch: 1, batch: 2800, loss is: [0.18836205]\n",
      "epoch: 1, batch: 3000, loss is: [0.2073532]\n",
      "epoch: 1, batch: 3200, loss is: [0.33036968]\n",
      "epoch: 1, batch: 3400, loss is: [0.0587175]\n",
      "epoch: 1, batch: 3600, loss is: [0.29357636]\n",
      "[validation]After epoch 1: accuracy/loss: 0.9427000284194946/0.1932850480079651\n",
      "epoch: 2, batch: 0, loss is: [0.23045069]\n",
      "epoch: 2, batch: 200, loss is: [0.23709688]\n",
      "epoch: 2, batch: 400, loss is: [0.1395596]\n",
      "epoch: 2, batch: 600, loss is: [0.2302069]\n",
      "epoch: 2, batch: 800, loss is: [0.11016472]\n",
      "epoch: 2, batch: 1000, loss is: [0.30414334]\n",
      "epoch: 2, batch: 1200, loss is: [0.13015276]\n",
      "epoch: 2, batch: 1400, loss is: [0.18106015]\n",
      "epoch: 2, batch: 1600, loss is: [0.06265001]\n",
      "epoch: 2, batch: 1800, loss is: [0.28324956]\n",
      "epoch: 2, batch: 2000, loss is: [0.19891857]\n",
      "epoch: 2, batch: 2200, loss is: [0.08718283]\n",
      "epoch: 2, batch: 2400, loss is: [0.68527734]\n",
      "epoch: 2, batch: 2600, loss is: [0.07361845]\n",
      "epoch: 2, batch: 2800, loss is: [0.2088323]\n",
      "epoch: 2, batch: 3000, loss is: [0.03541452]\n",
      "epoch: 2, batch: 3200, loss is: [0.13110353]\n",
      "epoch: 2, batch: 3400, loss is: [0.30135083]\n",
      "epoch: 2, batch: 3600, loss is: [0.16552006]\n",
      "[validation]After epoch 2: accuracy/loss: 0.95660001039505/0.1501796692609787\n",
      "epoch: 3, batch: 0, loss is: [0.08382733]\n",
      "epoch: 3, batch: 200, loss is: [0.1049588]\n",
      "epoch: 3, batch: 400, loss is: [0.04179634]\n",
      "epoch: 3, batch: 600, loss is: [0.18033217]\n",
      "epoch: 3, batch: 800, loss is: [0.02150084]\n",
      "epoch: 3, batch: 1000, loss is: [0.11953457]\n",
      "epoch: 3, batch: 1200, loss is: [0.15269548]\n",
      "epoch: 3, batch: 1400, loss is: [0.13793439]\n",
      "epoch: 3, batch: 1600, loss is: [0.73445624]\n",
      "epoch: 3, batch: 1800, loss is: [0.03000691]\n",
      "epoch: 3, batch: 2000, loss is: [0.16887581]\n",
      "epoch: 3, batch: 2200, loss is: [0.0982569]\n",
      "epoch: 3, batch: 2400, loss is: [0.1579621]\n",
      "epoch: 3, batch: 2600, loss is: [0.6890925]\n",
      "epoch: 3, batch: 2800, loss is: [0.06224689]\n",
      "epoch: 3, batch: 3000, loss is: [0.12175803]\n",
      "epoch: 3, batch: 3200, loss is: [0.055925]\n",
      "epoch: 3, batch: 3400, loss is: [0.18833198]\n",
      "epoch: 3, batch: 3600, loss is: [0.22020932]\n",
      "[validation]After epoch 3: accuracy/loss: 0.9610000252723694/0.13000799715518951\n",
      "epoch: 4, batch: 0, loss is: [0.10910551]\n",
      "epoch: 4, batch: 200, loss is: [0.23725396]\n",
      "epoch: 4, batch: 400, loss is: [0.08820383]\n",
      "epoch: 4, batch: 600, loss is: [0.07089349]\n",
      "epoch: 4, batch: 800, loss is: [0.04771547]\n",
      "epoch: 4, batch: 1000, loss is: [0.11563114]\n",
      "epoch: 4, batch: 1200, loss is: [0.21869035]\n",
      "epoch: 4, batch: 1400, loss is: [0.04117815]\n",
      "epoch: 4, batch: 1600, loss is: [0.01238732]\n",
      "epoch: 4, batch: 1800, loss is: [0.7434577]\n",
      "epoch: 4, batch: 2000, loss is: [0.07520805]\n",
      "epoch: 4, batch: 2200, loss is: [0.10622524]\n",
      "epoch: 4, batch: 2400, loss is: [0.07167714]\n",
      "epoch: 4, batch: 2600, loss is: [0.13450137]\n",
      "epoch: 4, batch: 2800, loss is: [0.0347844]\n",
      "epoch: 4, batch: 3000, loss is: [0.22061351]\n",
      "epoch: 4, batch: 3200, loss is: [0.3314528]\n",
      "epoch: 4, batch: 3400, loss is: [0.08649135]\n",
      "epoch: 4, batch: 3600, loss is: [0.02879941]\n",
      "[validation]After epoch 4: accuracy/loss: 0.96670001745224/0.11276546865701675\n",
      "epoch: 5, batch: 0, loss is: [0.19526841]\n",
      "epoch: 5, batch: 200, loss is: [0.03694985]\n",
      "epoch: 5, batch: 400, loss is: [0.14896889]\n",
      "epoch: 5, batch: 600, loss is: [0.05132245]\n",
      "epoch: 5, batch: 800, loss is: [0.04005899]\n",
      "epoch: 5, batch: 1000, loss is: [0.13996257]\n",
      "epoch: 5, batch: 1200, loss is: [0.0066003]\n",
      "epoch: 5, batch: 1400, loss is: [0.22487946]\n",
      "epoch: 5, batch: 1600, loss is: [0.09425925]\n",
      "epoch: 5, batch: 1800, loss is: [0.40322924]\n",
      "epoch: 5, batch: 2000, loss is: [0.10426824]\n",
      "epoch: 5, batch: 2200, loss is: [0.01998483]\n",
      "epoch: 5, batch: 2400, loss is: [0.17023058]\n",
      "epoch: 5, batch: 2600, loss is: [0.23945248]\n",
      "epoch: 5, batch: 2800, loss is: [0.01200881]\n",
      "epoch: 5, batch: 3000, loss is: [0.08995682]\n",
      "epoch: 5, batch: 3200, loss is: [0.10810863]\n",
      "epoch: 5, batch: 3400, loss is: [0.08244139]\n",
      "epoch: 5, batch: 3600, loss is: [0.02963408]\n",
      "[validation]After epoch 5: accuracy/loss: 0.9678999781608582/0.1036754846572876\n",
      "epoch: 6, batch: 0, loss is: [0.16709052]\n",
      "epoch: 6, batch: 200, loss is: [0.04048613]\n",
      "epoch: 6, batch: 400, loss is: [0.01891204]\n",
      "epoch: 6, batch: 600, loss is: [0.2778991]\n",
      "epoch: 6, batch: 800, loss is: [0.07364982]\n",
      "epoch: 6, batch: 1000, loss is: [0.087122]\n",
      "epoch: 6, batch: 1200, loss is: [0.14899199]\n",
      "epoch: 6, batch: 1400, loss is: [0.01872667]\n",
      "epoch: 6, batch: 1600, loss is: [0.14380908]\n",
      "epoch: 6, batch: 1800, loss is: [0.25131595]\n",
      "epoch: 6, batch: 2000, loss is: [0.0456029]\n",
      "epoch: 6, batch: 2200, loss is: [0.47817525]\n",
      "epoch: 6, batch: 2400, loss is: [0.06377736]\n",
      "epoch: 6, batch: 2600, loss is: [0.10049957]\n",
      "epoch: 6, batch: 2800, loss is: [0.04925292]\n",
      "epoch: 6, batch: 3000, loss is: [0.08959974]\n",
      "epoch: 6, batch: 3200, loss is: [0.02849579]\n",
      "epoch: 6, batch: 3400, loss is: [0.21384427]\n",
      "epoch: 6, batch: 3600, loss is: [0.04615399]\n",
      "[validation]After epoch 6: accuracy/loss: 0.9728000164031982/0.09313233941793442\n",
      "epoch: 7, batch: 0, loss is: [0.06036251]\n",
      "epoch: 7, batch: 200, loss is: [0.0088806]\n",
      "epoch: 7, batch: 400, loss is: [0.04554463]\n",
      "epoch: 7, batch: 600, loss is: [0.02970138]\n",
      "epoch: 7, batch: 800, loss is: [0.07503229]\n",
      "epoch: 7, batch: 1000, loss is: [0.42531383]\n",
      "epoch: 7, batch: 1200, loss is: [0.05688139]\n",
      "epoch: 7, batch: 1400, loss is: [0.08351619]\n",
      "epoch: 7, batch: 1600, loss is: [0.17962857]\n",
      "epoch: 7, batch: 1800, loss is: [0.11115222]\n",
      "epoch: 7, batch: 2000, loss is: [0.16248544]\n",
      "epoch: 7, batch: 2200, loss is: [0.03573458]\n",
      "epoch: 7, batch: 2400, loss is: [0.03525982]\n",
      "epoch: 7, batch: 2600, loss is: [0.07757434]\n",
      "epoch: 7, batch: 2800, loss is: [0.00967456]\n",
      "epoch: 7, batch: 3000, loss is: [0.02508735]\n",
      "epoch: 7, batch: 3200, loss is: [0.01228968]\n",
      "epoch: 7, batch: 3400, loss is: [0.0118267]\n",
      "epoch: 7, batch: 3600, loss is: [0.08522764]\n",
      "[validation]After epoch 7: accuracy/loss: 0.9743000268936157/0.08740811049938202\n",
      "epoch: 8, batch: 0, loss is: [0.00513723]\n",
      "epoch: 8, batch: 200, loss is: [0.02030907]\n",
      "epoch: 8, batch: 400, loss is: [0.02473801]\n",
      "epoch: 8, batch: 600, loss is: [0.12404788]\n",
      "epoch: 8, batch: 800, loss is: [0.14890853]\n",
      "epoch: 8, batch: 1000, loss is: [0.01409968]\n",
      "epoch: 8, batch: 1200, loss is: [0.01927419]\n",
      "epoch: 8, batch: 1400, loss is: [0.02971767]\n",
      "epoch: 8, batch: 1600, loss is: [0.01147169]\n",
      "epoch: 8, batch: 1800, loss is: [0.04683056]\n",
      "epoch: 8, batch: 2000, loss is: [0.45538688]\n",
      "epoch: 8, batch: 2200, loss is: [0.02973624]\n",
      "epoch: 8, batch: 2400, loss is: [0.00455463]\n",
      "epoch: 8, batch: 2600, loss is: [0.04498388]\n",
      "epoch: 8, batch: 2800, loss is: [0.00392637]\n",
      "epoch: 8, batch: 3000, loss is: [0.26791507]\n",
      "epoch: 8, batch: 3200, loss is: [0.02657811]\n",
      "epoch: 8, batch: 3400, loss is: [0.2637381]\n",
      "epoch: 8, batch: 3600, loss is: [0.05394607]\n",
      "[validation]After epoch 8: accuracy/loss: 0.9746999740600586/0.08181799203157425\n",
      "epoch: 9, batch: 0, loss is: [0.01983742]\n",
      "epoch: 9, batch: 200, loss is: [0.07943439]\n",
      "epoch: 9, batch: 400, loss is: [0.0265709]\n",
      "epoch: 9, batch: 600, loss is: [0.01709994]\n",
      "epoch: 9, batch: 800, loss is: [0.01746874]\n",
      "epoch: 9, batch: 1000, loss is: [0.27513]\n",
      "epoch: 9, batch: 1200, loss is: [0.20003495]\n",
      "epoch: 9, batch: 1400, loss is: [0.09203206]\n",
      "epoch: 9, batch: 1600, loss is: [0.03815668]\n",
      "epoch: 9, batch: 1800, loss is: [0.03843121]\n",
      "epoch: 9, batch: 2000, loss is: [0.01983667]\n",
      "epoch: 9, batch: 2200, loss is: [0.16358191]\n",
      "epoch: 9, batch: 2400, loss is: [0.02334835]\n",
      "epoch: 9, batch: 2600, loss is: [0.04909118]\n",
      "epoch: 9, batch: 2800, loss is: [0.00410498]\n",
      "epoch: 9, batch: 3000, loss is: [0.07865061]\n",
      "epoch: 9, batch: 3200, loss is: [0.11022164]\n",
      "epoch: 9, batch: 3400, loss is: [0.0039573]\n",
      "epoch: 9, batch: 3600, loss is: [0.21168862]\n",
      "[validation]After epoch 9: accuracy/loss: 0.974399983882904/0.08030121773481369\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#创建模型\n",
    "model = MNIST()\n",
    "\n",
    "\n",
    "#设置优化器\n",
    "opt = paddle.optimizer.SGD(learning_rate=0.001, parameters=model.parameters())\n",
    "EPOCH_NUM = 10\n",
    "for epoch_id in range(EPOCH_NUM):\n",
    "    model.train()\n",
    "    for batch_id, data in enumerate(train_loader()):\n",
    "        #准备数据\n",
    "        images, labels = data\n",
    "\n",
    "        #前向计算的过程\n",
    "        predicts = model(images)\n",
    "\n",
    "        #计算损失，取一个批次样本损失的平均值\n",
    "        loss = F.cross_entropy(predicts, labels)\n",
    "        avg_loss = paddle.mean(loss)\n",
    "\n",
    "        #每训练了 100 批次的数据，打印下当前 Loss 的情况\n",
    "        if batch_id % 200 == 0:\n",
    "            print(\"epoch: {}, batch: {}, loss is: {}\".format(epoch_id, batch_id, avg_loss.numpy()))\n",
    "\n",
    "        #后向传播，更新参数的过程\n",
    "        avg_loss.backward()\n",
    "        # 最小化 loss,更新参数\n",
    "        opt.step()\n",
    "        # 清除梯度\n",
    "        opt.clear_grad()\n",
    "\n",
    "    # evaluate model after one epoch\n",
    "    model.eval()\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    for batch_id, data in enumerate(test_loader):\n",
    "        #准备数据\n",
    "        images, labels = data\n",
    "        #前向计算的过程\n",
    "        predicts = model(images)\n",
    "        #计算损失\n",
    "        loss = F.cross_entropy(predicts, labels)\n",
    "        #计算准确率\n",
    "        acc = paddle.metric.accuracy(predicts, labels)\n",
    "        accuracies.append(acc.numpy())\n",
    "        losses.append(loss.numpy())\n",
    "\n",
    "    avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)\n",
    "    print(\"[validation]After epoch {}: accuracy/loss: {}/{}\".format(epoch_id, avg_acc, avg_loss))\n",
    "\n",
    "#保存模型参数\n",
    "paddle.save(model.state_dict(), 'mnist.pdparams')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# 查看 9 张输入的训练图像的样例\n",
    "dataset = MNISTDataset(mode='train')\n",
    "image_matrix = []\n",
    "for i in range(9):\n",
    "    image, label = dataset[i]\n",
    "    # 将 dataset 中的 CHW 排列的图像转换成 HWC 排列再写入 VisualDL\n",
    "    image_matrix.append(image.transpose([1,2,0]))\n",
    "# 将九张输入图像合成长宽相同的图像网格，即 3X3 的图像网格\n",
    "logwriter.add_image_matrix(tag='input_images', step=1, imgs=image_matrix, rows=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
