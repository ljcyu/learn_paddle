{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa3fece-d9fb-4797-abde-4a72463bde45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-21T13:45:19.053785Z",
     "iopub.status.busy": "2022-11-21T13:45:19.052769Z",
     "iopub.status.idle": "2022-11-21T13:45:50.503205Z",
     "shell.execute_reply": "2022-11-21T13:45:50.502312Z",
     "shell.execute_reply.started": "2022-11-21T13:45:19.053736Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1121 21:45:24.021994  9785 gpu_context.cc:244] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1\r\n",
      "W1121 21:45:24.026597  9785 gpu_context.cc:272] device: 0, cuDNN Version: 7.6.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\r\n",
      "Epoch 1/1\r\n",
      "step  10/938 - loss: 1.7369 - acc: 0.2219 - 27ms/step\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aistudio/.data/webide/pip/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\r\n",
      "  return (isinstance(seq, collections.Sequence) and\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  20/938 - loss: 0.8967 - acc: 0.4133 - 19ms/step\r\n",
      "step  30/938 - loss: 0.6190 - acc: 0.5234 - 16ms/step\r\n",
      "step  40/938 - loss: 0.5551 - acc: 0.6008 - 14ms/step\r\n",
      "step  50/938 - loss: 0.6793 - acc: 0.6491 - 13ms/step\r\n",
      "step  60/938 - loss: 0.2059 - acc: 0.6888 - 13ms/step\r\n",
      "step  70/938 - loss: 0.3903 - acc: 0.7181 - 12ms/step\r\n",
      "step  80/938 - loss: 0.3281 - acc: 0.7438 - 12ms/step\r\n",
      "step  90/938 - loss: 0.1673 - acc: 0.7644 - 12ms/step\r\n",
      "step 100/938 - loss: 0.1173 - acc: 0.7820 - 12ms/step\r\n",
      "step 110/938 - loss: 0.2528 - acc: 0.7932 - 12ms/step\r\n",
      "step 120/938 - loss: 0.1987 - acc: 0.8047 - 11ms/step\r\n",
      "step 130/938 - loss: 0.3252 - acc: 0.8139 - 11ms/step\r\n",
      "step 140/938 - loss: 0.1127 - acc: 0.8232 - 11ms/step\r\n",
      "step 150/938 - loss: 0.3554 - acc: 0.8303 - 11ms/step\r\n",
      "step 160/938 - loss: 0.2799 - acc: 0.8378 - 11ms/step\r\n",
      "step 170/938 - loss: 0.2000 - acc: 0.8439 - 11ms/step\r\n",
      "step 180/938 - loss: 0.1568 - acc: 0.8496 - 11ms/step\r\n",
      "step 190/938 - loss: 0.4939 - acc: 0.8543 - 11ms/step\r\n",
      "step 200/938 - loss: 0.4846 - acc: 0.8582 - 11ms/step\r\n",
      "step 210/938 - loss: 0.1717 - acc: 0.8621 - 11ms/step\r\n",
      "step 220/938 - loss: 0.0622 - acc: 0.8652 - 11ms/step\r\n",
      "step 230/938 - loss: 0.1378 - acc: 0.8695 - 11ms/step\r\n",
      "step 240/938 - loss: 0.2428 - acc: 0.8720 - 11ms/step\r\n",
      "step 250/938 - loss: 0.0644 - acc: 0.8752 - 11ms/step\r\n",
      "step 260/938 - loss: 0.0889 - acc: 0.8789 - 11ms/step\r\n",
      "step 270/938 - loss: 0.1728 - acc: 0.8814 - 11ms/step\r\n",
      "step 280/938 - loss: 0.1958 - acc: 0.8839 - 11ms/step\r\n",
      "step 290/938 - loss: 0.2576 - acc: 0.8860 - 11ms/step\r\n",
      "step 300/938 - loss: 0.2276 - acc: 0.8877 - 11ms/step\r\n",
      "step 310/938 - loss: 0.1164 - acc: 0.8897 - 11ms/step\r\n",
      "step 320/938 - loss: 0.0529 - acc: 0.8921 - 11ms/step\r\n",
      "step 330/938 - loss: 0.2309 - acc: 0.8940 - 11ms/step\r\n",
      "step 340/938 - loss: 0.1888 - acc: 0.8958 - 11ms/step\r\n",
      "step 350/938 - loss: 0.0992 - acc: 0.8975 - 11ms/step\r\n",
      "step 360/938 - loss: 0.1819 - acc: 0.8992 - 11ms/step\r\n",
      "step 370/938 - loss: 0.1883 - acc: 0.9011 - 11ms/step\r\n",
      "step 380/938 - loss: 0.1902 - acc: 0.9028 - 11ms/step\r\n",
      "step 390/938 - loss: 0.1399 - acc: 0.9042 - 11ms/step\r\n",
      "step 400/938 - loss: 0.0480 - acc: 0.9054 - 11ms/step\r\n",
      "step 410/938 - loss: 0.0677 - acc: 0.9069 - 11ms/step\r\n",
      "step 420/938 - loss: 0.0814 - acc: 0.9082 - 11ms/step\r\n",
      "step 430/938 - loss: 0.0712 - acc: 0.9095 - 11ms/step\r\n",
      "step 440/938 - loss: 0.2565 - acc: 0.9106 - 11ms/step\r\n",
      "step 450/938 - loss: 0.2235 - acc: 0.9118 - 11ms/step\r\n",
      "step 460/938 - loss: 0.0713 - acc: 0.9128 - 11ms/step\r\n",
      "step 470/938 - loss: 0.1968 - acc: 0.9139 - 11ms/step\r\n",
      "step 480/938 - loss: 0.2098 - acc: 0.9148 - 11ms/step\r\n",
      "step 490/938 - loss: 0.0993 - acc: 0.9158 - 11ms/step\r\n",
      "step 500/938 - loss: 0.1057 - acc: 0.9165 - 11ms/step\r\n",
      "step 510/938 - loss: 0.0057 - acc: 0.9176 - 11ms/step\r\n",
      "step 520/938 - loss: 0.0468 - acc: 0.9183 - 11ms/step\r\n",
      "step 530/938 - loss: 0.0562 - acc: 0.9191 - 11ms/step\r\n",
      "step 540/938 - loss: 0.1034 - acc: 0.9197 - 11ms/step\r\n",
      "step 550/938 - loss: 0.1203 - acc: 0.9207 - 11ms/step\r\n",
      "step 560/938 - loss: 0.1817 - acc: 0.9212 - 11ms/step\r\n",
      "step 570/938 - loss: 0.1041 - acc: 0.9221 - 11ms/step\r\n",
      "step 580/938 - loss: 0.0879 - acc: 0.9228 - 11ms/step\r\n",
      "step 590/938 - loss: 0.0248 - acc: 0.9234 - 11ms/step\r\n",
      "step 600/938 - loss: 0.2146 - acc: 0.9242 - 11ms/step\r\n",
      "step 610/938 - loss: 0.1666 - acc: 0.9250 - 11ms/step\r\n",
      "step 620/938 - loss: 0.1800 - acc: 0.9257 - 11ms/step\r\n",
      "step 630/938 - loss: 0.0276 - acc: 0.9265 - 11ms/step\r\n",
      "step 640/938 - loss: 0.0319 - acc: 0.9272 - 11ms/step\r\n",
      "step 650/938 - loss: 0.1167 - acc: 0.9278 - 11ms/step\r\n",
      "step 660/938 - loss: 0.1935 - acc: 0.9285 - 11ms/step\r\n",
      "step 670/938 - loss: 0.0659 - acc: 0.9291 - 11ms/step\r\n",
      "step 680/938 - loss: 0.0314 - acc: 0.9296 - 11ms/step\r\n",
      "step 690/938 - loss: 0.0203 - acc: 0.9303 - 11ms/step\r\n",
      "step 700/938 - loss: 0.0426 - acc: 0.9308 - 11ms/step\r\n",
      "step 710/938 - loss: 0.1673 - acc: 0.9313 - 11ms/step\r\n",
      "step 720/938 - loss: 0.0504 - acc: 0.9319 - 11ms/step\r\n",
      "step 730/938 - loss: 0.0565 - acc: 0.9324 - 11ms/step\r\n",
      "step 740/938 - loss: 0.0475 - acc: 0.9330 - 11ms/step\r\n",
      "step 750/938 - loss: 0.0546 - acc: 0.9336 - 11ms/step\r\n",
      "step 760/938 - loss: 0.0362 - acc: 0.9342 - 11ms/step\r\n",
      "step 770/938 - loss: 0.1486 - acc: 0.9346 - 11ms/step\r\n",
      "step 780/938 - loss: 0.2748 - acc: 0.9351 - 11ms/step\r\n",
      "step 790/938 - loss: 0.0396 - acc: 0.9355 - 11ms/step\r\n",
      "step 800/938 - loss: 0.0310 - acc: 0.9360 - 11ms/step\r\n",
      "step 810/938 - loss: 0.1830 - acc: 0.9363 - 11ms/step\r\n",
      "step 820/938 - loss: 0.0612 - acc: 0.9367 - 11ms/step\r\n",
      "step 830/938 - loss: 0.0392 - acc: 0.9373 - 11ms/step\r\n",
      "step 840/938 - loss: 0.0352 - acc: 0.9377 - 10ms/step\r\n",
      "step 850/938 - loss: 0.0593 - acc: 0.9382 - 10ms/step\r\n",
      "step 860/938 - loss: 0.0212 - acc: 0.9386 - 10ms/step\r\n",
      "step 870/938 - loss: 0.0733 - acc: 0.9391 - 10ms/step\r\n",
      "step 880/938 - loss: 0.0699 - acc: 0.9394 - 10ms/step\r\n",
      "step 890/938 - loss: 0.0438 - acc: 0.9399 - 10ms/step\r\n",
      "step 900/938 - loss: 0.1551 - acc: 0.9402 - 10ms/step\r\n",
      "step 910/938 - loss: 0.1034 - acc: 0.9405 - 10ms/step\r\n",
      "step 920/938 - loss: 0.1185 - acc: 0.9409 - 10ms/step\r\n",
      "step 930/938 - loss: 0.2537 - acc: 0.9411 - 10ms/step\r\n",
      "step 938/938 - loss: 0.1006 - acc: 0.9414 - 10ms/step\r\n",
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\r\n",
      "Epoch 1/1\r\n",
      "step  10/938 - loss: 1.8012 - acc: 0.2047 - 14ms/step\r\n",
      "step  20/938 - loss: 1.0087 - acc: 0.3609 - 12ms/step\r\n",
      "step  30/938 - loss: 0.7741 - acc: 0.4703 - 11ms/step\r\n",
      "step  40/938 - loss: 0.6780 - acc: 0.5559 - 11ms/step\r\n",
      "step  50/938 - loss: 0.6778 - acc: 0.6081 - 11ms/step\r\n",
      "step  60/938 - loss: 0.2727 - acc: 0.6508 - 11ms/step\r\n",
      "step  70/938 - loss: 0.6573 - acc: 0.6826 - 11ms/step\r\n",
      "step  80/938 - loss: 0.3625 - acc: 0.7109 - 10ms/step\r\n",
      "step  90/938 - loss: 0.2996 - acc: 0.7325 - 10ms/step\r\n",
      "step 100/938 - loss: 0.1436 - acc: 0.7511 - 10ms/step\r\n",
      "step 110/938 - loss: 0.3551 - acc: 0.7645 - 10ms/step\r\n",
      "step 120/938 - loss: 0.1518 - acc: 0.7768 - 10ms/step\r\n",
      "step 130/938 - loss: 0.3621 - acc: 0.7870 - 10ms/step\r\n",
      "step 140/938 - loss: 0.1170 - acc: 0.7972 - 10ms/step\r\n",
      "step 150/938 - loss: 0.2952 - acc: 0.8050 - 10ms/step\r\n",
      "step 160/938 - loss: 0.2439 - acc: 0.8123 - 10ms/step\r\n",
      "step 170/938 - loss: 0.2527 - acc: 0.8182 - 10ms/step\r\n",
      "step 180/938 - loss: 0.1641 - acc: 0.8242 - 10ms/step\r\n",
      "step 190/938 - loss: 0.6127 - acc: 0.8294 - 10ms/step\r\n",
      "step 200/938 - loss: 0.3932 - acc: 0.8334 - 10ms/step\r\n",
      "step 210/938 - loss: 0.1768 - acc: 0.8374 - 10ms/step\r\n",
      "step 220/938 - loss: 0.1164 - acc: 0.8418 - 10ms/step\r\n",
      "step 230/938 - loss: 0.2311 - acc: 0.8463 - 10ms/step\r\n",
      "step 240/938 - loss: 0.2105 - acc: 0.8497 - 10ms/step\r\n",
      "step 250/938 - loss: 0.1111 - acc: 0.8537 - 10ms/step\r\n",
      "step 260/938 - loss: 0.1387 - acc: 0.8578 - 10ms/step\r\n",
      "step 270/938 - loss: 0.1684 - acc: 0.8610 - 10ms/step\r\n",
      "step 280/938 - loss: 0.1829 - acc: 0.8640 - 10ms/step\r\n",
      "step 290/938 - loss: 0.2058 - acc: 0.8664 - 10ms/step\r\n",
      "step 300/938 - loss: 0.1772 - acc: 0.8691 - 10ms/step\r\n",
      "step 310/938 - loss: 0.1014 - acc: 0.8718 - 10ms/step\r\n",
      "step 320/938 - loss: 0.1167 - acc: 0.8744 - 10ms/step\r\n",
      "step 330/938 - loss: 0.1620 - acc: 0.8768 - 10ms/step\r\n",
      "step 340/938 - loss: 0.2678 - acc: 0.8789 - 10ms/step\r\n",
      "step 350/938 - loss: 0.1183 - acc: 0.8804 - 10ms/step\r\n",
      "step 360/938 - loss: 0.2575 - acc: 0.8826 - 10ms/step\r\n",
      "step 370/938 - loss: 0.2825 - acc: 0.8848 - 10ms/step\r\n",
      "step 380/938 - loss: 0.1441 - acc: 0.8866 - 10ms/step\r\n",
      "step 390/938 - loss: 0.1957 - acc: 0.8879 - 10ms/step\r\n",
      "step 400/938 - loss: 0.1019 - acc: 0.8896 - 10ms/step\r\n",
      "step 410/938 - loss: 0.1084 - acc: 0.8912 - 10ms/step\r\n",
      "step 420/938 - loss: 0.1112 - acc: 0.8926 - 10ms/step\r\n",
      "step 430/938 - loss: 0.0932 - acc: 0.8941 - 10ms/step\r\n",
      "step 440/938 - loss: 0.1600 - acc: 0.8952 - 10ms/step\r\n",
      "step 450/938 - loss: 0.2310 - acc: 0.8967 - 10ms/step\r\n",
      "step 460/938 - loss: 0.1248 - acc: 0.8982 - 10ms/step\r\n",
      "step 470/938 - loss: 0.2650 - acc: 0.8997 - 10ms/step\r\n",
      "step 480/938 - loss: 0.1013 - acc: 0.9011 - 10ms/step\r\n",
      "step 490/938 - loss: 0.1417 - acc: 0.9022 - 10ms/step\r\n",
      "step 500/938 - loss: 0.1054 - acc: 0.9036 - 10ms/step\r\n",
      "step 510/938 - loss: 0.0277 - acc: 0.9047 - 10ms/step\r\n",
      "step 520/938 - loss: 0.0191 - acc: 0.9060 - 10ms/step\r\n",
      "step 530/938 - loss: 0.0383 - acc: 0.9069 - 10ms/step\r\n",
      "step 540/938 - loss: 0.0890 - acc: 0.9079 - 10ms/step\r\n",
      "step 550/938 - loss: 0.0825 - acc: 0.9089 - 10ms/step\r\n",
      "step 560/938 - loss: 0.2859 - acc: 0.9097 - 10ms/step\r\n",
      "step 570/938 - loss: 0.1268 - acc: 0.9107 - 10ms/step\r\n",
      "step 580/938 - loss: 0.0447 - acc: 0.9115 - 10ms/step\r\n",
      "step 590/938 - loss: 0.0462 - acc: 0.9123 - 10ms/step\r\n",
      "step 600/938 - loss: 0.1902 - acc: 0.9133 - 10ms/step\r\n",
      "step 610/938 - loss: 0.1019 - acc: 0.9142 - 10ms/step\r\n",
      "step 620/938 - loss: 0.1567 - acc: 0.9150 - 10ms/step\r\n",
      "step 630/938 - loss: 0.0427 - acc: 0.9158 - 10ms/step\r\n",
      "step 640/938 - loss: 0.0481 - acc: 0.9167 - 10ms/step\r\n",
      "step 650/938 - loss: 0.1309 - acc: 0.9172 - 10ms/step\r\n",
      "step 660/938 - loss: 0.1630 - acc: 0.9180 - 10ms/step\r\n",
      "step 670/938 - loss: 0.1261 - acc: 0.9186 - 10ms/step\r\n",
      "step 680/938 - loss: 0.0541 - acc: 0.9193 - 10ms/step\r\n",
      "step 690/938 - loss: 0.0512 - acc: 0.9199 - 10ms/step\r\n",
      "step 700/938 - loss: 0.0287 - acc: 0.9204 - 10ms/step\r\n",
      "step 710/938 - loss: 0.1279 - acc: 0.9210 - 10ms/step\r\n",
      "step 720/938 - loss: 0.0623 - acc: 0.9218 - 10ms/step\r\n",
      "step 730/938 - loss: 0.0389 - acc: 0.9225 - 10ms/step\r\n",
      "step 740/938 - loss: 0.0710 - acc: 0.9231 - 10ms/step\r\n",
      "step 750/938 - loss: 0.0684 - acc: 0.9239 - 10ms/step\r\n",
      "step 760/938 - loss: 0.0175 - acc: 0.9244 - 10ms/step\r\n",
      "step 770/938 - loss: 0.1295 - acc: 0.9249 - 10ms/step\r\n",
      "step 780/938 - loss: 0.2478 - acc: 0.9253 - 10ms/step\r\n",
      "step 790/938 - loss: 0.0599 - acc: 0.9257 - 10ms/step\r\n",
      "step 800/938 - loss: 0.0719 - acc: 0.9262 - 10ms/step\r\n",
      "step 810/938 - loss: 0.1118 - acc: 0.9266 - 10ms/step\r\n",
      "step 820/938 - loss: 0.0160 - acc: 0.9272 - 10ms/step\r\n",
      "step 830/938 - loss: 0.0469 - acc: 0.9277 - 10ms/step\r\n",
      "step 840/938 - loss: 0.0859 - acc: 0.9281 - 10ms/step\r\n",
      "step 850/938 - loss: 0.0434 - acc: 0.9287 - 10ms/step\r\n",
      "step 860/938 - loss: 0.0199 - acc: 0.9292 - 10ms/step\r\n",
      "step 870/938 - loss: 0.0265 - acc: 0.9297 - 10ms/step\r\n",
      "step 880/938 - loss: 0.0702 - acc: 0.9300 - 10ms/step\r\n",
      "step 890/938 - loss: 0.0749 - acc: 0.9304 - 10ms/step\r\n",
      "step 900/938 - loss: 0.1639 - acc: 0.9309 - 10ms/step\r\n",
      "step 910/938 - loss: 0.0713 - acc: 0.9312 - 10ms/step\r\n",
      "step 920/938 - loss: 0.1261 - acc: 0.9316 - 10ms/step\r\n",
      "step 930/938 - loss: 0.1248 - acc: 0.9321 - 10ms/step\r\n",
      "step 938/938 - loss: 0.0337 - acc: 0.9324 - 10ms/step\r\n"
     ]
    }
   ],
   "source": [
    "import paddle\r\n",
    "import paddle.vision.transforms as T\r\n",
    "from paddle.static import InputSpec\r\n",
    "\r\n",
    "inputs = [InputSpec([-1, 1, 28, 28], 'float32', 'image')]\r\n",
    "labels = [InputSpec([None, 1], 'int64', 'label')]\r\n",
    "\r\n",
    "transform = T.Compose([\r\n",
    "    T.Transpose(),\r\n",
    "    T.Normalize([127.5], [127.5])\r\n",
    "])\r\n",
    "train_dataset = paddle.vision.datasets.MNIST(mode='train', transform=transform)\r\n",
    "\r\n",
    "lenet = paddle.vision.LeNet()\r\n",
    "model = paddle.Model(lenet,inputs, labels)\r\n",
    "\r\n",
    "optim = paddle.optimizer.Adam(0.001, parameters=lenet.parameters())\r\n",
    "model.prepare(optimizer=optim,loss=paddle.nn.CrossEntropyLoss(),metrics=paddle.metric.Accuracy())\r\n",
    "\r\n",
    "callback = paddle.callbacks.ProgBarLogger(log_freq=10)\r\n",
    "model.fit(train_dataset, batch_size=64, callbacks=callback)\r\n",
    "\r\n",
    "\r\n",
    "import paddle\r\n",
    "import paddle.vision.transforms as T\r\n",
    "from paddle.vision.datasets import MNIST\r\n",
    "from paddle.static import InputSpec\r\n",
    "\r\n",
    "inputs = [InputSpec([-1, 1, 28, 28], 'float32', 'image')]\r\n",
    "labels = [InputSpec([None, 1], 'int64', 'label')]\r\n",
    "\r\n",
    "transform = T.Compose([\r\n",
    "    T.Transpose(),\r\n",
    "    T.Normalize([127.5], [127.5])\r\n",
    "])\r\n",
    "train_dataset = MNIST(mode='train', transform=transform)\r\n",
    "\r\n",
    "lenet = paddle.vision.LeNet()\r\n",
    "model = paddle.Model(lenet,inputs, labels)\r\n",
    "\r\n",
    "optim = paddle.optimizer.Adam(0.001, parameters=lenet.parameters())\r\n",
    "model.prepare(optimizer=optim,loss=paddle.nn.CrossEntropyLoss(),metrics=paddle.metric.Accuracy())\r\n",
    "\r\n",
    "callback = paddle.callbacks.ProgBarLogger(log_freq=10)\r\n",
    "model.fit(train_dataset, batch_size=64, callbacks=callback)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84db29f9-5714-4946-9a73-a33b3497888c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
