{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **任务描述：**\n",
    "\n",
    "### 本次实践是一个多分类任务，需要将照片中的宝石分别进行识别，完成**宝石的识别**\n",
    "\n",
    "### **实践平台：百度AI实训平台-AI Studio、PaddlePaddle2.0.0 动态图**\n",
    "\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/7640d4434f894f5dbae1d85c62e54b8476e24856aabc4c5f9915fea1b26f3ebc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度神经网络(DNN)\n",
    "\n",
    "### **深度神经网络（Deep Neural Networks，简称DNN）是深度学习的基础，其结构为input、hidden（可有多层）、output，每层均为全连接。**\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/c60fc28848cf469fa3a7824aa637a03f3b2b213ce7b84659919cb24b4430bffb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集介绍\n",
    "\n",
    "* **数据集文件名为archive_train.zip,archive_test.zip。**\n",
    "\n",
    "* **该数据集包含25个类别不同宝石的图像。**\n",
    "\n",
    "* **这些类别已经分为训练和测试数据。**\n",
    "\n",
    "* **图像大小不一，格式为.jpeg。**\n",
    "\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/7640d4434f894f5dbae1d85c62e54b8476e24856aabc4c5f9915fea1b26f3ebc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-11-18T12:51:45.826481Z",
     "iopub.status.busy": "2022-11-18T12:51:45.825560Z",
     "iopub.status.idle": "2022-11-18T12:51:46.082433Z",
     "shell.execute_reply": "2022-11-18T12:51:46.081015Z",
     "shell.execute_reply.started": "2022-11-18T12:51:45.826438Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data55032  dataset\r\n"
     ]
    }
   ],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-11-18T12:51:46.085082Z",
     "iopub.status.busy": "2022-11-18T12:51:46.084764Z",
     "iopub.status.idle": "2022-11-18T12:51:46.096309Z",
     "shell.execute_reply": "2022-11-18T12:51:46.095452Z",
     "shell.execute_reply.started": "2022-11-18T12:51:46.085042Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#导入需要的包\n",
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import paddle\n",
    "import matplotlib.pyplot as plt\n",
    "from paddle.io import Dataset\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-11-18T12:51:46.097712Z",
     "iopub.status.busy": "2022-11-18T12:51:46.097420Z",
     "iopub.status.idle": "2022-11-18T12:51:46.103201Z",
     "shell.execute_reply": "2022-11-18T12:51:46.102419Z",
     "shell.execute_reply.started": "2022-11-18T12:51:46.097686Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "参数配置\n",
    "'''\n",
    "train_parameters = {\n",
    "    \"input_size\": [3, 224, 224],                           #输入图片的shape\n",
    "    \"class_dim\": -1,                                     #分类数\n",
    "    \"src_path\":\"data/data55032/archive_train.zip\",       #原始数据集路径\n",
    "    \"target_path\":\"/home/aistudio/data/dataset\",        #要解压的路径 \n",
    "    \"train_list_path\": \"./train.txt\",              #train_data.txt路径\n",
    "    \"eval_list_path\": \"./eval.txt\",                  #eval_data.txt路径\n",
    "    \"label_dict\":{},                                    #标签字典\n",
    "    \"readme_path\": \"/home/aistudio/data/readme.json\",   #readme.json路径\n",
    "    \"num_epochs\": 100,                                    #训练轮数\n",
    "    \"train_batch_size\": 96,                             #批次的大小\n",
    "    \"learning_strategy\": {                              #优化函数相关的配置\n",
    "        \"lr\": 0.001                                     #超参数学习率\n",
    "    } \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-11-18T12:51:46.105708Z",
     "iopub.status.busy": "2022-11-18T12:51:46.105196Z",
     "iopub.status.idle": "2022-11-18T12:51:46.111365Z",
     "shell.execute_reply": "2022-11-18T12:51:46.110567Z",
     "shell.execute_reply.started": "2022-11-18T12:51:46.105681Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def unzip_data(src_path,target_path):\n",
    "\n",
    "    '''\n",
    "    解压原始数据集，将src_path路径下的zip包解压至data/dataset目录下\n",
    "    '''\n",
    "\n",
    "    if(not os.path.isdir(target_path)):    \n",
    "        z = zipfile.ZipFile(src_path, 'r')\n",
    "        z.extractall(path=target_path)\n",
    "        z.close()\n",
    "    else:\n",
    "        print(\"文件已解压\")\n",
    "    for _dir in Path(target_path).glob('**/__MACOSX'):\n",
    "        shutil.rmtree(_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-11-18T12:51:46.122677Z",
     "iopub.status.busy": "2022-11-18T12:51:46.122440Z",
     "iopub.status.idle": "2022-11-18T12:51:46.134494Z",
     "shell.execute_reply": "2022-11-18T12:51:46.133730Z",
     "shell.execute_reply.started": "2022-11-18T12:51:46.122656Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件已解压\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'   \\n#生成数据列表   \\nget_data_list(target_path,train_list_path,eval_list_path)\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "参数初始化\n",
    "'''\n",
    "src_path=train_parameters['src_path']\n",
    "target_path=train_parameters['target_path']\n",
    "train_list_path=train_parameters['train_list_path']\n",
    "eval_list_path=train_parameters['eval_list_path']\n",
    "batch_size=train_parameters['train_batch_size']\n",
    "'''\n",
    "解压原始数据到指定路径\n",
    "'''\n",
    "unzip_data(src_path,target_path)\n",
    "\n",
    "'''\n",
    "划分训练集与验证集，乱序，生成数据列表\n",
    "'''\n",
    "#每次生成数据列表前，首先清空train.txt和eval.txt\n",
    "\n",
    "'''   \n",
    "#生成数据列表   \n",
    "get_data_list(target_path,train_list_path,eval_list_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-18T12:51:46.135948Z",
     "iopub.status.busy": "2022-11-18T12:51:46.135605Z",
     "iopub.status.idle": "2022-11-18T12:51:46.142503Z",
     "shell.execute_reply": "2022-11-18T12:51:46.141688Z",
     "shell.execute_reply.started": "2022-11-18T12:51:46.135925Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gen_train_eval():\n",
    "    from pathlib import Path\n",
    "    targetPath = Path(target_path)\n",
    "    class_dirs = sorted(targetPath.glob(\"*\"))\n",
    "    print(class_dirs)\n",
    "    train_parameters['class_dim']=len(class_dirs)\n",
    "    #获取数据 metadata\n",
    "    lst_data = []\n",
    "    for i, class_dir in enumerate(class_dirs):\n",
    "        lst_path = list(class_dir.glob(\"*.jpg\"))\n",
    "        lst_gemName = [p.parent.name for p in lst_path]\n",
    "        #zip当前路径、名称以及该路径下的宝石类别,\n",
    "        #lst_path、lst_genName都是一个list，所以类别也需要是一个list\n",
    "        lst_data.extend( zip(map(str,lst_path),lst_gemName,[i]*len(lst_path)) )\n",
    "\n",
    "    import pandas as pd\n",
    "    #构建数据 dataframe 并且打乱    \n",
    "    df_data = pd.DataFrame(lst_data,columns=['gem_path','gem_name','lbl']).sample(frac=1,replace=False,random_state=1001)\n",
    "    df_data.head(5)    \n",
    "\n",
    "    #得到标签字典，无重复\n",
    "    dic = df_data[['lbl','gem_name']].drop_duplicates()\n",
    "    dic.head(5)\n",
    "\n",
    "# gen_train_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-18T12:51:46.144006Z",
     "iopub.status.busy": "2022-11-18T12:51:46.143502Z",
     "iopub.status.idle": "2022-11-18T12:51:46.148733Z",
     "shell.execute_reply": "2022-11-18T12:51:46.147974Z",
     "shell.execute_reply.started": "2022-11-18T12:51:46.143982Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "if False:\n",
    "    #构建数据 dataframe 并且打乱    \n",
    "    df_data = pd.DataFrame(lst_data,columns=['gem_path','gem_name','lbl']).sample(frac=1,replace=False,random_state=1001)\n",
    "    df_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-18T12:51:46.149917Z",
     "iopub.status.busy": "2022-11-18T12:51:46.149658Z",
     "iopub.status.idle": "2022-11-18T12:51:46.156471Z",
     "shell.execute_reply": "2022-11-18T12:51:46.155455Z",
     "shell.execute_reply.started": "2022-11-18T12:51:46.149895Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    #得到标签字典，无重复\n",
    "    dic = df_data[['lbl','gem_name']].drop_duplicates()\n",
    "    dic.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-18T12:51:46.167756Z",
     "iopub.status.busy": "2022-11-18T12:51:46.166812Z",
     "iopub.status.idle": "2022-11-18T12:51:46.173915Z",
     "shell.execute_reply": "2022-11-18T12:51:46.172623Z",
     "shell.execute_reply.started": "2022-11-18T12:51:46.167700Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    train_parameters['label_dict'] = {v:int(k) for k,v in dic.to_records(index=False)}\n",
    "    train_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-18T12:51:46.176807Z",
     "iopub.status.busy": "2022-11-18T12:51:46.175803Z",
     "iopub.status.idle": "2022-11-18T12:51:46.182204Z",
     "shell.execute_reply": "2022-11-18T12:51:46.180981Z",
     "shell.execute_reply.started": "2022-11-18T12:51:46.176761Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_data, eval_data = train_test_split(df_data, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-18T12:53:48.898805Z",
     "iopub.status.busy": "2022-11-18T12:53:48.898305Z",
     "iopub.status.idle": "2022-11-18T12:53:48.919907Z",
     "shell.execute_reply": "2022-11-18T12:53:48.918890Z",
     "shell.execute_reply.started": "2022-11-18T12:53:48.898768Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#加载和保存数据\n",
    "train_df_json='train_data.json'\n",
    "eval_df_json='eval_data.json'\n",
    "train_params_path='params.json'\n",
    "def save_params():\n",
    "    train_data.to_json(train_df_json)\n",
    "    eval_data.to_json(eval_df_json)\n",
    "    with open(train_params_path, 'w') as f:\n",
    "        tmp_ = json.dumps(train_parameters)\n",
    "        f.write(tmp_)\n",
    "\n",
    "def load_params():\n",
    "    global train_data\n",
    "    global eval_data\n",
    "    global train_parameters\n",
    "    train_data = pd.read_json(train_df_json)\n",
    "    eval_data = pd.read_json(eval_df_json)\n",
    "    with open(train_params_path, 'r') as f:\n",
    "        tmp_str = f.read()\n",
    "    train_parameters = json.loads(tmp_str)\n",
    "#save_params()\n",
    "load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-18T12:53:51.586510Z",
     "iopub.status.busy": "2022-11-18T12:53:51.585706Z",
     "iopub.status.idle": "2022-11-18T12:53:51.597709Z",
     "shell.execute_reply": "2022-11-18T12:53:51.596700Z",
     "shell.execute_reply.started": "2022-11-18T12:53:51.586466Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== 数据集总体情况:总类别数 25\r\n",
      "== 训练集不同类别的样本数：\r\n",
      "Labradorite      34\r\n",
      "Beryl Golden     33\r\n",
      "Garnet Red       33\r\n",
      "Quartz Beer      32\r\n",
      "Alexandrite      30\r\n",
      "Pearl            30\r\n",
      "Iolite           30\r\n",
      "Sapphire Blue    29\r\n",
      "Carnelian        29\r\n",
      "Tanzanite        29\r\n",
      "Variscite        29\r\n",
      "Fluorite         29\r\n",
      "Emerald          29\r\n",
      "Danburite        29\r\n",
      "Zircon           29\r\n",
      "Hessonite        28\r\n",
      "Cats Eye         28\r\n",
      "Almandine        28\r\n",
      "Rhodochrosite    28\r\n",
      "Kunzite          28\r\n",
      "Jade             28\r\n",
      "Benitoite        28\r\n",
      "Diamond          27\r\n",
      "Malachite        26\r\n",
      "Onyx Black       26\r\n",
      "Name: gem_name, dtype: int64\r\n",
      "== 训练集样本数：729 验证集样本数：82\r\n"
     ]
    }
   ],
   "source": [
    "def info( ):\n",
    "    print(\"== 数据集总体情况:总类别数\", train_data.lbl.max() + 1)\n",
    "    print(\"== 训练集不同类别的样本数：\")\n",
    "    print(train_data.gem_name.value_counts())\n",
    "    print(f\"== 训练集样本数：{len(train_data)}\", f\"验证集样本数：{len(eval_data)}\")\n",
    "info()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-18T12:53:54.317269Z",
     "iopub.status.busy": "2022-11-18T12:53:54.315752Z",
     "iopub.status.idle": "2022-11-18T12:53:54.325868Z",
     "shell.execute_reply": "2022-11-18T12:53:54.324907Z",
     "shell.execute_reply.started": "2022-11-18T12:53:54.317223Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DatasetTask(Dataset):\n",
    "    def __init__(self, _df,transforms):       \n",
    "        super(DatasetTask, self).__init__()       \n",
    "        self._df=_df\n",
    "        self.transforms=transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self._df.iloc[index]['gem_path']\n",
    "        img = Image.open(img_path)\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB') \n",
    "        img = img.resize((224, 224), Image.BILINEAR)\n",
    "        img = np.array(img).astype('float32')\n",
    "        img = img.transpose((2, 0, 1)) / 255\n",
    "        if self.transforms is not None:\n",
    "          img = self.transforms(img)\n",
    "        label = self._df.iloc[index]['lbl']\n",
    "        label = np.array([label], dtype=\"int64\")\n",
    "        return img, label\n",
    "\n",
    "    def print_sample(self, index: int = 0):\n",
    "        \"\"\" 打印示例 \"\"\"\n",
    "        print(\"文件名\", self._df.iloc[index]['gem_path'], \"\\t标签值\", self._df.iloc[index]['lbl'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-18T12:53:57.873832Z",
     "iopub.status.busy": "2022-11-18T12:53:57.873056Z",
     "iopub.status.idle": "2022-11-18T12:53:57.879266Z",
     "shell.execute_reply": "2022-11-18T12:53:57.878366Z",
     "shell.execute_reply.started": "2022-11-18T12:53:57.873789Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import paddle.vision.transforms as T\n",
    "\n",
    "transforms = T.Compose([\n",
    "                T.RandomHorizontalFlip(0.5),#水平翻转\n",
    "                T.RandomRotation(15),#随机反转角度范围\n",
    "                T.RandomVerticalFlip(0.15),\n",
    "                T.RandomRotation(15)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-11-18T12:54:00.403698Z",
     "iopub.status.busy": "2022-11-18T12:54:00.402885Z",
     "iopub.status.idle": "2022-11-18T12:54:00.409758Z",
     "shell.execute_reply": "2022-11-18T12:54:00.408785Z",
     "shell.execute_reply.started": "2022-11-18T12:54:00.403649Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset=DatasetTask(train_data,transforms)\n",
    "eval_dataset=DatasetTask(eval_data,transforms)\n",
    "\n",
    "#训练数据加载\n",
    "train_loader = paddle.io.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#测试数据加载\n",
    "eval_loader = paddle.io.DataLoader(eval_dataset, batch_size = 8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-11-18T12:54:02.349790Z",
     "iopub.status.busy": "2022-11-18T12:54:02.348966Z",
     "iopub.status.idle": "2022-11-18T12:54:02.377078Z",
     "shell.execute_reply": "2022-11-18T12:54:02.376266Z",
     "shell.execute_reply.started": "2022-11-18T12:54:02.349741Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件名 /home/aistudio/data/dataset/Quartz Beer/quartz beer_16.jpg \t标签值 19\r\n",
      "729\r\n",
      "文件名 /home/aistudio/data/dataset/Beryl Golden/beryl golden_29.jpg \t标签值 3\r\n",
      "82\r\n",
      "(3, 224, 224)\r\n",
      "(1,)\r\n"
     ]
    }
   ],
   "source": [
    "train_dataset.print_sample(200)\n",
    "print(train_dataset.__len__())\n",
    "eval_dataset.print_sample(0)\n",
    "print(eval_dataset.__len__())\n",
    "print(eval_dataset.__getitem__(10)[0].shape)\n",
    "print(eval_dataset.__getitem__(10)[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-11-18T12:54:04.725202Z",
     "iopub.status.busy": "2022-11-18T12:54:04.724454Z",
     "iopub.status.idle": "2022-11-18T12:54:04.732252Z",
     "shell.execute_reply": "2022-11-18T12:54:04.731502Z",
     "shell.execute_reply.started": "2022-11-18T12:54:04.725161Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Batch=0\n",
    "Batchs=[]\n",
    "all_train_accs=[]\n",
    "def draw_train_acc(Batchs, train_accs):\n",
    "    title=\"training accs\"\n",
    "    plt.title(title, fontsize=24)\n",
    "    plt.xlabel(\"batch\", fontsize=14)\n",
    "    plt.ylabel(\"acc\", fontsize=14)\n",
    "    plt.plot(Batchs, train_accs, color='green', label='training accs')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "all_train_loss=[]\n",
    "def draw_train_loss(Batchs, train_loss):\n",
    "    title=\"training loss\"\n",
    "    plt.title(title, fontsize=24)\n",
    "    plt.xlabel(\"batch\", fontsize=14)\n",
    "    plt.ylabel(\"loss\", fontsize=14)\n",
    "    plt.plot(Batchs, train_loss, color='red', label='training loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-18T12:54:08.026502Z",
     "iopub.status.busy": "2022-11-18T12:54:08.025843Z",
     "iopub.status.idle": "2022-11-18T12:54:10.383657Z",
     "shell.execute_reply": "2022-11-18T12:54:10.381640Z",
     "shell.execute_reply.started": "2022-11-18T12:54:08.026462Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\r\n",
      "Requirement already satisfied: loguru in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (0.6.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.1.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m22.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-18T12:54:14.087393Z",
     "iopub.status.busy": "2022-11-18T12:54:14.086341Z",
     "iopub.status.idle": "2022-11-18T12:54:14.107856Z",
     "shell.execute_reply": "2022-11-18T12:54:14.106975Z",
     "shell.execute_reply.started": "2022-11-18T12:54:14.087350Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddle import nn as nn\n",
    "from loguru import logger\n",
    "# 基本上原样模仿facenet-pytorch中的网络\n",
    "class SelfModel2(nn.Layer):\n",
    "    def __init__(self,num_classes,pretrained=True):\n",
    "        super(SelfModel2,self).__init__() \n",
    "        self.backbone=paddle.vision.mobilenet_v2(pretrained=pretrained,with_pool=False,num_classes=num_classes)\n",
    "        state_dict=self.backbone.state_dict()\n",
    "        # for key in state_dict:\n",
    "        #     logger.debug(f'{key} {state_dict[key].shape}')\n",
    "\n",
    "        del self.backbone.classifier\n",
    "\n",
    "        self.avg=nn.AdaptiveAvgPool2D((1,1))\n",
    "        self.drop=nn.Dropout(0.5)\n",
    "        self.fc=nn.Linear(1280,128,bias_attr=False)\n",
    "        self.last_bn=nn.BatchNorm1D(128,epsilon=0.001,momentum=0.1)\n",
    "        self.classifier=nn.Linear(128,num_classes)\n",
    "\n",
    "    def forward(self,input):       \n",
    "        x=self.backbone.features(input)\n",
    "        x=self.avg(x)\n",
    "        x = paddle.flatten(x, 1)\n",
    "        x=self.drop(x)\n",
    "        #print(x.shape)\n",
    "        x=self.fc(x)\n",
    "        before_normalize=self.last_bn(x)\n",
    "        x=nn.functional.normalize(x,p=2,axis=1)\n",
    "        cls=self.classifier(before_normalize)\n",
    "        return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-18T12:51:46.283518Z",
     "iopub.status.idle": "2022-11-18T12:51:46.284187Z",
     "shell.execute_reply": "2022-11-18T12:51:46.283929Z",
     "shell.execute_reply.started": "2022-11-18T12:51:46.283904Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x=paddle.zeros((1,3,224,224),dtype='float32')\n",
    "# net=SelfModel2(25)\n",
    "# out=net(x)\n",
    "# print(len(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-18T12:54:16.984446Z",
     "iopub.status.busy": "2022-11-18T12:54:16.982941Z",
     "iopub.status.idle": "2022-11-18T12:54:17.097821Z",
     "shell.execute_reply": "2022-11-18T12:54:17.096591Z",
     "shell.execute_reply.started": "2022-11-18T12:54:16.984403Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import paddle\n",
    "from visualdl import LogWriter\n",
    "import datetime\n",
    "class LossHistory():\n",
    "    def __init__(self,log_dir):\n",
    "        date_str=datetime.datetime.strftime(datetime.datetime.now(),'%Y%m%d_%H%M%S')\n",
    "        self.log_dir=os.path.join(log_dir,date_str)\n",
    "        if not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "\n",
    "        self.log_wrt=LogWriter(logdir=self.log_dir)\n",
    "        self.losses=[]  \n",
    "        self.accs=[]\n",
    "       \n",
    "\n",
    "    def log_epoch(self,epoch,avg_acc,avg_loss,avg_eval_acc,avg_eval_loss,lr):\n",
    "        self.log_wrt.add_scalar(tag='train/avg_loss',value=avg_loss,step=epoch)\n",
    "        self.log_wrt.add_scalar(tag='train/avg_acc',value=avg_acc,step=epoch)\n",
    "        self.log_wrt.add_scalar(tag='eval/avg_eval_loss',value=avg_eval_loss,step=epoch)\n",
    "        self.log_wrt.add_scalar(tag='eval/avg_eval_acc',value=avg_eval_acc,step=epoch)\n",
    "        self.log_wrt.add_scalar(tag='train/lr',value=lr,step=epoch)\n",
    "        self.losses.append(avg_loss)\n",
    "        self.accs.append(avg_acc)\n",
    "\n",
    "    def log(self,epoch,batch_id,total_iter,train_loss,train_acc):\n",
    "        \n",
    "        #self.avg_loss_acc = np.around(self.avg_loss_acc, decimals=6)        \n",
    "        self.log_wrt.add_scalar(tag='train/loss', step=total_iter, value=train_loss.numpy().item())\n",
    "        self.log_wrt.add_scalar(tag='train/acc', step=total_iter, value=train_acc.numpy().item())\n",
    "    \n",
    "    def loss_plot(self):\n",
    "        iters = range(len(self.losses))\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(iters, self.losses, 'red', linewidth = 2, label='train loss')\n",
    "        # plt.plot(iters, self.val_loss, 'coral', linewidth = 2, label='val loss')\n",
    "        try:\n",
    "            if len(self.losses) < 25:\n",
    "                num = 5\n",
    "            else:\n",
    "                num = 15\n",
    "            plt.plot(iters, scipy.signal.savgol_filter(self.losses, num, 3), 'green', linestyle = '--', linewidth = 2, label='smooth train loss')\n",
    "            # plt.plot(iters, scipy.signal.savgol_filter(self.val_loss, num, 3), '#8B4513', linestyle = '--', linewidth = 2, label='smooth val loss')\n",
    "        except:\n",
    "            pass\n",
    "        plt.grid(True)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.savefig(os.path.join(self.log_dir, \"epoch_loss.png\"))\n",
    "        plt.cla()\n",
    "        plt.close(\"all\")\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(iters, self.accs, 'red', linewidth = 2, label='acc')\n",
    "        try:\n",
    "            if len(self.losses) < 25:\n",
    "                num = 5\n",
    "            else:\n",
    "                num = 15\n",
    "            plt.plot(iters, scipy.signal.savgol_filter(self.acc, num, 3), 'green', linestyle = '--', linewidth = 2, label='smooth lfw acc')\n",
    "        except:\n",
    "            pass\n",
    "        plt.grid(True)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Lfw Acc')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.savefig(os.path.join(self.log_dir, \"epoch_acc.png\"))\n",
    "        plt.cla()\n",
    "        plt.close(\"all\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3、训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预热学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-18T12:51:46.287699Z",
     "iopub.status.idle": "2022-11-18T12:51:46.288505Z",
     "shell.execute_reply": "2022-11-18T12:51:46.288267Z",
     "shell.execute_reply.started": "2022-11-18T12:51:46.288241Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -U paddlenlp==2.4.3\n",
    "#!pip install -U paddlepaddle-gpu -i https://mirror.baidu.com/pypi/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-18T12:54:20.606799Z",
     "iopub.status.busy": "2022-11-18T12:54:20.605983Z",
     "iopub.status.idle": "2022-11-18T12:54:20.618589Z",
     "shell.execute_reply": "2022-11-18T12:54:20.617560Z",
     "shell.execute_reply.started": "2022-11-18T12:54:20.606760Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import math\n",
    "def get_lr_scheduler(lr_decay_type, max_lr, min_lr, total_epoch, warmup_epoch_ratio = 0.1, warmup_lr_ratio = 0.1, no_aug_epoch_ratio = 0.3, step_num = 10):\n",
    "    def yolox_warm_cos_lr(max_lr, min_lr, total_epoch, warmup_total_epoch, warmup_lr_start, no_aug_epoch, epoch):\n",
    "\n",
    "        if epoch <= warmup_total_epoch: # 预热\n",
    "            # lr = (lr - warmup_lr_start) * iters / float(warmup_total_epoch) + warmup_lr_start\n",
    "            # 预热过程中，学习率每代前进多少\n",
    "            max_lr = (max_lr - warmup_lr_start) * pow(epoch / float(warmup_total_epoch), 2) + warmup_lr_start\n",
    "        elif epoch >= total_epoch - no_aug_epoch: # 不对学习率进行计算\n",
    "            max_lr = min_lr\n",
    "        else: #预热和不对学习率进行计算之间的代\n",
    "            max_lr = min_lr + 0.5 * (max_lr - min_lr) * (\n",
    "                1.0\n",
    "                + math.cos(math.pi * (epoch - warmup_total_epoch) / (total_epoch - warmup_total_epoch - no_aug_epoch))\n",
    "                # (现在代-预热总代数)/(总代-预热总代-退火总代)\n",
    "            )\n",
    "        return max_lr\n",
    "\n",
    "    # 非cos预热时如何计算\n",
    "    def step_lr(max_lr, decay_rate, step_size, epoch):\n",
    "        if step_size < 1:\n",
    "            raise ValueError(\"step_size must above 1.\")\n",
    "        n       = epoch // step_size\n",
    "        out_lr  = max_lr * decay_rate ** n\n",
    "        return out_lr\n",
    "\n",
    "    if lr_decay_type == \"cos\":\n",
    "        # 预热多少代\n",
    "        warmup_total_epoch  = min(max(warmup_epoch_ratio * total_epoch, 1), 3)\n",
    "        # 预热开始的学习率\n",
    "        warmup_lr_start     = max(warmup_lr_ratio * max_lr, 1e-6)\n",
    "        no_aug_epoch         = min(max(no_aug_epoch_ratio * total_epoch, 1), 15)\n",
    "        func = partial(yolox_warm_cos_lr, max_lr, min_lr, total_epoch, warmup_total_epoch, warmup_lr_start, no_aug_epoch)\n",
    "    else:\n",
    "        decay_rate  = (min_lr / max_lr) ** (1 / (step_num - 1))\n",
    "        step_size   = total_epoch / step_num\n",
    "        func = partial(step_lr, max_lr, decay_rate, step_size)\n",
    "\n",
    "    return func\n",
    "\n",
    "def set_optimizer_lr(optimizer, lr_scheduler_func, epoch):\n",
    "    lr = lr_scheduler_func(epoch)\n",
    "    logger.info(f'current lr {lr}')\n",
    "    optimizer.set_lr(lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-18T12:51:46.291968Z",
     "iopub.status.idle": "2022-11-18T12:51:46.292518Z",
     "shell.execute_reply": "2022-11-18T12:51:46.292281Z",
     "shell.execute_reply.started": "2022-11-18T12:51:46.292255Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 停止更新，可以\n",
    "# net=SelfModel2(25,pretrained=False)\n",
    "# _state_dict=net.backbone.parameters()\n",
    "# for key in _state_dict:\n",
    "#     key.stop_gradient=True\n",
    "\n",
    "\n",
    "# _state_dict=net.backbone.state_dict()\n",
    "# for key in _state_dict:\n",
    "#    print(_state_dict[key].stop_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-18T12:51:46.293852Z",
     "iopub.status.idle": "2022-11-18T12:51:46.294381Z",
     "shell.execute_reply": "2022-11-18T12:51:46.294155Z",
     "shell.execute_reply.started": "2022-11-18T12:51:46.294132Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 不行，不能停止更新\n",
    "# net=SelfModel2(25,pretrained=False)\n",
    "# net.backbone.stop_gradient=False\n",
    "# _state_dict=net.backbone.state_dict()\n",
    "# for key in _state_dict:\n",
    "#    print(key,_state_dict[key].stop_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-18T12:54:27.762080Z",
     "iopub.status.busy": "2022-11-18T12:54:27.761089Z",
     "iopub.status.idle": "2022-11-18T12:54:29.896421Z",
     "shell.execute_reply": "2022-11-18T12:54:29.895139Z",
     "shell.execute_reply.started": "2022-11-18T12:54:27.762028Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: paddlepaddle-gpu\r\n",
      "Version: 2.0.2.post101\r\n",
      "Summary: Parallel Distributed Deep Learning\r\n",
      "Home-page: UNKNOWN\r\n",
      "Author: \r\n",
      "Author-email: Paddle-better@baidu.com\r\n",
      "License: Apache Software License\r\n",
      "Location: /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages\r\n",
      "Requires: astor, decorator, gast, numpy, Pillow, protobuf, requests, six\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "!pip show paddlepaddle-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def fit_one(model,opt,start_epoch,end_epoch,cross_entropy,loss_history,scheduler,train_loader,eval_loader):\n",
    "    for pass_num in range(start_epoch,end_epoch):\n",
    "        accs=[]\n",
    "        losses=[]\n",
    "        eval_accs=[]\n",
    "        eval_losses=[]\n",
    "        # if pass_num>0:break\n",
    "        model.train()\n",
    "        lr=set_optimizer_lr(opt,scheduler,pass_num-start_epoch)\n",
    "        for batch_id,data in enumerate(train_loader()):\n",
    "            # if batch_id>0:break\n",
    "            image = data[0]\n",
    "            label = data[1]\n",
    "\n",
    "            predict=model(image) #数据传入model\n",
    "\n",
    "            loss=cross_entropy(predict,label)\n",
    "            acc=paddle.metric.accuracy(predict,label)#计算精度\n",
    "            accs.append(acc.numpy().item())\n",
    "            losses.append(loss.numpy().item())\n",
    "            loss_history.log(pass_num,batch_id,batch_id+pass_num*batch_size,loss,acc)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()   #opt.clear_grad()来重置梯度\n",
    "        model.eval()\n",
    "        for batch_id,data in enumerate(eval_loader()):#测试集\n",
    "            image=data[0]\n",
    "            label=data[1]\n",
    "            predict=model(image)\n",
    "            loss=cross_entropy(predict,label)\n",
    "            acc=paddle.metric.accuracy(predict,label)\n",
    "\n",
    "            eval_accs.append(acc.numpy().item())\n",
    "            eval_losses.append(loss.numpy().item())\n",
    "\n",
    "\n",
    "            #scheduler.step()\n",
    "        avg_acc=np.mean(accs)\n",
    "        avg_loss=np.mean(losses)\n",
    "        avg_eval_loss=np.mean(eval_losses)\n",
    "        avg_eval_acc=np.mean(eval_accs)\n",
    "        #loss_writer.add_scalar(tag=\"train/acc\",value=avg_acc,step=pass_num)\n",
    "        loss_history.log_epoch(pass_num, avg_acc, avg_loss,avg_eval_acc,avg_eval_loss,lr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-11-18T12:54:31.725196Z",
     "iopub.status.busy": "2022-11-18T12:54:31.723721Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aistudio/.data/webide/pip/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1483: UserWarning: Skip loading for classifier.1.weight. classifier.1.weight receives a shape [1280, 1000], but the expected shape is [1280, 25].\r\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\r\n",
      "/home/aistudio/.data/webide/pip/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1483: UserWarning: Skip loading for classifier.1.bias. classifier.1.bias receives a shape [1000], but the expected shape is [25].\r\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\r\n",
      "2022-11-18 20:54:32.097 | INFO     | __main__:<module>:22 -  0.001 1e-05\r\n",
      "2022-11-18 20:54:32.102 | INFO     | __main__:set_optimizer_lr:44 - current lr 0.0001\r\n",
      "/home/aistudio/.data/webide/pip/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.\r\n",
      "  \"When training, we now always track global mean and variance.\")\r\n",
      "2022-11-18 20:55:30.762 | INFO     | __main__:set_optimizer_lr:44 - current lr 0.00019999999999999998\r\n",
      "2022-11-18 20:56:35.981 | INFO     | __main__:set_optimizer_lr:44 - current lr 0.0005\r\n",
      "2022-11-18 20:57:31.246 | INFO     | __main__:set_optimizer_lr:44 - current lr 0.001\r\n",
      "2022-11-18 20:58:27.126 | INFO     | __main__:set_optimizer_lr:44 - current lr 0.000999636759620151\r\n"
     ]
    }
   ],
   "source": [
    "# 冻结网络\n",
    "import datetime\n",
    "# from paddlenlp.transformers import *\n",
    "loss_history=LossHistory('log')\n",
    "model=SelfModel2(25) #模型实例化\n",
    "#model=paddle.vision.mobilenet_v1(pretrained=True,num_classes=25)\n",
    "\n",
    "cross_entropy = paddle.nn.CrossEntropyLoss()\n",
    "\n",
    "epochs_num=train_parameters['num_epochs'] #迭代次数\n",
    "optimizer_type='adam'\n",
    "max_lr = 1e-3\n",
    "min_lr = max_lr * 0.01\n",
    "nbs = 64\n",
    "lr_limit_max = 1e-3 if optimizer_type == 'adam' else 1e-1  # 0.001\n",
    "lr_limit_min = 3e-4 if optimizer_type == 'adam' else 5e-4  # 0.0003\n",
    "# init_lr=e-3, min_lr=e-5\n",
    "# 自适应最大学习率不能小于最小，不能大于最大 min(max(0.0015,0.0003),0.001)\n",
    "max_lr_fit = min(max(batch_size / nbs * max_lr, lr_limit_min), lr_limit_max)\n",
    "# 自适应最小，不小于最小的0.01，不大于最大学习率的0.01  min(max(1.5e-5,e-7),e-5)\n",
    "min_lr_fit = min(max(batch_size / nbs * min_lr, lr_limit_min * 1e-2), lr_limit_max * 1e-2)\n",
    "logger.info(f' {max_lr_fit} {min_lr_fit}')\n",
    "#   获得学习率下降的公式\n",
    "\n",
    "#scheduler = CosineAnnealingWithWarmupDecay(max_lr_fit,min_lr_fit,0.1*epochs_num,0.4*epochs_num,verbose=True)\n",
    "scheduler =get_lr_scheduler('cos', max_lr_fit, min_lr_fit, epochs_num)\n",
    "opt=paddle.optimizer.Adam(learning_rate=max_lr_fit, parameters=model.parameters())\n",
    "for key in model.backbone.parameters():\n",
    "    key.stop_gradient = True\n",
    "fit_one(model,opt,0,epochs_num,cross_entropy,loss_history,scheduler,train_loader,eval_loader)\n",
    "\n",
    "postfix=datetime.datetime.strftime(datetime.datetime.now(),'%Y%m%d_%H%M%S')\n",
    "#obj = {'model': model.state_dict(), 'opt': opt.state_dict(), 'epoch': pass_num}\n",
    "path = f'selfmodel-{postfix}.pdparams'\n",
    "paddle.save(model.state_dict(), path)\n",
    "\n",
    "#draw_train_acc(Batchs,all_train_accs)\n",
    "#draw_train_loss(Batchs,all_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-11-18T12:38:21.401191Z",
     "iopub.status.busy": "2022-11-18T12:38:21.400361Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aistudio/.data/webide/pip/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1483: UserWarning: Skip loading for classifier.1.weight. classifier.1.weight receives a shape [1280, 1000], but the expected shape is [1280, 25].\r\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\r\n",
      "/home/aistudio/.data/webide/pip/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1483: UserWarning: Skip loading for classifier.1.bias. classifier.1.bias receives a shape [1000], but the expected shape is [25].\r\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\r\n",
      "2022-11-18 20:38:21.734 | INFO     | __main__:<module>:21 -  0.001 1e-05\r\n",
      "2022-11-18 20:38:21.739 | INFO     | __main__:set_optimizer_lr:44 - current lr 0.0001\r\n",
      "/home/aistudio/.data/webide/pip/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.\r\n",
      "  \"When training, we now always track global mean and variance.\")\r\n",
      "2022-11-18 20:39:14.781 | INFO     | __main__:set_optimizer_lr:44 - current lr 0.00019999999999999998\r\n"
     ]
    }
   ],
   "source": [
    "# 不冻结网络\n",
    "import datetime\n",
    "\n",
    "cross_entropy = paddle.nn.CrossEntropyLoss()\n",
    "optimizer_type='adam'\n",
    "max_lr = 1e-3\n",
    "min_lr = max_lr * 0.01\n",
    "nbs = 64\n",
    "lr_limit_max = 1e-3 if optimizer_type == 'adam' else 1e-1  # 0.001\n",
    "lr_limit_min = 3e-4 if optimizer_type == 'adam' else 5e-4  # 0.0003\n",
    "\n",
    "max_lr_fit = min(max(batch_size / nbs * max_lr, lr_limit_min), lr_limit_max)\n",
    "min_lr_fit = min(max(batch_size / nbs * min_lr, lr_limit_min * 1e-2), lr_limit_max * 1e-2)\n",
    "logger.info(f' {max_lr_fit} {min_lr_fit}')\n",
    "#   获得学习率下降的公式\n",
    "\n",
    "#scheduler = CosineAnnealingWithWarmupDecay(max_lr_fit,min_lr_fit,0.1*epochs_num,0.4*epochs_num,verbose=True)\n",
    "scheduler =get_lr_scheduler('cos', max_lr_fit, min_lr_fit, epochs_num)\n",
    "opt=paddle.optimizer.Adam(learning_rate=max_lr_fit, parameters=model.parameters())\n",
    "for key in model.backbone.parameters():\n",
    "    key.stop_gradient = False\n",
    "\n",
    "#训练数据加载\n",
    "train_loader = paddle.io.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#测试数据加载\n",
    "eval_loader = paddle.io.DataLoader(eval_dataset, batch_size = 8, shuffle=False)\n",
    "\n",
    "fit_one(model,opt,epochs_num,200,cross_entropy,loss_history,scheduler,train_loader,eval_loader)\n",
    "\n",
    "'''\n",
    "for pass_num in range(epochs_num+1,200):\n",
    "    accs=[]\n",
    "    losses=[]  \n",
    "    eval_accs=[]\n",
    "    eval_losses=[]    \n",
    "    # if pass_num>0:break\n",
    "    model.train()\n",
    "    lr=set_optimizer_lr(opt,scheduler,pass_num-epochs_num)\n",
    "    for batch_id,data in enumerate(train_loader()):\n",
    "        # if batch_id>0:break\n",
    "        image = data[0]\n",
    "        label = data[1]\n",
    "\n",
    "        predict=model(image) #数据传入model\n",
    "       \n",
    "        loss=cross_entropy(predict,label)\n",
    "        acc=paddle.metric.accuracy(predict,label)#计算精度\n",
    "        accs.append(acc.numpy().item())\n",
    "        losses.append(loss.numpy().item())      \n",
    "        loss_history.log(pass_num,batch_id,batch_id+pass_num*batch_size,loss,acc) \n",
    "        \n",
    "        loss.backward()       \n",
    "        opt.step()\n",
    "        opt.clear_grad()   #opt.clear_grad()来重置梯度\n",
    "    model.eval()\n",
    "    for batch_id,data in enumerate(eval_loader()):#测试集\n",
    "        image=data[0]\n",
    "        label=data[1]     \n",
    "        predict=model(image)       \n",
    "        loss=cross_entropy(predict,label)\n",
    "        acc=paddle.metric.accuracy(predict,label)\n",
    "\n",
    "        eval_accs.append(acc.numpy().item())\n",
    "        eval_losses.append(loss.numpy().item())        \n",
    "\n",
    "\n",
    "    #scheduler.step()    \n",
    "    avg_acc=np.mean(accs)    \n",
    "    avg_loss=np.mean(losses)\n",
    "    avg_eval_loss=np.mean(eval_losses)\n",
    "    avg_eval_acc=np.mean(eval_accs)\n",
    "    #loss_writer.add_scalar(tag=\"train/acc\",value=avg_acc,step=pass_num)\n",
    "    loss_history.log_epoch(pass_num, avg_acc, avg_loss,avg_eval_acc,avg_eval_loss,lr)\n",
    "'''\n",
    "\n",
    "postfix=datetime.datetime.strftime(datetime.datetime.now(),'%Y%m%d_%H%M%S')\n",
    "#obj = {'model': model.state_dict(), 'opt': opt.state_dict(), 'epoch': pass_num}\n",
    "path = f'selfmodel-{postfix}-melt.pdparams'\n",
    "paddle.save(model.state_dict(), path)\n",
    "\n",
    "#draw_train_acc(Batchs,all_train_accs)\n",
    "#draw_train_loss(Batchs,all_train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4、模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#模型评估\n",
    "para_state_dict = paddle.load(\"selfmodel-99.pdparams\")\n",
    "model.set_state_dict(para_state_dict['model']) #加载模型参数\n",
    "model.eval() #验证模式\n",
    "\n",
    "accs = []\n",
    "\n",
    "for batch_id,data in enumerate(eval_loader()):#测试集\n",
    "    image=data[0]\n",
    "    label=data[1]     \n",
    "    predict=model(image)       \n",
    "    acc=paddle.metric.accuracy(predict,label)\n",
    "    accs.append(acc.numpy()[0])\n",
    "    avg_acc = np.mean(accs)\n",
    "print(\"当前模型在验证集上的准确率为:\",avg_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5、模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "def unzip_infer_data(src_path,target_path):\n",
    "    '''\n",
    "    解压预测数据集\n",
    "    '''\n",
    "    if(not os.path.isdir(target_path)):     \n",
    "        z = zipfile.ZipFile(src_path, 'r')\n",
    "        z.extractall(path=target_path)\n",
    "        z.close()\n",
    "\n",
    "\n",
    "def load_image(img_path):\n",
    "    '''\n",
    "    预测图片预处理\n",
    "    '''\n",
    "    img = Image.open(img_path) \n",
    "    if img.mode != 'RGB': \n",
    "        img = img.convert('RGB') \n",
    "    img = img.resize((224, 224), Image.BILINEAR)\n",
    "    img = np.array(img).astype('float32') \n",
    "    img = img.transpose((2, 0, 1))  # HWC to CHW \n",
    "    img = img/255                # 像素值归一化 \n",
    "    return img\n",
    "\n",
    "\n",
    "infer_src_path = '/home/aistudio/data/data55032/archive_test.zip'\n",
    "infer_dst_path = '/home/aistudio/data/archive_test'\n",
    "unzip_infer_data(infer_src_path,infer_dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "模型预测\n",
    "'''\n",
    "para_state_dict = paddle.load(\"MyDNN\")\n",
    "model = SelfModel2(25)\n",
    "model.set_state_dict(para_state_dict) #加载模型参数\n",
    "model.eval() #训练模式\n",
    "\n",
    "#展示预测图片\n",
    "infer_path='data/archive_test/alexandrite_3.jpg'\n",
    "img = Image.open(infer_path)\n",
    "plt.imshow(img)          #根据数组绘制图像\n",
    "plt.show()               #显示图像\n",
    "\n",
    "#对预测图片进行预处理\n",
    "\n",
    "infer_imgs = []\n",
    "infer_imgs.append(load_image(infer_path))\n",
    "infer_imgs = np.array(infer_imgs)\n",
    "\n",
    "label_dic = train_parameters['label_dict']\n",
    "name_label={v:k for k,v in label_dic.items()}\n",
    "print(name_label)\n",
    "\n",
    "for i in range(len(infer_imgs)):\n",
    "    data = infer_imgs[i]\n",
    "    dy_x_data = np.array(data).astype('float32')\n",
    "    dy_x_data=dy_x_data[np.newaxis,:, : ,:]\n",
    "    img = paddle.to_tensor (dy_x_data)\n",
    "    out = model(img)\n",
    "    lab = np.argmax(out.numpy())  #argmax():返回最大数的索引\n",
    "\n",
    "    print(\"第{}个样本,被预测为：{},真实标签为：{}\".format(i+1,name_label[lab],infer_path.split('/')[-1].split(\"_\")[0]))\n",
    "        \n",
    "print(\"结束\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "infer_imgs = []\n",
    "infer_labels=[]\n",
    "num_labels=[]\n",
    "for infer_path in Path(infer_dst_path).glob('*.jpg'):\n",
    "    infer_imgs.append(load_image(infer_path))\n",
    "    text_label=infer_path.stem.split('_')[0]\n",
    "    infer_labels.append(text_label)\n",
    "    num_labels.append(label_dic[text_label.capitalize()])\n",
    "\n",
    "infer_imgs = np.array(infer_imgs)\n",
    "\n",
    "label_dic = train_parameters['label_dict']\n",
    "\n",
    "\n",
    "for i in range(len(infer_imgs)):\n",
    "    data = infer_imgs[i]\n",
    "    dy_x_data = np.array(data).astype('float32')\n",
    "    dy_x_data=dy_x_data[np.newaxis,:, : ,:]\n",
    "    img = paddle.to_tensor (dy_x_data)\n",
    "    out = model(img)\n",
    "    lab = np.argmax(out.numpy())  #argmax():返回最大数的索引\n",
    "\n",
    "    print(\"第{}个样本,被预测为：{},真实标签为：{}\".format(i+1,name_label[lab],infer_labels[i]))\n",
    "        \n",
    "print(\"结束\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
