{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "来自于[百度架构师手把手带你零基础实践深度学习](https://aistudio.baidu.com/aistudio/projectdetail/4163534)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 使用Python和NumPy构建神经网络模型\n",
    "\n",
    "上一节我们初步认识了神经网络的基本概念（如神经元、多层连接、前向计算、计算图）和模型结构三要素（模型假设、评价函数和优化算法）。本节将以“波士顿房价预测”任务为例，向读者介绍使用Python和NumPy来构建神经网络模型的思考过程和操作方法。\n",
    "\n",
    "波士顿房价预测是一个经典的机器学习任务，类似于程序员世界的“Hello World”。和大家对房价的普遍认知相同，波士顿地区的房价受诸多因素影响。该数据集统计了13种可能影响房价的因素和该类型房屋的均价，期望构建一个基于13个因素进行房价预测的模型，如 **图1** 所示。\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/abce0cb2a92f4e679c6855cfa520491597171533a0b0447e8d51d904446e213e\" width=\"500\" hegiht=\"\" ></center>\n",
    "<center><br>图1：波士顿房价影响因素示意图</br></center>\n",
    "<br></br>\n",
    "\n",
    "对于预测问题，可以根据预测输出的类型是连续的实数值，还是离散的标签，区分为回归任务和分类任务。因为房价是一个连续值，所以房价预测显然是一个回归任务。下面我们尝试用最简单的线性回归模型解决这个问题，并用神经网络来实现这个模型。\n",
    "\n",
    "## 1. 线性回归模型\n",
    "\n",
    "假设房价和各影响因素之间能够用线性关系来描述：\n",
    "\n",
    "$$y = {\\sum_{j=1}^Mx_j w_j} + b (公式1)$$\n",
    "\n",
    "模型的求解即是通过数据拟合出每个$w_j$和$b$。其中，$w_j$和$b$分别表示该线性模型的权重和偏置。一维情况下，$w_j$ 和 $b$ 是直线的斜率和截距。\n",
    "\n",
    "线性回归模型使用均方误差作为损失函数（Loss），用以衡量预测房价和真实房价的差异，公式如下：\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat{Y_i} - {Y_i})^{2} (公式2)$$\n",
    "\n",
    "------\n",
    "**思考：**\n",
    "\n",
    "为什么要以均方误差作为损失函数？即将模型在每个训练样本上的预测误差加和，来衡量整体样本的准确性。这是因为损失函数的设计不仅仅要考虑“合理性”，同样需要考虑“易解性”，这个问题在后面的内容中会详细阐述。\n",
    "\n",
    "------\n",
    "\n",
    "\n",
    "神经网络的标准结构中每个神经元由加权和与非线性变换构成，然后将多个神经元分层的摆放并连接形成神经网络。线性回归模型可以认为是神经网络模型的一种极简特例，是一个只有加权和、没有非线性变换的神经元（无需形成网络），如 **图2** 所示。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/f9117a5a34d44b1eab85147e62b4e6295e485e48d79d4a03adaa14a447ffd230\" width=\"300\" hegiht=\"\" ></center>\n",
    "<center><br>图2：线性回归模型的神经网络结构</br></center>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. 使用Python和NumPy实现波士顿房价预测任务\n",
    "\n",
    "深度学习不仅实现了模型的端到端学习，还推动了人工智能进入工业大生产阶段，产生了标准化、自动化和模块化的通用框架。不同场景的深度学习模型具备一定的通用性，五个步骤即可完成模型的构建和训练，如 **图3** 所示。\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/5325a6356c164bf38b5665d1ae5e48acceccfdaa9ddd414cb182b98e6f6054c2\" width=\"800\" hegiht=\"\" ></center>\n",
    "<center><br>图3：构建神经网络/深度学习模型的基本步骤</br></center>\n",
    "<br></br>\n",
    "\n",
    "正是由于深度学习的建模和训练的过程存在通用性，在构建不同的模型时，只有模型三要素不同，其它步骤基本一致，深度学习框架才有用武之地。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.1 数据处理\n",
    "\n",
    "数据处理包含五个部分：数据导入、数据形状变换、数据集划分、数据归一化处理和封装`load data`函数。数据预处理后，才能被模型调用。\n",
    "\n",
    "------\n",
    "**说明：**\n",
    "\n",
    "* 本教程中的代码都可以在AI Studio上直接运行，Print结果都是基于程序真实运行的结果。\n",
    "* 由于是真实案例，代码之间存在依赖关系，因此需要读者逐条、全部运行，否则会导致命令执行报错。\n",
    "\n",
    "------\n",
    "\n",
    "#### 2.1.1 读入数据\n",
    "\n",
    "通过如下代码读入数据，了解下波士顿房价的数据集结构，数据存放在本地目录下housing.data文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.320e-03, 1.800e+01, 2.310e+00, ..., 3.969e+02, 7.880e+00,\n",
       "       1.190e+01])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入需要用到的package\n",
    "import numpy as np\n",
    "import json\n",
    "# 读入训练数据\n",
    "datafile = './work/housing.data'\n",
    "data = np.fromfile(datafile, sep=' ')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "####  2.1.2 数据形状变换\n",
    "\n",
    "由于读入的原始数据是1维的，所有数据都连在一起。因此需要我们将数据的形状进行变换，形成一个2维的矩阵，每行为一个数据样本（14个值），每个数据样本包含13个$X$（影响房价的特征）和一个$Y$（该类型房屋的均价）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 读入之后的数据被转化成1维array，其中array的第0-13项是第一条数据，第14-27项是第二条数据，以此类推.... \n",
    "# 这里对原始数据做reshape，变成N x 14的形式\n",
    "feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE','DIS', \n",
    "                 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\n",
    "feature_num = len(feature_names)\n",
    "data = data.reshape([data.shape[0] // feature_num, feature_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14,)\n",
      "[6.320e-03 1.800e+01 2.310e+00 0.000e+00 5.380e-01 6.575e+00 6.520e+01\n",
      " 4.090e+00 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00 2.400e+01]\n"
     ]
    }
   ],
   "source": [
    "# 查看数据\n",
    "x = data[0]\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "####  2.1.3 数据集划分\n",
    "\n",
    "将数据集划分成训练集和测试集，其中训练集用于确定模型的参数，测试集用于评判模型的效果。为什么要对数据集进行拆分，而不能直接应用于模型训练呢？这与学生时代的授课和考试关系比较类似，如 **图4** 所示。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/a1c845a50e28474d9aa72028edfea33f1a3deca1d54d40ec94ba366d3a18c408\" width=\"600\" hegiht=\"\" ></center>\n",
    "<center><br>图4：训练集和测试集拆分的意义</br></center>\n",
    "<br></br>\n",
    "\n",
    "上学时总有一些自作聪明的同学，平时不认真学习，考试前临阵抱佛脚，将习题死记硬背下来，但是成绩往往并不好。因为学校期望学生掌握的是知识，而不仅仅是习题本身。另出新的考题，才能鼓励学生努力去掌握习题背后的原理。同样我们期望模型学习的是任务的本质规律，而不是训练数据本身，模型训练未使用的数据，才能更真实的评估模型的效果。\n",
    "\n",
    "在本案例中，我们将80%的数据用作训练集，20%用作测试集，实现代码如下。通过打印训练集的形状，可以发现共有404个样本，每个样本含有13个特征和1个预测值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 14)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio = 0.8\n",
    "offset = int(data.shape[0] * ratio)\n",
    "training_data = data[:offset]\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "####  2.1.4 数据归一化处理\n",
    "\n",
    "对每个特征进行归一化处理，使得每个特征的取值缩放到0~1之间。这样做有两个好处：一是模型训练更高效，在本节的后半部分会详细说明；二是特征前的权重大小可以代表该变量对预测结果的贡献度（因为每个特征值本身的范围相同）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 计算train数据集的最大值，最小值，平均值\n",
    "maximums, minimums, avgs = \\\n",
    "                     training_data.max(axis=0), \\\n",
    "                     training_data.min(axis=0), \\\n",
    "     training_data.sum(axis=0) / training_data.shape[0]\n",
    "# 对数据进行归一化处理\n",
    "for i in range(feature_num):\n",
    "    #print(maximums[i], minimums[i], avgs[i])\n",
    "    data[:, i] = (data[:, i] - avgs[i]) / (maximums[i] - minimums[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "####  2.1.5 封装成load data函数\n",
    "\n",
    "将上述几个数据处理操作封装成`load data`函数，以便下一步模型的调用，实现方法如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # 从文件导入数据\n",
    "    datafile = './work/housing.data'\n",
    "    data = np.fromfile(datafile, sep=' ')\n",
    "\n",
    "    # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数\n",
    "    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \\\n",
    "                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\n",
    "    feature_num = len(feature_names)\n",
    "\n",
    "    # 将原始数据进行Reshape，变成[N, 14]这样的形状\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\n",
    "\n",
    "    # 将原数据集拆分成训练集和测试集\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\n",
    "    # 测试集和训练集必须是没有交集的\n",
    "    ratio = 0.8\n",
    "    offset = int(data.shape[0] * ratio)\n",
    "    training_data = data[:offset]\n",
    "\n",
    "    # 计算训练集的最大值，最小值，平均值\n",
    "    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \\\n",
    "                                 training_data.sum(axis=0) / training_data.shape[0]\n",
    "\n",
    "    # 对数据进行归一化处理\n",
    "    for i in range(feature_num):\n",
    "        #print(maximums[i], minimums[i], avgs[i])\n",
    "        data[:, i] = (data[:, i] - avgs[i]) / (maximums[i] - minimums[i])\n",
    "\n",
    "    # 训练集和测试集的划分比例\n",
    "    training_data = data[:offset]\n",
    "    test_data = data[offset:]\n",
    "    return training_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 获取数据\n",
    "training_data, test_data = load_data()\n",
    "x = training_data[:, :-1]\n",
    "y = training_data[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02146321  0.03767327 -0.28552309 -0.08663366  0.01289726  0.04634817\n",
      "  0.00795597 -0.00765794 -0.25172191 -0.11881188 -0.29002528  0.0519112\n",
      " -0.17590923]\n",
      "[-0.00390539]\n"
     ]
    }
   ],
   "source": [
    "# 查看数据\n",
    "print(x[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 模型设计\n",
    "\n",
    "模型设计是深度学习模型关键要素之一，也称为网络结构设计，相当于模型的假设空间，即实现模型“前向计算”（从输入到输出）的过程。\n",
    "\n",
    "如果将输入特征和输出预测值均以向量表示，输入特征$x$有13个分量，$y$有1个分量，那么参数权重的形状（shape）是$13\\times1$。假设我们以如下任意数字赋值参数做初始化：\n",
    "$$w=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, -0.1, -0.2, -0.3, -0.4, 0.0]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, -0.1, -0.2, -0.3, -0.4, 0.0]\n",
    "w = np.array(w).reshape([13, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "取出第1条样本数据，观察样本的特征向量与参数向量相乘的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03395597]\n"
     ]
    }
   ],
   "source": [
    "x1=x[0]\n",
    "t = np.dot(x1, w)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "完整的线性回归公式，还需要初始化偏移量$b$，同样随意赋初值-0.2。那么，线性回归模型的完整输出是$z=t+b$，这个从特征和参数计算输出值的过程称为“前向计算”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.16604403]\n"
     ]
    }
   ],
   "source": [
    "b = -0.2\n",
    "z = t + b\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n",
      "(404, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "将上述计算预测输出的过程以“类和对象”的方式来描述，类成员变量有参数$w$和$b$。通过写一个`forward`函数（代表“前向计算”）完成上述从特征和参数到输出预测值的计算过程，代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，\n",
    "        # 此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "基于Network类的定义，模型的计算过程如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.63182506]\n"
     ]
    }
   ],
   "source": [
    "net = Network(13)\n",
    "x1 = x[0]\n",
    "y1 = y[0]\n",
    "z = net.forward(x1)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "从上述前向计算的过程可见，线性回归也可以表示成一种简单的神经网络（只有一个神经元，且激活函数为恒等式）。这也是机器学习模型普遍为深度学习模型替代的原因：由于深度学习网络强大的表示能力，很多传统机器学习模型的学习能力等同于相对简单的深度学习模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.3 训练配置\n",
    "\n",
    "模型设计完成后，需要通过训练配置寻找模型的最优值，即通过损失函数来衡量模型的好坏。训练配置也是深度学习模型关键要素之一。\n",
    "\n",
    "通过模型计算$x_1$表示的影响因素所对应的房价应该是$z$, 但实际数据告诉我们房价是$y$。这时我们需要有某种指标来衡量预测值$z$跟真实值$y$之间的差距。对于回归问题，最常采用的衡量方法是使用均方误差作为评价模型好坏的指标，具体定义如下：\n",
    "\n",
    "$$Loss = (y - z)^2 (公式3)$$\n",
    "\n",
    "上式中的$Loss$（简记为: $L$）通常也被称作损失函数，它是衡量模型好坏的指标。读者可能会奇怪：如果要衡量预测放假和真实房价之间的差距，是否将每一个样本的差距的绝对值加和即可？差距绝对值加和是更加直观和朴素的思路，为何要平方加和？ 损失函数的设计不仅要考虑准确衡量问题的“合理性”，通常还要考虑“易于优化求解”。至于这个问题的答案，在介绍完优化算法后再揭示。\n",
    "\n",
    "在回归问题中，均方误差是一种比较常见的形式，分类问题中通常会采用交叉熵作为损失函数，在后续的章节中会更详细的介绍。对一个样本计算损失函数值的实现如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39428312]\n"
     ]
    }
   ],
   "source": [
    "Loss = (y1 - z)*(y1 - z)\n",
    "print(Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "因为计算损失函数时需要把每个样本的损失函数值都考虑到，所以我们需要对单个样本的损失函数进行求和，并除以样本总数$N$。\n",
    "$$L= \\frac{1}{N}\\sum_{i=1}^N{(y_i - z_i)^2} (公式4)$$\n",
    "在Network类下面添加损失函数的计算过程如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        cost = error * error\n",
    "        cost = np.mean(cost)\n",
    "        return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "使用定义的Network类，可以方便的计算预测值和损失函数。需要注意的是，类中的变量$x$, $w$，$b$, $z$, $error$等均是向量。以变量$x$为例，共有两个维度，一个代表特征数量（值为13），一个代表样本数量，代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict:  [[-0.63182506]\n",
      " [-0.55793096]\n",
      " [-1.00062009]]\n",
      "loss: 0.7229825055441156\n"
     ]
    }
   ],
   "source": [
    "net = Network(13)\n",
    "# 此处可以一次性计算多个样本的预测值和损失函数\n",
    "x1 = x[0:3]\n",
    "y1 = y[0:3]\n",
    "z = net.forward(x1)\n",
    "print('predict: ', z)\n",
    "loss = net.loss(z, y1)\n",
    "print('loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.4 训练过程\n",
    "\n",
    "上述计算过程描述了如何构建神经网络，通过神经网络完成预测值和损失函数的计算。接下来介绍如何求解参数$w$和$b$的数值，这个过程也称为模型训练过程。训练过程是深度学习模型的关键要素之一，其目标是让定义的损失函数$Loss$尽可能的小，也就是说找到一个参数解$w$和$b$，使得损失函数取得极小值。\n",
    "\n",
    "我们先做一个小测试：如 **图5** 所示，基于微积分知识，求一条曲线在某个点的斜率等于函数在该点的导数值。那么大家思考下，当处于曲线的极值点时，该点的斜率是多少？\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/94f0437e6a454a0682f3b831c96a62bdaf40898af25145ec9b5b50bc80391f5c\" width=\"300\" hegiht=\"\" ></center>\n",
    "<center><br>图5：曲线斜率等于导数值</br></center>\n",
    "<br></br>\n",
    "\n",
    "这个问题并不难回答，处于曲线极值点时的斜率为0，即函数在极值点的导数为0。那么，让损失函数取极小值的$w$和$b$应该是下述方程组的解：\n",
    "$$\\frac{\\partial{L}}{\\partial{\\boldsymbol{w}}}=0,(公式5)$$\n",
    "$$\\frac{\\partial{L}}{\\partial{b}}=0,(公式6)$$\n",
    "\n",
    "其中$L$表示的是损失函数的值，$\\boldsymbol{w}$为模型权重，$b$为偏置项。$\\boldsymbol{w}$和$b$均为要学习的模型参数。\n",
    "\n",
    "把损失函数表示成矩阵的形式为\n",
    "\n",
    "$$\n",
    "L=\\frac{1}{N}||\\boldsymbol{y}-(\\boldsymbol{X}\\boldsymbol{w}+\\boldsymbol{b})||^2, (公式7)\n",
    "$$\n",
    "\n",
    "其中$\\boldsymbol{y}$为$N$个样本的标签值构成的向量，形状为$N\\times 1$；$\\boldsymbol{X}$为$N$个样本特征向量构成的矩阵，形状为$N\\times D$，$D$为数据特征长度；$\\boldsymbol{w}$为权重向量，形状为$D\\times 1$；$\\boldsymbol{b}$为所有元素都为$b$的向量，形状为$N\\times 1$。\n",
    "\n",
    "计算公式7对参数$b$的偏导数\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\boldsymbol{1}^T(\\boldsymbol{y}-(\\boldsymbol{X}\\boldsymbol{w}+\\boldsymbol{b})), (公式8)\n",
    "$$\n",
    "请注意，上述公式忽略了系数$\\frac{2}{N}$，并不影响最后结果。其中$\\boldsymbol{1}$为$N$维的全1向量。\n",
    "\n",
    "令公式8等于0，得到\n",
    "$$\n",
    "b^* = \\boldsymbol{\\bar{x}}^T\\boldsymbol{w}-\\bar{y}(公式9)\n",
    "$$\n",
    "其中$\\bar{y}=\\frac{1}{N}\\boldsymbol{1}^T\\boldsymbol{y}$为所有标签的平均值，$\\boldsymbol{\\bar{x}}=\\frac{1}{N}(\\boldsymbol{1}^T\\boldsymbol{X})^T$为所有特征向量的平均值。将$b^*$带入公式7中并对参数$\\boldsymbol{w}$求偏导得到\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\boldsymbol{w}} = (\\boldsymbol{X}-\\boldsymbol{\\bar{x}}^T)^T((\\boldsymbol{y}-\\bar{y})-(\\boldsymbol{X}-\\boldsymbol{\\bar{x}}^T)\\boldsymbol{w}) (公式10)\n",
    "$$\n",
    "\n",
    "令公式10等于0，得到最优参数\n",
    "\n",
    "$$\n",
    "\\boldsymbol{w}^*=((\\boldsymbol{X}-\\boldsymbol{\\bar{x}}^T)^T(\\boldsymbol{X}-\\boldsymbol{\\bar{x}}^T))^{-1}(\\boldsymbol{X}-\\boldsymbol{\\bar{x}}^T)^T(\\boldsymbol{y}-\\bar{y})(公式11) \\\\\n",
    "b^* = \\boldsymbol{\\bar{x}}^T\\boldsymbol{w}^*-\\bar{y}(公式12)\n",
    "$$\n",
    "\n",
    "\n",
    "将样本数据$(x, y)$带入上面的公式11和公式12中即可求解出$w$和$b$的值，但是这种方法只对线性回归这样简单的任务有效。如果模型中含有非线性变换，或者损失函数不是均方差这种简单的形式，则很难通过上式求解。为了解决这个问题，下面我们将引入更加普适的数值求解方法：梯度下降法。\n",
    "\n",
    "#### 2.4.1 梯度下降法\n",
    "\n",
    "在现实中存在大量的函数正向求解容易，但反向求解较难，被称为单向函数，这种函数在密码学中有大量的应用。密码锁的特点是可以迅速判断一个密钥是否是正确的(已知$x$，求$y$很容易)，但是即使获取到密码锁系统，也无法破解出正确得密钥（已知$y$，求$x$很难）。\n",
    "\n",
    "这种情况特别类似于一位想从山峰走到坡谷的盲人，他看不见坡谷在哪（无法逆向求解出$Loss$导数为0时的参数值），但可以伸脚探索身边的坡度（当前点的导数值，也称为梯度）。那么，求解Loss函数最小值可以这样实现：从当前的参数取值，一步步的按照下坡的方向下降，直到走到最低点。这种方法笔者称它为“盲人下坡法”。哦不，有个更正式的说法“梯度下降法”。\n",
    "\n",
    "训练的关键是找到一组$(w, b)$，使得损失函数$L$取极小值。我们先看一下损失函数$L$只随两个参数$w_5$、$w_9$变化时的简单情形，启发下寻解的思路。\n",
    "$$L=L(w_5, w_9) (公式13)$$\n",
    "这里将$w_0, w_1, ..., w_{12}$中除$w_5, w_9$之外的参数和$b$都固定下来，可以用图画出$L(w_5, w_9)$的形式，并在三维空间中画出损失函数随参数变化的曲面图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcUAAAE1CAYAAACWU/udAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzsvWuMZFd59/t71tq7qu8z02OP7cyM5449M4Dx3T4H5YQEx0qCnJMYEUMUWwoJikUkH0gkkAyRkSIuUogQgg8ocQRBISbwKvI5uZD4fTmEEwK+xTbYA8z0baanu6enp+9V3VW191rrfFh7l6vv1d3V3TPd+yc1Zqq7du2q7tr/em7/R5xzZGRkZGRkZIDa6hPIyMjIyMi4WshEMSMjIyMjIyETxYyMjIyMjIRMFDMyMjIyMhIyUczIyMjIyEjIRDEjIyMjIyMhE8WMjIyMjIyETBQzMjIyMjISMlHMyMjIyMhICFb585n9TUZGRkbGtYjU80NZpJiRkZGRkZGQiWJGRkZGRkZCJooZGRkZGRkJmShmZGRkZGQkZKKYkZGRkZGRkIliRkZGRkZGQiaKGRkZGRkZCZkoZmRkZGRkJGSimJGRkZGRkZCJYkZGRkZGRkImihkZGRkZGQmZKGZkZGRkZCRkopiRkZGRkZGQiWJGRkZGRkZCJooZGRkZGRkJmShmZGRkZGQkZKKYkZGRkZGREGz1CWRkbCbOOYwxiAhKKUTqWsadkZGxQ8hEMWNHkIphHMeUy2WccwAopVBKEQQBWuvqvzOxzMjYmUh6caiTVf1wRsZWUyuGzjlEhCiKqP27d84x/32glEJrXf3KxDIj45qnrjdvJooZ25LFxDAVtEqlUr1tqfum/83EMiNj25CJYsbOwzlHHMcYYxaIYcpKorjcsdP/Oue4fPkyWmv27t2biWVGxtVPXW/GrKaYsS1IxTCOY4BqI00jSQUu/W/6WOnjxHFMFEVz7pOJZUbGtUUmihnXNKkYnj9/nt27d9PW1rai4IjIgrToWkiPM18sa88NvFhWKpU538/EMiPj6iQTxYxrkvmRYaFQoKWlZUVRcc5x5coVRIS2tjZyudyGnWOtWNZGrWn6NYqiTCwzMq4yMlHMuKaw1lYbaIBqzVAptWz0Z61laGiI8+fP09HRAcD58+eJoogwDGltbZ3zFYbhiuey1ohzsTonrCyWIoLWmjAMM7HMyNggMlHMuCZYSgxTlhIoay0DAwNcuHCB66+/nrvuugsRwVo7pxu1WCxSLBYZHh6mWCwSxzG5XK4qkm1tbbS0tBAEG/eWWUkse3t7aWlp4frrr69+b35UqbVe8jgZGRkrk4lixlWNtbbaTQpLC8d8UbTWcvHiRfr7+9m3bx/33HNPNfqb3wyTy+XI5XLs2bOneptzbo5YDg4OUiwWMcaQz+dpbW2tRpnGGLTWG/H0q88t/UoFMD3H9MPC/G7aWrFMBTMTy4yMlclEMeOqpF4xTFFKVQWiv7+fgYEBbrzxxjliuBpEhHw+Tz6fp7Ozs3q7c45yuUyxWGRoaIhiscgrr7yCtZampqYFkWWjO2Dnn+NSkWUmlhkZayMTxYyrhtqamrUWWFkMa+87PDxMd3c3N910E/fee++GpDpFhKamJpqamqhUKsRxzMGDB3HOUSqVqpHl2NgYMzMzWGtpbm6uCmVrayvNzc1bLpbzycQyI8OTiWLGlrMeMYzjmAsXLtDf309nZ+eGieFi1KZsRYTm5maam5u57rrrqj/jnGN2drYqlpcvX2Z2dhZgUbHcSBFarVimPrF79uwhCII5zT2ZWGZsVzJRzNgy0ovxlStX0FrT2tpa9wU3iiLOnz/P8PAwBw4c4PDhw4RhuGmCWC8iQktLy4IGGWttVSynp6e5dOkSpVIJgJaWlqpQtra20tTUtCViWSwWGRgYoLW1lUqlMufn06aeTCwzthtX1xUkY0eQimEcx1hrGRsbo6mpifb29hXvW6lUOH/+PJcvX+bgwYPcf//9KKW4cOFCNcpciUZduNdjAqCUqorevn37qrdba5mZmaFQKDA5Ocng4CClUqlaM52dna3eN5/Pb7gIpc09tdRGlplYZmw3MlHM2DTmi2G9M4bgxbC3t5crV65w6NChqhimKKUWrZXVTRzDKqPMRjnj1KKUoq2tjba2tjm3G2M4d+4cSinGx8e5ePEi5XK5GmHXfuVyuYaI0FL+sPWkYRcTy8XWc2VimXG1kYlixoaz1MaK9IKYRkGLUS6X6e3tZWxsjEOHDnHixIlFm1TWK1DhP3wVc9//gT16Ys3H2Ei01uTzedra2uakYeM4rkaWo6OjXLhwgUqlQhAEc4Syra1t1V24qzVNX0ksy+Xygp9fzL1nI5uQMjJWIhPFjA1jMTFc7IK3WJRXKpXo7e1lfHycI0eOcMstt6w4krEaUZxzwR8dQb/wn1AsYI/+X3UfYyMixdUSBAEdHR1Vl56UKIqqzT0jIyP09fUtcO9J65ZL1WHXsklkMRYTy/R1qzVkABgdHa2e22KmBBkZG00mihkNp14xTFFKVQfqZ2dn6enpYWpqiiNHjnDrrbfWdTFMXWrWQvjsN3BNzeizrxNPjOF2d658p6ucMAzZvXs3u3fvnnN7rSHBpUuXlnXv2UiWMlGfmJioGiLUimX6s/PrlZlYZjSaTBQzGsZqxTBFKUW5XOb111+nUChw9OhRTp06terU3Zp8SC/0oM+8igtyuHye4Dv/SPTIBzf0MbeS1bj3pOlOEZkjlhvp3uOcq0aH82+HhZGlc25Oc09mop6xXjJRzFg3iy32rbcuVCwWuXDhAjMzM5w+fZrTp0+v6WK2XF1yOcL/+xu45hbERBCVUT9/FSoV2MDtGWtlowR4KfeeK1euMDo6yp49eygWi1y8eJFisbih7j21nrTzz7H2vynzxXL+Kq9MLDNWSyaKGWsmdXGpbZ6p98JYKBTo7u6mXC6zd+9edu3aNaeBZLWsJWpTr7+EGugFpXBhDvIKwRB8/1+J3/2bG/KY62WzL+hhGLJ371727t1bvW2+e8/o6CgzMzM459bt3rOW5p7a/9YeBxYXy2w9V8ZyZKKYsWpqdxm+9NJL3HbbbeTz+bruOz09TXd3N1EUcezYMTo7OxkdHWVkZGRd57SamqKIgHOE3/k2rrkFcIg14Awul0efeZn4Vx6COu3ldhr1uPcUCoUF7j21hgRLufdYaxsSca4klnEcV+vYly5doqOjY05zTyaWO5dMFDPqZv5i3zQ9VY8wTE1N0d3djTGGY8eOzalprTX1Wctqu0+DH/wbMjMNzoAOcU3NYC0SV2B6FPX6C9i33bvsMbb7xXItUVu97j2zs7PVWmXt11Lp00axmFhOT09X50JrxTIliyx3FpkoZqzIYmJYz4wh+G7C7u5uAI4dO7agGzI9xnojrlWlMmdnCH/4HC6fAyVIVEHiMi4IcM3NiLOE//WvlOsQxe0cKTZqJGM17j3j4+O8/vrrc6LKjXbvsdYuuYdysciy9nllYrn9yEQxY0lWWuwLfqh8MVEcHx+nu7sbrTXHjx9n165dSz7OesYpUlYTber/9T/AOZStYAlx+TyCQ0yM2AirFFKaQZ3/OfbQLes6r4ylWcy959VXX+WWW24hjmMKhcKmuPcsl7JdTRq29nllYnntkolixgJWs8tw/uD92NgY3d3dhGHILbfcUpefaSPSp3VHbSND6DdewASC1S2+lmgqoDUul8MFCrEWsWWCF/6dyjKimEWKjcdaSxiGNDc3L/jbieN4TnNPo9x71lLHXEksoyiiUqnM+X4mltcGmShmVFntYl94UxSvXLlCT08P+XyekydPLvDuXOkYmyWK+jvfwAUhEle8IAYhLt+COOvHMpz2nagqh7rSj4wO4fbetORjbme2QhSXe8wgCNi1a9eCrMN63Xsa1dwDb/5NLGainq5Hq1QqzMzMMDExwf79+zOxvMrIRHGHs97FvqVSiTNnztDR0cHp06dpbW1d9Tk0qtFmpWPIuddgsIdSaZYKiiAXkrMxWgzk8rh8K2IixEVgHIQ5gpeeI3rw0SWPmUWKW/+Y63XvMcZsuN/q/PdUHMdUKpVqPX2pyLJ2xnKpumdGY8lEcYeyXjEcGRmhp6cHYwyHDx/mwIEDaz6XzYgU40qF6J/+lvLsLM35kGYcsVhmnGDKETJTRGmNDnPoXOiHvsWh+88QFSehdWFNdLtfnK4VUVyKetx7BgYGKBQKvPzyy4saEmyUe08anS5nop6e6/zXZH5UmYllY8lEcYeRvtnGxsaI45hdu3atSgyHh4fp7e2lo6OD2267jcHBwXUv9m1EbW6pYxhjuHDhAvH/9yyHytPs3dWKC/M4JYTW0GwqELbglG8YMnEFG0eU4gqRCOJg9p/+hsl7/s85A+np67WdI8WtYqNHMua79xQKBe666y7K5XJVLPv7+5mZmZnj3pP+/hvh3mOMWVZwV9o4UuselVIrlqlgZmK5ejJR3CHM32U4PT1NqVSa8yl6ufteunSJ3t5e9uzZw+23305TUxOweanP1R7DGEN/fz8DAwMc6NzFrYUB3G7/XMVUwFpfWwxbcVjExmitULlW0JomG/thfhHazRQmF8yZsVNKkcvlqFQqjI2N0dbWRu4qtIZbD1sRKW4F6fNsamqiqalpSfeeQqHAlStX5rj3zDckqFcsVxLFpcjEcuPJRHGbs9Ri36VGKWqx1jI0NMT58+fp7OzkzjvvXOBcc7WIYhop1orhTTfdxL333kvu37+KKHytUMR3mYJvtCFCdIjLNyMOxJTBWVwYABqcI3AxNw3/hOvv+PXq4xljGBkZYWBgYEEnZHqhXGk109VOara9k9ko9x5jTEP/LuoRS/DuPS0tLXR0dFRFsnbrSCaWmShuW1Za7Ku1XnJTvbWWwcFBLly4wN69e7nrrruWjIKWO069NOJNmF6kfvSjH1XFMAgCGOxC9f8UlIKwGecsGD936fI5EEHiCLERLggh9GMa4gwIflQjyKMHzhDf9qug/VtGa01bWxvNzc2cOPHmYuIoiigUChSLRYaGhigWixhjGmKgnaVqrx7W695TKpUW7MDcqPOsfX8Vi8Xq314qlpVKZc59drpYZqK4zah3fdNikaK1losXL9Lf38++ffuWFcOU2l2IW0F6zhcuXMBa+6YYAjiH/q9/xDU1eQs3WwKdx4XNYCLEVHChxjXlkuF9i9gKVjSSy+MQBIOyETa26K4fYW5557LnE4Yhe/bsWdDcUS6Xq2KZGmjDm1FFGlk0NTUte/HZzAvTTkmfNvI51uvec+XKFUZGRujv719gSLCR7j1phFpPZFkrlul1ZCeIZSaK24S1LPZNIzxjDBcvXuTixYvccMMN3HPPPXUPPzci9bkWagX8xhtv5N577+XFF1+ck5KSH38XKVwBSzJ7GCJxDK6M0xoXeks3bAxKIBdglfZRoo0RrXE6B84BlrD7h5i3/G8g/nWtt0Gotl5Vm4JLo4pCocDU1BSDg4OUSqXqhbU2DbsV9cqdIoqbwXz3Hmst1113HR0dHdXmnuXce1JDgvX+PuI4XneDTyqW/f39fOlLX+IrX/nKus7paiMTxWuctS721VoTxzF9fX1cvHiRX/iFX5gbZdVJPbXJRmKtZWBggAsXLiwv4LPT6Nf/AxckUaCLvTjmQyCEOEKo+OF91YzflBEhDlwQgIQ+jUoEAiIBTkPQ8zzxsfuB9XfN1kYVtRhjqrWq0dFRzp8/TxRFVUeXcrlcTcFuZL1yJ4jiVqWk00YbrTUdHR0LUqkb5d6z1lrmYmI5MTFRraNuJzJRvEZZz2LfOI4ZGBjgypUr7Nq1i/vuu2/NF9f5Nm8bRa0Y7tu3b8VoVv/wf+DEJbVBgTCPExBncNZALsSpPDiD2ApOh7hck7d4I0ZwvsYoIU4rxFQQV0FfeKkqihvFUhfKs2fPks/nMcYwODi4oF7ZyJEB2Bmi2Eg3m9WwUvdpve49vb29xHFct3vPSpHiapicnNyUuuhmk4niNcZiGyvqfVNHUcSFCxe4dOkSN954I7t27eLIkSPrOp+NTp+mTT/nz5+vSwwBZPAsMnQ2SZF6s2/iChKEuCDvRzDiCuDHMhyBjySdBSU4lceJIOK8LyohhCEOBaaE7n8Fc/D2Tfc+TZt76hkZAGhpaZlzkVypXrnVbEXUdrWK4lJcTe49U1NTi269udbJRPEaIRXDS5cuYa3lhhtuqPsCF0URfX19XL58mYMHD3LffffhnOPKlSvrPq+NEsVaMbz++uu5++6766urWYP67/8HmpqSxcEVXJBHghZwMeIqOKVwuTzg/G0CTgWgQ8AgNsapAKc0BM2ItT6iVBZUSDD4Kubg7Q1/zmthqZGBtLGjWCxW1zKVSqU5tapULJd6XTc7UtwqA/KNcq1ZjrWK4lLU694zOzvLiy++SD6fX7d7z+Tk5LLbb65VMlG8ypkfGcZxTLlcruviUalU6OvrY2RkhJtvvpn777+/+ikxrUWul0bWFFO3nXQ2cu/evfWLYYL6yXPI7CRImv4MfDONq+DCpJ6Ymn8rDUFTNa0KEQQBjibA+ugRQAc4FfraojPI7Ajq8hmk4+hVOyZR29hxww03VG+vrVXNN8+eP1+5E0RxM3xPF2MzItTF3Hump6e5++67G+LeMzk5OeeD2HYhE8WrlKUW+wZBQLFYXPa+5XKZvr4+RkdHF4hhSqMuPo2sKaY1w5VmI5ciX5lCDT+PyyXzhzYGjBdDyXnhMzEEITYIELxYokLQObAWxPqZRR3gJAc6WSUlMQ6N0wGicoQXX0BOH2vI895MlqpVVSqV6sjIwMAAxWKRUqnE2NgYU1NT1QvlalxbVstWpDK3Kn26FY+bfoBbr3tPWr+cnp7m2LHl3wOlUolf/MVfpFwuE8cx733ve/nUpz5Fb28vjzzyCKOjo9x55518/etfJ5fLUS6XefTRR3n55ZfZu3cv3/zmNzl8+DAAn/nMZ3j66afRWvPFL36RBx98cENep0wUrzJWWuy73LB8qVSit7eX8fFxDh8+zIkTJzb8jbfe9GkaGabDzou55tTLwUsv4tpDJE6da/KIJLZu4vx4RZjzKVMTYYMQUS1ePF2U1BMDCFWyhDjyQhiEQOBrjM6Ci5DKNMF491UbKa6WXC5HZ2dnNaIA6OrqoqWlhTAMq5Fl2m3Y0tIyJ7JsxGzdVqVPt8q152pLFS+Xiq8Vyx/96Ef8xV/8BZVKhR/+8IecPXuWt771rdx2221VAUvJ5/N897vfpa2tjSiKeOc738mv/dqv8Zd/+Zd85CMf4ZFHHuGP/uiPePrpp3n88cd5+umn2bNnD11dXTzzzDN87GMf45vf/CZnzpzhmWee4Y033mBwcJB3v/vdnD17dkNS35koXiXUu8twMVGcnZ2lt7eXyclJDh8+zK233rppb7i1imKtn2pnZycdHR0cO3ZszfN40vMCzZUxoBOXzyPW+fqhaMg1+bEMG4EoXy/UGsEmnaeBjyRVmkY1oENsGCT/jhEAlcwt4i3imoZ+iJM713S+1wpNTU10dnYucG2pHUSfX6+sFcvVjAtsha3cVoriZhPH8Zq6zJVSc9x7jhw5wvvf/34ef/xx3vOe96CU4r//+7957bXX+LM/+7M59xWR6mxmFEVEUYSI8N3vfpdvfOMbADz22GM89dRTPP744zz77LM89dRTALz3ve/lj//4j3HO8eyzz/LII4+Qz+c5cuQIx48f54UXXuD++xvfCZ6J4haz2sW+taI4MzNDb28vU1NTHD16lJMnT276p8/V1hTTTRs9PT3s2bOnGhm+/PLLa484ywX02e9idA5RQTKMr3DaiyEuBgSX8ybm4mLEGFyQgyCHiAVbSeqGfq4RYpQDpxSimpLxDm8cjlJ+ltGU6bBDwDvWdt5XOUtFbvMH0VPSemWhUJgzLpB2QNb6gS72CT/15t1MdpIoNrq5Z3p6mjvuuIO3vOUtKz7unXfeSVdXFx/+8Ic5duwYu3fvrgr0gQMHGBgYAHwJ5eDBg8Cbqf7R0VEGBga47777qsesvU+jyURxi7DWrmmXodaacrnMT37yE4rFIkePHuXUqVNrvpisN2VVb02xdu3U7t27ueOOO6qbNtLjrFUU1ev/BEoIJAaF7zZ1ycA+ys8f4ptrnA5wKocLXCJwLuk81YmA+mgSFWBVUne0FUDjAg3i3zKCQ0nMdaWuNZ3zdmSxemW6s3N+vdJaS3Nz8xyxBLJIcQNp5Iwi1D+SobXm1VdfZWJigt/6rd/iZz/7WcPOYSPIRHETWc9iX/B737q6uqpp0uuuu25dgpbO2a33GMvhnOPy5cv09PSwa9euOWunFjuXVT/+0BuokW5cEBAH+aQxJk6aa/xYhtiKd7YJkvohSRo1zOEtbIxPj0raZeqH/gWD1QoJmvz3nQGxgMapAIsiL1OoyW7sro1vutns+mUjanwismi9crEtE4VCgSiKOHPmzByx3Egv0K1q7tmKedFGb+aYmppa1UjG7t27ede73sUPf/hDJiYmquncixcvsn//fgD2799Pf38/Bw4cII5jJicn2bt3b/X2lNr7NJpMFDeB9Yrh9PQ03d3dVCoVjhw5wuzs7Jwaz1pJU7EbcVGoVwxT1hQpxiX02ef8zKGzBMrgtPIepy7Zh5iMZZCOZQQaJ03e29QZQHAqTGqMBnFRMqPoPU8VxqdflWB1iPi5DH9s5zAK1OhrmyKKsH0MwRfbMjE9PU1/fz8333wzhUJhgRfo/JGR1dQrl2IrRLHRacx6WWtNcbnjrdQDMDIyUjUcmJ2d5bnnnuNjH/sY73rXu/j2t7/NI488wte+9jV+8zd/E4CHHnqIr33ta9x///18+9vf5pd/+ZcRER566CE+8IEP8NGPfpTBwUHOnTvHPffc07DnUksmihtI7S7DH//4x5w+fbrqLF8PU1NTdHd3E8cxx44do7OzE+cc586da8j5paLYiItLinOOkZERuru76ejo4B3veAfNzc0r3m8toqh+9m845/1JndaYMI9ToMT4wXsd+CgwEUcnKhnDSLpKcwE4hxDjHL7hhsCPZRDjIEm3ai+qGKzDR5k6h0WI9Qy2cgWmeqDj6FpesquWze4GTbsjV6pXplFlelGuFcvVDqFvxZziVoliIx+33qzF0NAQjz32GMYYrLW8733v4z3veQ+nTp3ikUce4ROf+AS33347H/zgBwH44Ac/yO/93u9x/PhxOjs7eeaZZwA4ffo073vf+zh16hRBEPDlL395w17DTBQ3gMUW+87OztZ9kZmcnKS727f7Hz16dI5LRSMvUo0evB8ZGaGnp4e2tra6xTBltaIoI+dQV36G0yHoZgSD1tZbu+kgGbOIvaepUr7zFOv/LZq4XEa7CkoFOPEfCkQsOItVGqv8WIi4GEvsI1ByOCXgHA6DA6zSmCAH468i21AUN/vxlvr7XqpeWTtfWTuEXjtXl85XLnbsrXC02S6RIqx8PXr729/OK6+8suD2o0eP8sILLyy4vampiW9961uLHuvJJ5/kySefXNuJroJMFBvIcot9gyBY8Y9yfHycnp4eRIRjx45tuIVSIwbv07Tw888/T1tbG29/+9tpaWlZ9XFEpH5RjCvorn/Hhd7QGxd5T1OV8w01GGwYIoBgAIfVOUQcpdkis6VZVJjDlB0OR6BABSEqzCe74gBirNbgAt9tKoA1OCU4NIjG4ZJ/G5yZRBV6kLbtJYybHSmuJmqrdWyZP4Q+v16ZLvqdP19pjNn0tVxb6bfaqKxQqVRathxyLZOJYgNYacs9UBXFxRgbG6O7u5sgCDhx4sSmOc8vZwSwEql3ak9PD3Ecc/fdd69JDFOUUnVHJqr733Eu8g01QRIJYlDa+oH8QKNc8rxUiAUqlVlmZ4roXDMd1+3D75GyWKWwToiNIY7KlMplImOxoggDTRAGKCXoXBMS5ACXjGfEGKWItcLm8jgHdvIVwm0kiteqzdti9UrwopDOV6b1yunpacIwZGJiYs7ISCNLCvPZDpHixMTEtvQ9hUwU18VqdhkGQTBHgJxzVTHM5XLceuuttLe3r+qx13sBWYsoOucYHR2lu7ublpYW3va2t/Haa6+t+1NjvelTGetCjf7c1/+C5mTswuBUDhvkETEIys8gClRmixRLJXTYRNve61E4nHIYF4CSpNnGosMA3ZwnJwI4nI2JLUTWUTERpjiNxSCikMBHlDoAJ2CswQLOlpDiOYLWE+t6La4WrlVRXAqtNe3t7XPeZ93d3bS3t5PL5SgWiwwPD1c3TKSm2bVi2YgIbzvUFFfbeXotkYniGljLYt90qW9thNXc3MypU6cWNBWsRCog6/0DX01NMRXxrq4umpubeetb3zpntmy9KaG6RDEuo3v/Jy7IQ9owEyT1QCKUstggh1JCuTJDYbZMEIa07elEi8VqMJIDZ3EYrPMziqJDwOGcxYnDSYCoACWWnDOEKvQpU6WxzhJHFSIbUypXKFlLVCiitSYMhNLICzTJ/iVrWBlLs1XjEYutY5pfrxwbG6vWK+ev5Frt73o7RIrbdZciZKK4Kta65R68AI2OjnLu3DlaW1vniMpqSSO89b6x6q0pppFhPp/n9OnTC0S8Eeuj6jmG6v03300qsR+jUKHvErUOG+QgCIjiMlOlMuiA9t27EeVwIkQqj2jBWYPDYiX0Q/pisC5ZFZUKplh8mlThxDfcWPHOOE5pVL6JvFbkUEQTV2jf3YE1lji2VGzElb7vM1HoXDBG0NbW1vBGh41ku0WKi7GUEK9UrywUCkxPT3Pp0iVmZ2erVmjp7zldybXY89kOkeLk5OS23KUImSjWxXq23KfzegMDA7S0tKy5EaWWtD653gaBldKntend5SLaRnSxriSKMvpT1GSvH4+QPILx0Z4OQUOlUqIUVygLdOzejWhwKN+disO5GItGghwOhzXJkmEd4Jz2huEuwmmFI8Q7iTusNYnDjcYSIMoBDusMTiBWAagcSkOYdwTO0tRe5MTJX8UYW232GB4epru7G2PMnPU8G715Yj1sxUjG1e5oU1uv3LdvX/X22nrl2NgYFy5coFKpEATBgvnKrWq0aWSkuF0XDEMmisuMt7d+AAAgAElEQVSyni33qeF1X18fu3bt4uabbyYIgnULIqyvQaae44yPj9PV1UUYhpw8eXLF9G4juliX7T6NCqiB72PDHOK8eFmdA4TIVCgUi1gUkm+ipSUPYZBEfgZnY1ABqDw2TbkqhXUhaElSptanR3Ueh+Ak8UpN5hFRABbrvECKBFj/EzgtxJIKpwKnQWB25ie0tN626BhBuVymUChUPULnd0amX5vdFTmfnRwprpbF6pXgTbDTD0aXLl2iUCgwOztLLpejVCrNma/caKFsdKSY1RR3EKkY9vX1sXfvXlpaWup+s1prq2K4Z8+eqpPL0NBQde3OemmkKFYqleq/a8VwNY0/jUqfLvWc5Py/4bA4Yj8TKCEmKlOYLRJZob2jAx1qpqeLmCBEa4WzFawKIMiDWJzzc4UuyCOAlRjrHGi/PNhaC8qAaCDE4byLjbNJd2vo748DsVgcziliUaBzOPHfjcUCCls+T1PzKZSa28VYu8uudj1PbaQxOjrK+fPn5yz/bWtro1KpNGyutF42WxSv9khxtSxWr+zp6SGfz9PU1FSNLNMdqfP9YBtZm27kc52amuLQoUMNOdbVRiaKNcyPDAuFAu3t7XXV/qy1DA4OVpfkzt8LuNxIxmpplCimQjQxMUFXVxda61V3wabHaYQoRlG04HY3/BLMDGOV4CSPMRGF2RliA61t7bTmtI/udIjTJRwWoxToJhCHcZG3edM5LICNcEpjJfT2e2n9MND4t4MD5arONajA/5yyvjEHhSRC5wCUo0IMEiBoRMAKxBimZl9ld+vddT3/pSKNtNkjrWFNTU3R09OzoH7V1NTUcAHb7OF9a+2m11y3Yng/NRfo7OycU6+01lbnK+fXK+ev5NrqLEJWU9zmLLXYNwiCRS/U8++bboy//vrrl9wYP38kYz00ShRLpRIXL15kamqKt7zlLWvuJtuomqKdvQKjL2ODEGstxcIEFeNoaeugNRfgxGIlV60HWi1YFWKVd5yxSiPkfYqUGCtJTVEczsRYrbBotCgf/TlvDycEWHGIWC+qWuOcRrQCHEYc1lkQTYxGgrwXTHHYagpQU7KXMbaEVmsfV6k10y6Xy+zdu7fqI7nYPsPa9Gtra+u6RGYnpE+vJpu3VPxaW1sX1CuLxSLFYpHR0dFl65Wb9aEiG8nYpqy05T4MwyWjO2MMAwMD9Pf3s2/fPu65555lB37TkYxGsN6oc3Jykq6uLqIoYvfu3dx2223rOp9G1BTni6KzBjPwHA5hZmaaUhTR0trBrlyAUyRbKgSIk0ixCacjImURyfl1UInQWaVRBKDAYHwDTuBrkmCIxPrZxmpUaLwXqmiUyuEErIuxyqFEcFYgCHyoqIUIgygBpyAIktojWCLGyq9xffO963ptaknr2unF84Ybbqh+L47jalSZ1q/Sxp5aoay3HJA12mwMqxVirTUdHR0LPrTWruQaGhqiWCzOaeSqrVeuZgFBPWQ1xW1GvYt9FxMfYwz9/f0MDAxw4403riiGyx1rraw1UkzFUEQ4fvw4zrmGLOpsRPp0fqNNZfC7zE5dplwp0dTczp5du5NoMPCChQGU342Y7EGMlSAqJNaCuAhUCAQ4Z5JmGL8xw2EwNsaJ+KXEDqxyGGtAab9lQ3wTTozBiYAKEBRWwGnjH1MLRsTXFPF2b1a8IOIUohUzboyKnSKnNn6mKwiCReftSqVSVSwvX77MzMzMVZmS24pIcauaexqRsg3DkD179szxRq5t5Eojy5mZmertfX191d/3elLuU1NTcx53O7GjRHG1W+6DIKg2x8RxTH9/P4ODg9x0003ce++9q0pVNFoUV0rr1jI1NUVXVxfOOY4fP179hDc9Pd2w2mQj0qdpTXeo6wfkJl8h39xKR9s+UD6+c7oZsL6LVEJQXrisEkTyEFWwYnA6jxBiMd6FRimchEk6NMaJwpJ6mvpUqJMAFYQY58c3EI0o7QUxsXYzzvrVUSqtRwouUFh82tU6hRKFE8G34vgGn5HKT9jf9L+v+3VeCyJCc3Mzzc3NCyzPalNy58+fp1KpkM/nq0IZRdGm1hW3alRhK1K2G1XHXKqRa3Z2ljNnztDc3Mz09DRDQ0OUSqU1fzjKRjK2AeknJah/l2EQBJTLZXp6ehgcHOTAgQOrFsOURtUB02OVSqUVf65WDI8dO7bgj7iRXayN6IqcmJjg5ef/gxMdZ2m+bh9+vtD4QXuVzBGqENB+3ELwTTQIsYuwohDxK6IqLkYIEK2w4rA29k0y2qdDjat4D1O0n31MO0cFHH7o2kePFpRCicY5SRxvHMaBIMROYZWfa3TKEaWvg/iOVVFQlmlm7Agtav07MBvFYim51MUlHSEol8u89tprANWtE+nXRiz+3YqobSvYiuF9ay1NTU1z0u3puaS/7/ldz7WztC0tLXOue8VicdVOXNcKO0YUFzPpXo4oihgeHmZ4eJjjx49z//33r+sPuRHRVMpKYjY9PU1XVxfGGI4fP77kJ7pGndN6aorWWi5evEhfXx9hGPC2gxMQNXuXGR2CaKxNU5h531zjLKICHAJicOIH+l0QY8RgNYhr8mlSMT71qQIcltgZL3A6rHaPGr9MEVEKEi9TlEJEgVa+cikOkwitctqPawh+TlFZQCOiQWs/+A/Eab0TGI5/xpHc1SOKi1Hr4tLZ2cnly5e54447AOY09gwMDFAulwmCYM6F81pr7NkqtkIUjTGL/m6WqlfWfjgaHBys1iv/+Z//mampKQB++tOfcssttyxZPurv7+fRRx9leHgYEeFDH/oQTzzxBE899RR/9Vd/Vc1cfPrTn+bXf/3XAfjMZz7D008/jdaaL37xizz44IMAfOc73+GJJ57AGMMf/MEf8PGPf7xhr818dowoQn2bGCqVCufPn+fy5cvs27ePvXv3NmQep5Fv9qU6WWvF8NixYyvm/Bs92rEanHMMDg7S19fHDTfcwNve9jbG+v8nzhSxYn0KNBFAFYbJOISfIxStsRgMFpEQJWCcwYqA0jgUMRGowO9LxBHZ2IucymGVb4KxDhReDA22enylc17wfGLUn7AESBD6OmOSchW0Nw1QOT+3KGCcH+tHxEeKIjgsZZllzF2gU25e9+u92SzV2FM7mD6/0aM2qqx31m4r0qebPXaSstniH8fxqoQ4l8uRy+UW1Cuvu+46XnzxRb7//e/zmc98hrNnz+Kc49Of/jS/+qu/OucYQRDw+c9/njvuuIPp6WnuvPNOHnjgAQA+8pGP8Kd/+qdzfv7MmTM888wzvPHGGwwODvLud7+bs2fPAvDhD3+Y5557jgMHDnD33Xfz0EMPcerUqbW+HMuyo0RxOSqVCr29vVy5coVDhw5x//33U6lUeOONN7b61BYwv5O1UChUu0mPHz9edwG8kaJYb40zdfrp7e1l79693H333eRyOaZGf0oTgxjd6W3XMJhEAGO84InkQAwGg0jgh/AxGEmEScdYLJESlMrjgMjGvos0yAM+2gPt07DOd6MaMSilccnbIVIWa0FQiGhfw1QW4wQUKAJECRawSfep767xpuGIA4QYm0oqDhixfexWB1By9Vm6LcZKkdtSRtq1jT3Dw8MLZu3Sr/kRxk6JFLeC1YriYogIR48e5dChQ3z5y1/m7/7u7wB/7VysX+Kmm27ipptuAqC9vZ2TJ08u29j37LPP8sgjj5DP5zly5AjHjx+vLiI+fvw4R4/6tWyPPPIIzz77bCaKjcB3FM79ZFgul+nt7WVsbIxDhw5x4sSJ6qfVRjbHNJJUzGrF8NixY3R2dq7qOKvZYbjS+ayUhnXOMTIyQnd3N7t3755jbmCiKaLpF4hF+2hLSPxNrU9/inediW0EaJTSPk3qJNlooTA2worgxO9XjJM1Ty7wi4er/8avlDIu9gP6BChRGOX8cxCFuMCPVCgfPXqJC3y6WXz06JtzFMb56NOJS8wCXCKK4EVSJSlUR5mIQX7OAU6u+zXfLFYrUis19qTWdr29vVX/3lQk05r/ZrGTRHip9OlamJ6enmMykUaVy9HX18crr7zCvffeyw9+8AO+9KUv8bd/+7fcddddfP7zn2fPnj0MDAxw3333Ve9z4MCBqogePHhwzu3PP/98Q57LYuwoUaylVCrR29vL+Pg4R44c4ZZbblnwBmlkcww0ZsUSeCEfHx/nzJkzHDt2bI4rxlawXPq0dv9ia2sr73jHO2hubq75vqFw5f/1HZs6caMR7WuKaQTofE3Rm3mTRGbeu9Ti9yGKzoO2Xv5EJQ00PhLEeXHywprMKRIgWjDOYsQm9UAFSCK4+N2J+BqhFeMH9iHpTPW/Q6uESGy1accLJ4klnE/BOoE0khznCjdQJiTPTmK5xp40qiwWi/z0pz9ddOPERjT2bFW361bQiEgxZbVrowqFAg8//DBf+MIX6Ojo4PHHH+eTn/wkIsInP/lJ/uRP/oS/+Zu/aci5NYIdJ4qzs7P09PQwNTXFkSNHuPXWW5d8szX6TZimPdc6D1YsFunu7mZmZoZcLsfdd999VXzSXaphZ3x8nHPnztHU1LTkqqzC+H9hzBRWOWIXYpVgXYxoXws0+LVOSmkMsR97UPlkH6L1P+fwwxciWBfixAubF0ZvBG6djxRVkPMrwFzsI0DRiPgI0OJwuGrXqBW/DcMLGn6mMRm3MGIRJ8QAOkQlMaVJxjd8b6oCAgR/H4CKxPRwhlvc7Rv167hmmL+eaWpqiuPHj5PP56s+sBMTE1y8eLHa2DN/Fdd6LvQ7qYZpjJljO7kepqam6hbFKIp4+OGH+d3f/V1++7d/G2BOXfoP//APec973gPA/v376e/vr37v4sWL7N+/H2DJ2zeCHSWK58+fZ2BggCNHjnDq1KlNF5S1rnxKxXB2drbaQPPCCy9cFYIIC0VxcnKSc+fOobXm5MmTS3qpzhR+Rrl0HtKBfF0gRqF1mIihgMvhJE1/5kA5ImcgicpMIlBKApw2xCYiViD4OcbY+Y0XokKc8nU+CxjrnWGckkQgfcOOk3QSUrwUqiTdSrJn0QmI8s42onBaeWMAAHQihZIIrO9aFfzdHIJCU3AFppikg+3pCLJWateypaJXy3wHl0KhUPURnb+K62pt7NkKWzlobKQ4MTFR14yic44PfvCDnDx5ko9+9KPV24eGhqq1xn/8x3/krW99KwAPPfQQH/jAB/joRz/K4OAg586d45577sE5x7lz5+jt7WX//v0888wzfOMb32jIc1mMHSWK+/fv58CBA6sWk0bVHlZbo5yZmalGhmmaND2PrfrEuRhpmjntfrXWcuLEiWVtoCrlyxQKr3jRwRK7mBj9pm2a5JJ5wNhHc0p7yzZRgJ9bjNNuUQkwGGItWKMRHRBZ4wVK+cgxdjGCYKx4+7NkH6IfQ9QEYYSEGhVoAuUXEDvASDLfmDbHKIXDUUlkz0htNOiwiYCK4NdRAWmk6cf5wShHFz/nDnPPxv1SrkFW2pKxlINLaqK92saerbKV26oFw43cpViPxdsPfvADvv71r/O2t72Nd7zjHYAfv/j7v/97Xn31VUSEw4cP85WvfAWA06dP8773vY9Tp04RBAFf/vKXq6/Vl770JR588EGMMfz+7/8+p0+fbshzWYwdJYq5XG7VjTPpBb8Rf1D11ihnZmbo6emhUChw7Ngxrrvuug2NCtcr+uVymdHRUUqlUl3dr8bMMjH5nzjRbxp36yasLmBEEAl8gw0KpfMYsdgkGkRBbL1QQoBTLhmD0H6EQifTgSpEhERgfUNMsTBDbCOa21pp1jkvZOLHNUxsiWNLVC5h3AwoQemAQAcEOkSHfpjfiUU55Uc0nMKhiMQ36PhYUHsfVfyaKR95pviZR0FRwTCoBvkF+wt1v85X0wehjcBau6bGnnTpbz2NPfl8viqWWutt5WazHI1cMFzvhox3vvOdi/7NpjOJi/Hkk0/y5JNPLnqf5e7XSHaUKK6FNLprxB/USpHi7Ows3d3dVTE8ffr0hr9p1yP66flOTU3R3NzM3XevvCbJOcuVyf/Autg3wagAEcE4g0FjlcY5g6Qdp6nlGtqLIwpR+WRThUElXaExBqsdJtagA1/3s36WsFCcITIVmlqaaQl3+RESF2OcRYlG6yZ06Dc2BoBCgYPIxFTiiHJUIp61OAeB1ugghJwQ6NB7n0rgU6biz8MlKVqNAnxkqUicdZKqpRNDL+fZZ28goP6L5NWSMt8IGtkNWk9jz/j4ONPT07z44ouLLnjeiNd6q0SxkY+7nTdkwA4TxbX8kafro5qa1r7+p/ZYi4li2vwzPT3N0aNHN0UMU9Zi0VYqlejp6WFycpJjx45x/Pjxuuc5x6efJzbTWOUQcjgxWMEv+9WS1AdDrPJ9pUrlsc4mjTM5REOcNL+IhNgkxap0iDMWq2LiZK3TTKlEOSqTa26mPb/Xp0+VJXKxd6xB+1lDrK/3ia7WA61yiM6Rz+XIJ6JlxRAZg4kNUWSYnS0RG8vY1CQ6CNFaEwQhWif1ScR3wDoSsfRxJM6LpRXh57qH0+bEql7/7cpGpzPnN/bs3r2bS5cuceLEiWpjz/j4OP39/XNWM9V2wa5XWLZLpHjzzdeeCUW97ChRXAvLrY9aLfOdaEqlUjXSOnbs2KqbfxpxEVnN2ElqcDA6OsrRo0c5efIkIkIURXUdY2rmp8xW+hOvUYVxxqchlRATEzu/jaLi/PZ6pTQxNnGH8Q03NhFDJxC5GHEapcNkaN8QO2GmHDFTmiJoCWlv3evrkWKJnU0iS8FULd8UkiwXjvEdp4L4zlUASWqX+J8LdIAOIAQMjsmJSdrbdxHHltgYZkplYlvE4tA6l/y8ItT55DwS5xtx4BzDepSD9iY63Pb0kVwNW7WqaqXGnkKhwMDAAMVisdrYUyuWq9k2kUWKVz+ZKK7ARmy3qI20jh49uqZO2FRg1yuK9Vi0RVFEX18fly9f5vDhw7zlLW+Zc771eKjOVAaYLL2erHOyxDiQEBGHEQuSw2lFJPjUqXLEziISVNOoSjQiAbEkA/USJk0vBhFNVIFSVMHlNLs6fVOSFYfxSVd0EllGzs8OCmHSTOMbYLz4+lnFCgZEUE755+q816kVBy51uxE/86gDJLAEBAQ0oZyvKcbGEMeGUhQxXZ7GGosoIdQhEgTkdIgEmjeCPu6P3rqu3+N2YTNFcaX3z3KNPYVCgenpaS5dulRt7KmNKBdr7IGtm41s5AeOTBS3EWtNnzZKFJ1zDA0NMTQ0xJEjR6qR1lpII7x6djnWc5zFiOOYCxcuMDQ0xM0338z999+/6Bt6JVEsxxOMFl/AEWKVqXZzxok9mt90b3yTjc5hiN+8XXkBVCqHdQ5L5MctRPz9nVCJDMXZaQgElW+itWUXLhFVhZ9DNOIwzqDQoILEwzRt2PFCaHGYJFKEtE6YzDyKJOckyeop56PPxO4NNMolIpxGgsrXH1toSlZQCcYajDFEccR0aZbYxozKJGasxM3xvjk1rYyNZS0CVdvYs2/fvurtcRwv2dhTK5SNHI3YKrbz2ijYYaK4Fhohiql7zsjICG1tbdx+++3r/tS2kWufahcp79+/n/vuu2/ZN/Jyz8XYEsPFH+AQYokhMfCuWOOH4f0uC4QApxQRMVp54+2IRMREYZTFWb830Yjfr2hjmC5MIaHQsns3WKE4UyASL4ZKAoz4rYoahZJ8Ejn6+p44v6A4HaNQKDQahyT3S31pNCA4hEiM33vhkhSrU1jRPrUrvolGORDnPVAdyq+gSowB0ApRAblcjrzTSPL4hRaHDKo563tqLdDa29u3fffpZtPI8YggCNi1a9ecCKp24W+6mmlychLnXHX1Uu0Ow42Mkht57EwUtxFrjRTX6smY7mJMreRuvPFGBgcHG/IHuhEbLqy1DAwMcOHChTUtUp6Pc4bBmf8kdhWsWC98AhE22URhMQ40IUZZrPiI0IjFOodSgU9ZJr2cKIgkxhiYKczglKN5Vxs5nceJUKZMLA4kqAqdQ9CiscKbkakEYCEWf2TfchNicMTK4lw6gu/Tpk45P+bhfPTqRzn8/WMFUTKWL06SlK1NrN4EJzZJySqUk+rSYov4/Y142SUUhg9WuCs6kbx2b3ZKTk9Pc+XKFcbHxxkfH2fXrl3Lzt9l1MdmNPbMX/jb39+PUoqOjo4FjT1hGM4RykY09gANW1mXUu9IxrXKjhLFtRCGIYVCYVX3qTUZr7WSKxQKDa1PNipSjOOYgYEB+vr62LdvH/fcc09DLrQDsz+iYovE4lCS84uYBJTkvAuMaMTia4L4mb+Ki/zAvVJEymGtoJVPr1aMYWa6hHGGlrYW8mFTEoH6dU1CDqrCqLwYQrU+6IWPZO7RIU77TRuSdqCCuDBJmzriZAg/Ta+6mhqlF0GNOJWcu0+9uuTn/Sxj6g0uyfeSaFHAobwzTvJ9gEu6wGUzzT7bvqBTEuDs2bNcd911BEGwZJquvb19Va4uO5mtcrTJ5/O0t7cvcHpKPwQVi8U5jT0tLS1zjAhW09iTPmajOk/T82xEN/7Vyo4TxcU2ZSzHatKntd2Zhw8fXmAy3sj6ZCOO5ZxjZmaGgYEBbrzxxuoap0ZwqfQys/EV39AiobdpQ6OUJGnR0C/9VdbXCHEYnYxZJEInBKAdZRczM10mjiOa2pppzeVR+E7O2CUCJRqjIiKF724VIfIbEv3QP96n1KdJk7RlOvgvCu2Sucg0ZSpJCtQnS4nF4dI0LL7BxypHrCESHymqxOTNe7Z6v1MnkqyPSr+X2McJyQ5GSR4TwPHf4UUeLN+a/tQC0ihj/vxdmqabnp6e4+pSG1G2tbU19OJ4rWOt3fTXYzkhzuVydHZ2ztl2M7+xZ2hoiFKphNZ6gQ/sUs+lkZ2nOyGFn71DVqAe8akVw/nrp2pp5NaN9Ryrdo2TUoqbb76ZI0eONOS8AC6X32AyuuibWKr+oyGIoyKgXJIudUnUiPGb7FFE2g/cKwkxGArFGcqliNb2Vpo6mtHiBTRKOk6VhEn3qHfGwWpvKg5oNJq0oYfEeVT7cQwFWKl2oMbia3vO+fs55/cqOhziVHJvhYWayFThnKpu4Yi9P4+PhvFbMVJxc6nFm1gfJfobkzStAxQijhKGV4Mhbo/rd7pZLE0Hc5s/hoeH6e7unrMEOI0qVxt5bBe2KlJcjUDV09hT+7tNMwa1PrCNnFFMz2k7/73sOFFsZKRYqVTo6+tjZGSEw4cPLymG9RxrtaxVFEdHR+nq6qqucRodHW2YUDvnGI96GY+6k72GyYZ6lfPuNfhUo1FJ64sSyi5G68A3pIQ+hSpKMzVboDRbprmplV3Xt3qhJOkiJRnbEKHifBpWS4CzMZG2ScuMVFOlPk0qyX2TiNX5DlGTRGwKjTifSi2LtwMXp1CJZdubox0Kam53ShEluxSlphbp64lpg03atyool4pk4oea1B5TA3GLpTsY46jpZJdbmKJazcVoqeaP5SKP2q/tzrUgiktRT2PPyMgIs7Oz1ff3hQsX1t3dnNY+tzM7ThRXS+poU0utGB46dGjJUYX5NPLT1WpFcXx8nK6uLnK53Jw1TlprKpXKus9HRJiILnI5egOHximbCFzo9xOi/aqlZKUTacNN0lhjnMVaRbFUYbY8RVO+hV1725ManMaJUEpqj0p00kSDrz/iqBB7oxjr64gGl9TsdLWT1Euyn1WsuGTUw2kk8Sc1yrfdaOfTrSYRtaqgJV2oVhxR0mlqnYDTiNM1nazgxKCdH/dIY0E/CwmmOicpSbSMjzjxHxucgx+FgzxYObru38tiv6elIo/0YppuoCgWi7z++utzhHIj9hpuFdeyKC7GUhmDkZERrly5QhiGc7qb08ae9KulpWXFc1vtLsVrkR0nimsdkoe5Q+yrEcONQGtdV1fs1NQU586dQynFrbfeuqC436iUrmuZYSB6FZXOFjqFJJ6kkqROo8SqzYpJbNUCTLKmqVKpUHERzmp277kOp8E4l6RH0y7RACdChSTFiq/HeflNHkODlbT5BSq1qU78QuBE+kgNwdNoLXC+AzUSsGLBSlKP9CJbdddx4p10EAyKijiUJJ2uzlu52epoh0tSp8433TjvjCNJskIlKdt0DMQl0eooZX6sr/B28+bFbSPrOUEQsHv37mpXoXOOF198kSNHjlAoFJicnGRgYGDOXsNG2p9tRa1qKzZWbIWjjXOO5ubm6rqmlFof2P7+fmZmZqqNPbW/29r0+nYf3IcdKIqrJU23dnV1MTw8vOwQ+2YSBAHFYnHJ7xcKBc6dO4cxZtk1TmvxPl3wWPEobt8wjt1+ntBpRAsRNnGRSfcT+v/v21ICKsSYimG6OIPk/EaM1pZ2Yu1dQlFS7UxVopNdiC6J2NIGlzdToQ5wxneFRsqLoXIBDvwmC0DQaIRY+eg0FTcvoC6ZadQoq3Hp2AUgzp+zOMHhiJO0q1X+MbQTbDKqYaorhd9MqaY7NBCpzj8avCKmqVSs39OoHOCEn+lxjtgO2t3mD/Kna5xaW1tpbW2dsxh2Mfsz59y6TLV3ym7DrRDFpWqKizX2WGur6fXJyUkGBwcplUoopfjrv/5r9u7dizFm2UXD/f39PProowwPDyMifOhDH+KJJ55gbGyM3/md36Gvr4/Dhw/zD//wD+zZswfnHE888QT/8i//QktLC1/96le54447APja177Gn//5nwPwiU98gscee2wDXqG5ZKK4DHEc09fXR7FYJJfLNUwMG2G5tFSENzMzQ1dXF6VSiRMnTqy4xqkem7flKNoJzpuX/IYIMX7Zr5KqDVssBlyQrHGKUWicKGbiCjNTM0igaNnTgdYBU5NTROIIJUzExaGSP9GoJjo0GJwI2vl5xEqSklXi1zuVlUO5VNQcVmoiu0S0sIJYv4qqUp1WTI8pxCqNXDRBMl+Y1gdtUhvUCFiV1C59M0061J8uiUp3KFpxpMGQc0naNgnVGwYAACAASURBVEnxCuBcWoNU1TRshOW7wSC/GR1e8+9nrSz3N7qY/Zm1dlFT7fkputbW1kXfQ5vte5qe81bsU9wKIa630Wa5D0IPP/wwzz33HOfPn+c3fuM3mJ6e5tChQ3zrW9+aU6MMgoDPf/7z3HHHHUxPT3PnnXfywAMP8NWvfpVf+ZVf4eMf/zif/exn+exnP8vnPvc5/vVf/5Vz585x7tw5nn/+eR5//HGef/55xsbG+NSnPsVLL72EiHDnnXfy0EMPrXhNWy87ThTreePFccz58+e5dOkSBw8epLW1lYMHDzZ06H693WDzRbF27dTx48fnLCRezXFWw4ydpDd6EecEG/iBBVGSRIXqzbEKBZGzaMlRMRHTxWmcFVp2tRGEucSGzWCUwjrl5wwTfxmbRIeawNcOxVaFsqx8JKbxdckKDuN87c830Ti0e7OuGOFXOGkXEIsjVt6+TdKtFUCsfNymknEMP+doq2MVb6ZHHRVxRAoiIHS+TupH8/1IicMLoZ9pTKLFtNFG/AC/39KBd79xb25cJPnfCYn5sR7n7WZjLwTrZSlT7doU3YULF5iZmQFYsABYRHaEQG3FY8ZxvO65wjAMeeCBBygUCtxwww089dRTOOfo7+9f0LRz0003VVO17e3tnDx5koGBAZ599lm+973vAfDYY4/xS7/0S3zuc5/j2Wef5dFHH0VEuO+++5iYmGBoaIjvfe97PPDAA9VI9oEHHuA73/kO73//+9f1XFZix4nictSK4YEDB6qR4eDgYMPqD43az5iKWeqaMzExsaYdjGsVxVk7TXf8AqkhtyOs+oaKaG/Xht9mEeMQqxkvTVKJDO1t7ah8kDTcKCouTtpRfNpTkwf8PCPinWKiWqEUQ4Sr7iGMEgFSaJzyNT4f9alkx2GcpDC9sJaVwzp/TiDEaUONS2zenP+3EQsumUB0iWiSdLQmt73ZoSpJetfLonLewQZ5s8nG1oircwoRkvRq0nSTjGbgfHQLDuPgJT3KIbO53aCNityWStGl4wRp40e5XCaKIrq6uqrjIs3NzRsqIFtlzn0tLzaurSmKyIorpPr6+njllVe49957GR4erorljTfeyPDwMAADAwMcPHiwep8DBw4wMDCw5O0bzY4TxcX+IGuNrw8cOLDA6zMMQ6IoaqgorhdrLRMTE7z88stzXHNWSz0bLuYzY6foMi8lImEIVAgKjBMCCSiLQRMSi2+WmSnMUjYVWlraaO3IJ04xOhESg5KcH+TXjsAF1RSnJl0Jhd9WkUSf6Qyij+ws2vkN6hGOSBTKBVhxyWiF85Ek1jfdOEGsX/8bie8CxflKY7o+yoh/PJVGcpKmb32TTVqntEAkQsUlk4fWH8cJWOeqKVfBP4ZDo53CJQJvnU22b9jkeA5xPkXrc61eMK0V/lcwzOlV/3bXzkamM5VSCxxdZmdn+fnPf86ePXuqtnazs7OIyIKoslEjAVsliptNo3cp7t+/v66fLRQKPPzww3zhC19YUH+8mmcdd5wo1hLHcdX4ejExTGm0E816anhpnfPSpUvVdMN63tirjRRn7CTnzMs4DDEWnY5UqACHUFEWRUjk/OB9pRSRb29id9N1OHG++YbAR2I4AnwHqhFvsRbz/7P37jGaXle552/tvd/vVlXd7osdt9tOHMchxAESQuKQGWACYiZzMkzQIBECHDHIwDlCgCICYiLuaKREM3NAEUo0QkzECRyhTKQRJ5ooJyfAEBgxCbESQkiC4/albffFdl+rq+q7vO/ee80fa71fdfta3V22k7S3ZNmu+m51e59vPeu5KMH3hcVBKmp0n6BPYNJPh4FEQxVZ7itDheyfTw6kHd5+oeZ1rEGZLRaUUhnGARI9Uk6VoN6EQb8rVIJC9D8VxQREiuBGRd8PBrN6oD18+hTY11H1IIpHCVz0RsQBuBfjBPUUHLeKCMopOr66r3DjFf+kL+883zs+VSWlxIEDB5axdmBTzjO1T1xNrN21BIq7OSm++tWvftbb9TvIn/zJn+RHfuRHAHjJS17CyZMnOXToECdPnlxagg4fPswjjzyyvO+xY8c4fPgwhw8fXtKt/cff8pa37MrX8UznmgTFUgoPP/wwJ06c4PDhw7z5zW9+xl+a3TbdX8ljlVJ46KGHOHnyJLfccgtvetOb+PznP/+8lgxv1vMcKV/AEmWUKMlVnWZvWITKiMSF+ZTpdM5kMmHPwVViMNoyY+HfHdU9hIFWCoFEUCVbaQai0ac8o0uLA42JdJTWPYBBhAXqcdu+c/RknIhQxJNtnAKtAtPcsrUxNeCNDRvdlEXJiApNHBCbBCnQxIh4fZQoZI9jC1SPf7NE1IpYSg+4yCYQVVEJy+lPUaoNfq5itT2m9riK0bJmzQAjWoPvI+2OChxZq3xHzjxfgvjnGxSf6vlijE8Zazefz5e7yiuNtbtWQHE3s0930pChqvzMz/wMr371q3n3u9+9/Pjb3/52PvzhD/Oe97yHD3/4w/zwD//w8uMf+MAHeOc738k//MM/sHfvXg4dOsRb3/pWfv3Xf51z584B8KlPfYr3ve99u/J1PNO55kCxbVs++9nPctNNNz1rJVJ/XsjM0lorjzzyyPLdU/+aVXVX0u93Sp9eqGc4Ur8IQCeFRHJBjOeMRpBZYXPrLE0zYt/+A/Ypj1WzicdAVNwE34pZNlRhIV4TRbxETGOgadNYXnoBm2VnYdS4LWzRAMX9iPRTn3UfLkrH1tYU7ZTVtTWkieRaGRJYAapCWzvarlJnC2bZ9nk0QogNg5RIMSEhWscilapC9qg3A0oXz4hcKrLB3jYUp0QzGMWKgwH9TrGfMFnKbXq/pahQNfA34w1uZb8pX5/D83z7Bi8HoESE8XjMeDzm+uuvX378SmLtnk/gr7W+IJThbk6KO2nI+Pu//3v+7M/+jG//9m/nda97HQDvfe97ec973sM73vEOPvShD/Gyl72Mj370owC87W1v4xOf+AS33347k8mEP/mTPwFg//79/NZv/RZvfOMbAfjt3/7tS3bTz9W55kBxOBzuGAz780LQpxfXON14441PqnHarT+unTzOufo499d/RrA+wSCN7/bMV7e5mDGbdqRhYs91+4kx2a7QqVEV8xJ2DnZ9o0SgoaiSpTrQtWRRGk+dMfGL1U21Uq15gsA8mHAmaXRBjKlMBaFEyyANNYFYb+PWbEpedKyurBHWBtaQUfH9XvBpV0hxSBOFMmLbSpELmpWuzWyV1gLEQyCFSBMatAilCho9zk0V/HWGJVVq9hK1GBvEFah9hg1u3u8zULNbRPQiv6OKkFU4R+FTcpZ/pQd4Ls8LQZ9e7fNdbqzdfD7n+PHjy6nyufYPvhAeRdjdkIKdTIrf8z3f87Rvqv76r//6SR8TET74wQ8+5e3vuusu7rrrrst/oVdxrjlQFJHL/gV5qqi3Kz3PRp+qKidPnuTo0aNcf/31u1bjdKXn8Xqco/ovQKQES6fJUkEDXZc5P92goWG4MiaFBDG69cEmSXGFaE+T9v7DqNHASIRIIodKlUCowRsphOjTYQESgSIWExcd8BaiPkWGZSSbViVoQyeZzemMdrpgdWWV8f41m+BUiNVuP6cQcHDsd4jLvJtAVJAUKAkSQwZq+75SlS5nFiWzqJXFxgaJQEyRGBOxaSBZlupSLqNClG0AxAMHasUrpXpAUKrKEjAFlv/fK13v1Rm3M+OVjJ+zn/sLAYrPBZX5TLF2d9999/LvbXNz80lpLrsda/d8g2LpfcG7+LP8Zu9ShGsQFOHyQ8GbprniouEnnpQSs9nsSR9XVR577DEeeOAB9u/fzxve8IZdq3G60nO8HuUR7kdI5FjM7xega5WtzQ1qhL2r+6GB+XRBERO5JAe8INY20VJpHExtFooXWSzCkvrsLRkNZsNYBIOUoHEZ15a0IQdLmUmeLbpdFhypElhvt5huzRgPx+w9cAAN5l9M3nax8Di1i2lZqx/2+De1MPGMqUOjetmUKB1CjYqEhjEDSleZjCcQA/OS6XJhPp+Tc4VaSbExsEwNIeHBBi6kcc+jqnqqjoGmEbH4NrK3ZgSKmieyonySs9zEjazw3FxkXwj69PkE4ZQSKSVuvvnm5cf6KrXnKtbu+UzQ2eQClY497C6jsLm5+WL26Yvn2SPVLvexLp4UVZXTp09z//33s2fPHl7/+tdfltH2uXpH/4B+jUc5ZtVLoQKJrhQ2LmxQizLaM2HcjJcVTho639FF2mBilKosqdEsPgm6uCZilUsLqU5jWoqMFnEVq3qgti6p04g1UhQMEM0S4opUYKttWVQltsr+6/ZTYrD8VG0I9MrQStREwOqnspihvhe/FFErRa4WHq4ueul6IFVBPKGnqlkyFkASYZiGSHJAUxPhLHJHKZV511Fnc7v4a6RpEiEmUpOIMVAIPqWK319Zdi/6drGoTbpKYAb8X3qGn5IbnurHtyvnG40+vdrT2z+eKdbu2LFjy2vB5cbaPV9Zqxc4x1ke41a+ddcfW1VfEAr4+TwvguIOznNV+dTXOE0mE77jO76DyWRyWY/Vi2R245e0vyhVKl/TL3OWUyCe/FKU6eYm89qyd7wHRomGZLmk7jcsQaAEOjFBjXUHWhJNcYGMEJYimuqm+d5u0YqCJ8+0Asm9hh3qE6FVOgWPa2s9Vi1qpM2Z9a0LBIkkIuO1NRbUbdUpvRUk0BApAp0qWY2W7XNIq/hkWgOiuF/Sn6eEpRE/A0Vt66fVTPpBI9Ufp6+YUoUUBzQRRgOjTCvQVaUrC+ZtoSxaSqkgkGKiiYmYIik1SBCHRV0m4VAjUm2ifJTCX7HOD8bd16M+3yD1fCtBVXXH0/Buxdo9H/TpGR7nGA/xbbx++Tp36/t6LRQMwzUKirvZqXi5J6XEdDrl7rvvpmkaXvOa11xxd10PsFf7h9aDa42Vr/BPbMkGqhGtyoXpJnnRMVxb4eBgL50UIsmrkwKo0Plk2IbMwKeaKkok0gn03YMd1VNm7PNGXVrOZyIxD5mclYlGMiZcCZqW+8LYi24wcNScOTu7QMmV1dU9xGbA6XNnjColLX2DVgcVUQJz9xj2k2dx32H0PFL1EIAqRnIGF/DY8/YbQrV002p0bKt9LqqFefcNHIILbZSleKaiiMAgjRkmAzyAWqHLhZw75vOOUmYG2jESUiLFQFcgaEWrqVtR4bNMebmMeEUYXtXvwBPPN6LQ5nKf72rA4kpi7USEUgpt2z4nq5FHOMpxHuHbeb2HUeyu8rS/Zr7QE/1zfa5JULzcs1ugeOHCBe655x62trZ4wxve8KQapyt9XVf7BxZjZKNe4N54Dy0tRZX51oJZN2cynDA5uOZAWBEaN97bZb8LrhyNhVpAPXc0kqziycO0W4oLZ7zUV02Qg9OWPXhJtYzSPkYtU00cQ/SaKJAqbGxsMisda5NVRqsjcp9tqsGUrsEAqfFuxM5D1KIGROxxWyqhBqL0Jn+shNgBE7yoeAnQfaRbPxEqnQhJ+6xUA6oi6vYL3wn2alR8h8j2xInvLxHQ1BCbxKTibziUUiq56+jmlXaR6dqChDlN0xBjJKXEf9QL/Nu0n9XwwtFacvo0un8/XCHQPN+T4nP1fM8Ua3fixAmm0ylf+cpX6LqOwWCwBNaribVTlPv5Fx7lcV7Fa1hhZfm53fQobm1tLXtYv5nPNQmKl/tOp495u9KzubnJfffdR86Z2267jaNHj141IMLudSEu1mb8s3wJqcJWO2e2MaVZGXHguoN0wZJgen/hco9HpA2FVOPSwK++o4tEWgcoRCz2rUZykKXApZO+KUKWZnwoLIIy9ElrLsUCvRFaUbTCfHPGtG0ZjsccWNtDL5wxi4ZQJTLHACwBGSFrdUmPAdJCvc+xWjxcwXePBJJHuNkEWZbxcn3aTNt7LtWj52oADRQ1cz/Y69y+13b2aUHM++gUa183ZYUc/urqdvqNqhBCZDBsaIaQKwxGQ0KMdF1Hm5XpYs65vMW/y2d455aw5hfYq+04vNzJbfA7v4Me2E/7a/8TXAHz8c1M1/axdnv27GEymXDLLbegqsup8mpi7TKZr/FljvMYr+Y1XM/1l35+lz2K3+wiG7hGQfFyz5VOihfXON1+++3s37+fUgr33Xffrryu3QDF+7mf84fO08yHbHVTxmnC6sEDNJJoHQiLQHWTQBGWU2OoiRwAghX8qrJCXHYgVvcApmoUaPUdX+u7Qb0IEAtij1UMROZSlwA1o9JtLZhO5wwmY/bs22fZp4LtHKt1Ly5Qi/7WAGJUacSi5FRgoYWeSkXtuVXUo9rsgtzRJ+9sT4YWz6agfVScuqne1KgBJaoSl0BoIptMpShLIz8ihGoEbOrt927RUKBq8NBwWVox7D8EpVJUqP6mIqUhMYFxBEZ/f3GP8D0XWk6cOMHm5uaTOg7X1tZ2zCpcDkjFv/tbuHCecPpxhr/+a7Tvejf6itt3dN/+fLNMijt9ThFhOBwyHA6vONZuJlPu4Ss8zuPcxrdyCzc/6Tl3c1K8FuwY8CIo7uhc7jvY+XzO/fffz8bGBq94xSs4ePDg8jGuJID76c7VgGIm8xX9Co93p2gXmToM7NtzAJJRfdvRagYIUR3gNNIFtV1fcD8hQicCVWhRhO1i31jjcv9oE19dptB0PkHW0KtIE0U6OiDVRJbKdNEy35jSDAesHNhLIhFE6DCjfHK6tlPb84Ua6FSpatmn6lNmUVx1alNaFoVqAGr7TpZwFLFUmSrCTL2eSs2wAUbLln4nXUGKEJN93mwT1fcvnt2j0Q2LlmojQOcTo6p5NbUX7rg3kd6rWPstpiAVqDad2uQKaoJVlMg9CV5140G+M9nFsS+M3djY4Ny5czz88MN0Xbej3NAdg2KtxI/9BaIFhgk2LjB83+/RvfNfU37wrTv+ffxmnhT7U0p51jclO421O8MpNq8/SzdUVvJLuKE5SF59cvD3boaBP1Ox8DfTuSZB8bn647u4xum2227jjjvueNJz7eZzXykornOBf2y/yIWt82iIpNGQ0XgCKXkrvIFUxBsvtKdKG7dnRAosq3kXYm31XT81XgR+uVd0Elg4qFYCHYWBNnRSLRjcadg+AHwrt2xsbjEgsGffPkoUmmr5pTM8z9T3kgUT9QhCG4RBhYHYJFnUprTk892iF8RUA6LsYQI9wFafFLNuU6RhuVusyyi2tDRLWKiA9u9zBKQG71y0KbCoTcmKorVPtbGb97VU/m1f5p9ax6J424bdr+tp3D5I1Z+wLv9f+YvZJjdOEodiuqQwtj89bbexsXFJbmiM8ZKJcqdv3OJ//gRy9nF0ECAlJHfUwYjmIx9G7ruH/NP/BkbPHjLwXJn3n+48n57Bi5/zSqjMi2PtDl5/kKPcxxmAvMZkOuSO6W08dvapY+3m8/mufZ0vToovnh2frut48MEHOX369FXVOF3uuRJa92uze7mn/RoUYbR3lWEccXZrnaqwwDJNWyk+EW4DoWizBD3BskaDJhahELWhi5356Ogp04YspddpOh1qas9MJZHcRG/m+FaUIkCGUxsXqFXZs7oHmkRR8yWW3qJRAwjMff6MWLRbwQBJ1PJFq7JU4bXAMi5AbdrrqGaWd4Vp1z8G0KjNbZXAwvo5LOXGDfYVm05bDBAH1RJresDqYEmdolaTYzF16jmm9vMoaok2tbIEQUGW4CjqU6JWarEaqRCiT5D++JYxgCrMET403eBXVq9j/BS/gxfTdgcPHlx+POe8nESOHz/O+vo6XdeRc75kqrxk0mlb4l/+JySADodIu0BXx4RFS12ZkL78BeL/+tu0//rfoLe98hl/L2utuzbR7OQ8X57Bi8/VKsUXLLiHr3Kas2xSGKQ1fmDPG2n2bO8cnxhrd/r0abqu48yZM08KS7/c17K+vn5JhN4367kmQfFKAKuXU1/8i9TXOD322GO87GUvu+oap8s9lzMpbsw2+MzG3WwNNlkdrxFGjQtezMzeijKhIQclaUMJlju6Pf31NUZCdpFMFwwYC4JKMPpUlManPhWjHedSaXpAFKVxFWpRIWLK01qU6cacaSnsm+xhOBzSLqlLr3/yCa0IZO1pThPkBI0+vQmt2O3McgGdVld+BlSFRW/LqOI9jB7vpoHGJ8EMS4o2SCKogaxRp94HV4VYhBgDSqSrYtmoYuAXfP+HGshVhSIBVXMy6vKbartGJBCqIkEMXD2WzoZCC1GX6rJab9hAI0G8g1ECVZWtqvwfm5v8ojfa7+SklLjuuuuWk8C5c+c4deoUN9100yVlwBfTr4c+/Sma86epK2uEbo6urSLdHF2bEEqG1MDGOsM/+t/ovu+/ofyrH3ladeq1Qp9eKSie4lHu4z62aJlSiazwvbyWhktFOE+MtUspMRgMOHjw4PJNz4kTJ9ja2rrsWLud5J5+M5xrEhSv5DRNs1RyXVw9dcstt/DmN7/5sv7ARGRX/iijqxCf6bRtyxePf4njex5lfN2EPcN+qW9t9kKkxECqwSY1pwq9W56CNTy0ojQE5g5qnai30RtIhZroQr97VIoISYWZmL+wwtKI3+LJGEQWWphtzugWHWkyoplDGgyZ+0QVCMyXiTa2s1SFpMGFNkbv9kC2EGFYBEKgpbdlBBfWQFZTfSa3YHRaESARURfqFFO/OLUp1MrSN4lis69Pcp0IFSVVIYgTxermf9/L5uXe0ILIlT42zraFBnwOnBKo1e5H9ZDxoiZaqkJUqEUdeN2ELmJGR7Gpkqo8KIUPb27x02tX5oHt6cz+YnnjjTcuP75YLNh69ASjz/4dGwJy/hR1NKLZPEcYjxnk1iq4mgYpmToe0/z9p4gP3UP7P/yPcOiWJz3ftSC0uRLKNpO5n3s5yWNkhE0gM+IH+DZWeXZ7RM6ZyWTypDc9cPmxduvr68vfg6c7d911Fx//+Me54YYb+PKXvwzA7/7u7/LHf/zHy0aT9773vbztbW8D4H3vex8f+tCHiDHyh3/4h7z1rbaH/uQnP8m73vUuSin87M/+LO95z3su6/t2NedFUNzhSSnRti2PPvoox44du6zqqad6rN3yF87n86f8XM6ZB44+wJH4ANyU2DfcRxcsNaaTvASVoOJVSFaAG3v6k+TpM2GpBF34nrBgysiITXCNBtpQKcV6Bwv4hNiHbRsgRnpvotGc5xdT5psLViZjBgfWCEVZLDItJtABMQWpq0X7iTA6AJqNwn6FFy75idXsEcUnyaQG0C0G2FFtEzrXavSqeLuG54qaXMYo1uwfV90W4PTxbRUT79RqAeNRLbC7qsXE5VrBs19NpOPlwSIOmIZjlW3KtKqpfS0L1YPABVT7XSImDgqm7KWq7zB7L6TvFz14/J8WmU/GBf/tZPeM/SLCaDRiz1/937BvL5N2jg73obkjo+SSmZWOLnfE6QY6GjOctshgxOD0SUb//t/R3fn9lO//7+EiX+W1MCleLmV7mlMc5X42WDBHWCDMgLdwB/t32Kr5TNPpTmPtPv/5z/P+97+fvXv38vDDD7N//35e+9rXcuONNz7pZ/bTP/3T/OIv/iI/9VM/dcnHf/mXf5lf/dVfveRjX/3qV/nIRz7CV77yFU6cOMEP/uAPcu+99wLwC7/wC/zlX/4lN998M2984xt5+9vfzh133LGjr/lqzzUJilfS0L1YLPjHf/xHDh8+/KQap8s9uwmKT6RPa608/PDD3H/uQfKrhNFkzczq7iXs+l2gFBptlpaILlSGnkWaNNG6wrSnV7PvxnDQbKqFdKc+3NuVnn1Qdys26alu2zh6I343bzk/mzJqzOjcBhO0WGqMOCVrvr6o3ojhE2HwaZF+ktV+mrWproRApzBSu+C1KLWqWyCUrMEtF9GaHtXUqj2F2afvzFUv2kEakGaFrGpAhXgKjvGYRW2vWVWJKohasLiZLczf2apNhQZ+fW9in32zHViuGiC4f7EXuS4p2ECViBZF+h2mAAQT8fiUa/yq8qmtBS8Jge8cXV7TyjOBlDx6HHnwCKIdujKCqoRhomkaBrmlDkeEbkEdjSjt3GwupbDYmpNjoPl/PoZ+6TNM3/LDDF/1Osbj8fMutPl6pk87Ou7nXh7nFIXAFMgENmh5C9/BocsI+b4S9ekTY+3uuOMO3vGOd/BLv/RLvPSlL+Vv/uZveP/7388dd9zBH/zBH1xy3+/7vu/j6NGjO3qej33sY7zzne9kOBzy8pe/nNtvv53Pfe5zANx+++3cdtttALzzne/kYx/72Iug+PVwLq5xEhFe9apXXfKO6krPTjsVd/I4vdBGVTlx4gQPPPwg9ZWBemukccBK2ETiwWWoKJFmaaDPUdBiu0LBU2fcNtEHUBeppBodRNMy9LuTbfDIArFGFg6SojZ9WmsGdIvC5sYWOghct2cfMRrF2agpNRdUQsbFOJDUIMXsFnZRn7t0J6iQ1cQ5SYPJ1inGIvYtF2qxcsmVoJ2av7DBi4cdcKKrP6sn6/SG+j7VJqt5Fatug6+oUa0Lt2tYZZXQEJaCmSpCrp7Co27qDxBr8LWj9GoZVMXN/e5pVJsMVYFg1GuuQldAghVMK/Z1B89krbUSJLjKtbr9R/kPF+bsCcIrBjv/c38mUEwf/z9BOxgOQASRgg4GhNxSJmOkGFiKKmk8JMUGKQvqaD+SW0qMlLKg+eSfcf6Lf8c9r7iTWRwynU6Zz+fL8IHnErS+HkGxUjnBMU5ynC1pKSTmqkxRZhTexCu5hcsLgN+tvNXhcEjXdfzoj/4o3/Zt33bZ9//ABz7An/7pn/KGN7yB3//932ffvn0cP36c7/7u717e5uabb+b48eMA3HLLLZd8/B/+4R+u+mvY6Xl+fyu+Ts6zTYp9jdNnP/tZ1tfX+a7v+i5uuOGGXQvEfbZOxct9nMcee4zPfOYzHKsnyd8dKdcrQRq6IARXedLvAT1NRt2Urgg1CJVAFvfa+W4s+z6wRS+amqaMVwAAIABJREFUHg00cSDLWEzbHIUavBKqV5RaJ+I8Z86eO8/W1ozh3lX2rF5nqSwYQGRMHZp8QuzU9pYFa6AIPmnN1YCHGphRbZKskaxGn0ZXnnYKuSrBs0kXao/f10J1qizUw7xroKqwqNBWQAPJ6chOhXmt3rRhO1JUyGqvJVffexZZ7jU7AvOqtFVoM2gNbh/p1amgEig10KqwyMqi2vN3xSbZWo2WtXatgDjQSw1IEbQG71aMCJFaxT2XrmqtNlnmItQqbHbKB8/OeCzv3B/7dKAY7v1nwrEHIAk6ahAKdWWMaKaurhCkwniIpIgksf/WFl1ZRUKFUUMYNgyHgfGB/Rxqz/Nf3P83fMuZe9i/tkIphWPHjvGFL3yBu+++m69+9as8/PDDnD17dtc6TeHrDxTXOceX+QJHeYgtqcwRpsAGygaFN3Irr+Jll/2cu+1TvDgUfafn53/+57n//vv54he/yKFDh/iVX/mVXXk9z9W5ZifFpwoFv7jGaW1tje/8zu9c1jjtdij4bjzWxsYGp06doh12yJtGXGhmRA2uDnXAE4iSjMLU5LVIiTbYfm4RCkET89AyqYE2KI1a4HeofQ2UpdCoQgm9h9F8h41apFvEArczgaQGWiHD2a0N2lLYs7JCGDSgEURplzYLvK3CPIMdgURa2iMaooGc7y5tt2i+RkFYuBk+OXBaiACMeyFNhSCQaqSqmkm/FrYubNG1HeqB201MDGJCU/KAbwiynaVaxMqFe5tE0oCKAWrWQHCQDNiUHNGl8EWrTZn9P2pxNQ46EalKkD7mTa15RD1wvXiakHsdkeCNGQEt237FfqrHQVdcxVoRoiqzLPz+qRm/dcOEtXiFuztV0l/9RzPVjMeuOF3xyXAMWtFBgpSgdtRmTCgLA0sXG5EapHbU0cjoYxVqCOx75J8ZbJ5AXvf9lG99M4S4bKLY2NjgzJkzHD16lJzzJT681dVVRqPRFa1Enk8LSH+e+Dq32ORhHuS8rDOjokRmUlANTDWwTst/yUt5NS+/oufbzWaOK/UpXsyu/dzP/Rw/9EM/BMDhw4d55JFHlp87duwYhw8fBnjajz8f55oFxSees2fPct999zEajZ6yximltGvvVK8WFC9cuMC9995LjZXF7ZXzt/YFyIFOTBLSp9C0QUlYAHYQp1F9B1VECETaqGgLOZhYpLNrKzX0qTYGEsk9e72CNFYDYHWrQw4GiAutzDanLLqO0cqE/c2IHKTXhbBQGGhwihMabFrMKKEYYOK+vlbNTN842GeUptreb+GClECko9K52CX65JfVeg5D9cepldnWnNwuGK+uMFjx/NmcyV1lo53TlgoVBjGQ4gBJBpr999d6F5Xs6TVtFWIQUg1EpzIt5zSQs9lS1HeJ0SnTuMyjsc9VAqUAEqxGKhiQCiwTcBSbZFOFkr13UfsUOLEdY3Cfhu9IpbN/95vLC0X5Xx6f8ZsvGTF6linpqSbF+Lm/IZw+gU4mSGmpkxVC7Xxi9F3oYIDkBToeEWqmDkbGSSjocGTmfgcxqZk6GCJa0RRBM+lLf0l64G66V303vOLOJzVRPDHd5eTJk8zn80sUkzuhX0spDIe72yxyOWfKlId5kHXO00q1UAlJdCK0Clkj51jwZl7Kt3F5kXkXn90MKVgsFozHzx7E8MRz8uRJDh06BMBf/MVfLOnXt7/97fzET/wE7373uzlx4gRHjhzhzjvvRFU5cuQIDz74IIcPH+YjH/kIf/7nf74rX8NOzjUPiufPn+e+++4jpcQdd9zxtDVOKSVms9muPOeVJtFsbW1x5MgR2twyvmMPpyZnmZ5vGbBKDkYfFik+tYnRp26REI0sgse0SbX8UmzSq1LRalRocEVpckN9c9G/5+DJM57NSfBKp0hLhQznpzNm0zmT8ZiVfRMGmuicbg1qgdrJqdsWA8S2nwo10IWw9BB2LnaJDtSlVhps4mvx/R5i1gsVGhUWBOYVRk53KoGZx521W3MmqxNGKwecErVJsEbb8Y1HI1aqEFRpa2WWM928o3YzFKUJEUkNkiIxJKIEYoWmWkJNJdKVugwAsJxVWUa82YRnqlVjMnubh+0btb92+eOJ9CIfmxypgrq3kh5YxePflKUfsvdECmYn6deXZDiZlfc91vKbNw5pnmG6ehIotnPS//dJ6sqEUAp1OCQIEKP5ETVDM0Byi47HQEHTwIAa0JiQmtFhQmL0/x75plsog0RICk0DZcbgK3+NHv0c+dbXU15+JyQDsIvTXXqJP1yqmHzkkUeWRcB9uHY/Vfbh2i8EfQqwwQVO8DDnWWcmmUBDJxaYURHmqswrbNLxXbyE114FIIL7aXdB1bvT2qgf//Ef59Of/jSnT5/m5ptv5vd+7/f49Kc/zRe/+EVEhFtvvZU/+qM/AuA1r3kN73jHO7jjjjtIKfHBD35wOdV+4AMf4K1vfSulFO666y5e85rXXPXXsNMjl7kn252l2tfBOXPmDPfeey+qyitf+cpnzfQ7c+YMjz/+OK9+9auv+rmPHz9O13XceuutO7r9Mkt1c4PVO/Zyds8GnWS0CqcunOXAdQdsWgBvjy8XKUz7XaIs6VSpUIJNUCUotS2sz6fsW9trE2YN5KD+eZaqxioGbJ3gAAQDhDlKnrWcv3CB8coKq+MJiwAD3z9azJpFrCV/HR3me8zu52vURDdnzpzjhv0HaT0/dIDQ+UTV10eZFUOWSlSjMw0Y16dTmhCZjEa0CrPFgvnWFqPBmOFkYrSoA4aqxcrlal9PELNgZPcOSp8/6tTpohRyrtSuo+RMLT4JhshgOCaGYP9gNKeoRcoVB6lSqylGffpDPOfUlaLuWrwE3HQ5ESrrm1uMh2NSEw1g/W/XH205IWpRCPY8ltYmFhXngh5Uedko8ts3DYlPc5F79NFHaduWl770pQA0/+k/IP/yBQOSABqCA+EQ6RboaGQ0atMYVgdFU2MgORiaaihaULvUTG0aRDBzSzNk48I5xqsrpMEIyEYTR5dINUPqoVfRvfQNsHr9U77epzp9ZVOf7rK5ubmkX9u2Zf/+/Rw6dOiK6NfLOYpylsf5wom72XvTdRSxN3VFIhnzC1eMfdkshS0q38V1vInLF7Q88Xzuc5/jzjvvvPqvQZXv/d7v5Z/+6Z+u+rFewLOjH/I1OymePXuW2267bccc+W7vFHcydXZdxwMPPMCpM6dYu2M/3b7Io3IO3FYQgqDZdnGxV3oCLMt9TSUaq+0OG41UKtGTTxADkhoCtSjF1YzVvXF91ZEFflsmqVk0gglbNLDZtWxuTGlSgjRgdbJGK0pSsT2cGviZj7AXvti02Kn5GhsNLADUUmA6KlJ9b4lSURoSWQsdNl2qCnPdbrNYANUFMaqw0XVsbUxpJLJ3zz5qMLozubasVSjV7tv4nq/16TQhS8VqQSjZFKQDSQwSkAZLZenWdG77vi7T5UzOBQkBiQ0xJkKMxBAQiQSJS2WbYnu/qkZnK0adqsjSnK9abfftxvyuQKqCFvtal9JVX9dtv7+1Ra2ITakCNob2ylYJHJ1W3nu85TcODwjPAghy7hTx/i+hwwbVDuIQ0Y6ajCLVyRgRpcbB9hTY2G10PDJ1soiBZO3QwcC+RxRqHJptJQoMBh7cE9GUEHXbSVDC4/cwOvUv1L2HyC95NfX6OyA9s6Wpr2xaW1tb0nc9/Xrvvfcyn885cuTIJfRrP1Huhvp1xpTTnOAcp5nS0Y4ybRCKBA+gtzeNi2oe2VkVTgHfx37exNXbD3ZLGAi7W0H19X6uWVB8+ctfflkUZp9osxvn2SwZpRQeeughTjx6guGrVqnfOuRxOWfKT0DEMlHsHaZ7AfEdXB/OTXWzeSSLkJzGjDXSBiVWM+cbZen0qai3WhSi9ik2iblUBh6+bcW/gdx1nN/YoATYu2eNnAKLMxssvLvQfH3V95DQK1Ln9KHgdiFoqiwTbhqCdRvWfn9pas8k5iUsmEm+qEWzJQU0mqhHzYifqzCbLwgIe1f3IDGQVQhFSSFQ1fyCKNan6K+jWFApyemmrqqFgmNTpf2h2F6zWNmhkc0ipBAZDocGoMX2gF1XyCVTFnNq7mzX2SRiSITU2Bsa+kmwr7oQo0PpJ7r+H3szIWr0KSpIHwKupjStYpSr9iDrlo4QDDiXO0dYUq1Htirvfbjl11/6ZGC8mD5tPv1RtElmqxiMCMWALWhFmwEaAiIF4sB2jeMRQTMaBxACQQs1+e6waVBpCLWlOr0qopRmQIggEWocYrl3RrsiIFrQZoDMzzB4+P+F45+h7L2Zsv8V1Otuh7gzz29Pvw6HQ2666aYlQ/RM9GsPlM/WbQiwYMY5TnKeM2wxdaW3sR1tDHRiqwDzy9qbzhlKroHHyPxXXLcrgAi7m+96rTRkwDUMipd7dnNSfDpLRq2VY8eO8cDJB0mvHNHdFlmEqYlYEII32vegFRA0BkR7NSmAWx00UoKlqXSYRKT65OHOAxPVBEWKAadll9al7aJXsi5TbBAohXNbF8ymtjZmbxrRinsKxaaxKJE5tv9r+69ZjWYNXsjbUhlooEOo9CpWvOIpMPcpqPGJslR7jv5CknDxiRpQB1XWZ1M2p3PGgyF71lbJ2NQVBUQiXYW2KEkMzDqt1kKhkKSfkgOlKKVCFPsem8cxUGp1T6Yb68UAaq6W+1qKT5khkFKAZmCPj1BLZZEri1zI0wUlZ1D7XWhCIjYNKUZi6KPacKoVB0aWIeA9WNp3yOIG1Cdka99YkrKUavdVgMxSCCSWR8A9U+V/Ptrymy9riBdNRj0ohge/DOcfNcJ7YpmmOhgDanm3MZnYpgfL0cBeT4hoaAjaos3QnKESIDYE6VDxqVI7NDRIAmkiGhufIis1DizEgILGgZHvwcbiGgJh6wRh8xic+HvqykvIKzehe14Go4MuMHr688Sd4hMN6/1ttra2lirvvttwNBotgXKyOiGPZmzIaTY5z5yWgtGjiFmdisC8VrIMaIl0bt2pwJZWchVOUXkjI757lwARvj6Up9+I50VQ3OF5Li0ZfUjAV0/dA7dE5M5IlbKMXxMPuO7cFqFqHsLgCs4qFneW3RieQ7EsT1duSq9C1cBCXESDq1J9v1Z6/xx20c9qQGYeRWGhmcXGnGnJ7B2vIGsDM9BL3+gg5GACmUXoQdhVq+5jFJO90qqa1QKjAXvwrAoUM8SDTYutmhHeXrPt3KIDY6dKVJjP5mzOZzTjMauTFdRVofguUyUwLwZ+jasxu2rTYEQInjfaqVCL0cUJm9Cy2zh6Ki/5rq/D7jMvHjRXvTsRo6Irttsr9LRmMLBsGsKQJVB1XSWXjtm8peuyJeLEuJwoU4w2xYnQlkAqXjQc7PuC9tmn0E+b9hXihcT2sxH1WLlg/+6FN4jwtS343Qczv3NbQ3IwUfU6rc9/kiAVHQ1NFRsiJNsL6mDowpqRK0iTCW9KgabxCdGsF/T0qVY0JDQMCGRUBnafiH0+BfM8xsYAVzMaGwgR8ZSk2iTExsdtn+3iNLSnYf1L6GBMHVyHDg/A6AAMDhLTnkv2hjsR2lxMv/b3mZULXMinWV88xtl6L7PpjLxZqBEkDZGmQQYNMSVUoq0QJNCVQgmBVgILVRfVBKY1c7IKb5ER3y+vvYKrydOf3fQoXisNGXANg+LlLtaDCxd24/T0qaryyJlH+OqFr1EOwvDQiCCR6jVKFhNmpbiBxkU0LqbB9lxaLeg6+j7wYj9irMIiVBoXqhSn4Pp2+q6fCkPf14fZF8R2c63Y/1+YzZhP5wwnE/atrXliCxhF2u/+KlJsqgvqaleppBroqMs0mD6D1IK3nSb1i0RSpbh1Y+C0a61qoh5fmiVxCrQWcpc5v7FFHI7Ys2cfjQS2FnNav39Q6AJkn5Ci08xZrZUjIhaBV4XO/z8GmzpbhFIqFRPhiNsk8vLjulSfBkCqvVHJxYQ7yyg4u3wbCLoYqigUNyzWEAgyYphgaJZYSla6rqNbdMzyjFIrQUyxPJfIoBlZbKj0vkRP8VnWSeETrdGp/b4xmGTYblPo15EA3LcFv3Ff5ndvS4zdx7h25DMwv4DGAKnfFY7McjEaIrVQRwMCAjGiIdqOcTyCmtEQt6fBNDDvbIgQG4SMSqJGj9uLDTJIIP2eUUysE5I9DgbStUkggSq2+65hiAZP8Ym4/SVT2rNQzqJbR+x2aYCmEYQxksY0zRnmbUthgoRknlBPZyoUlEJHS6Uj05FZ0ElHEaE2UAYQQ2JV9ln4hCpdrixyR561zPMWmUBIDdIMUBoWGkkqzIFaAxvacaxLvC2M+N6w++rK3ZwUL1y48CIovnieu7OIC06tnubjpz5JGVfWXraHYUyUUCkqiFnAqW6lqCGgVEQThWK0KbYLRIIlmnh+qNU5WSaoihC0MUrUjfhNtZqoAeYrU58uNETUQ8LBIsfmswXTrSnNcMi+/QfogmV9Lnz/19siLEjbFJedwqhGCwOvXhelBiz97Y2OdH+iVjJmp+gQSrXklrk3iTQqdGKAZJOtMM0L5hemhBiY7L2OFBKxYrSUWoapVLN8kE3DIdXEQrUavRuoZI02IVf1XaJRpwvFQ7kDjYNbLmal6HeJPqug1QBbs2W4iU9nAR/J2O52rNWQqRS2d3iucr34LVqIgRTDMju2+pS3vr5OaSvT+YaBKkKKidQkYkrWs2iyFVAD32CMt68RPW/V6W50W5wjwCMb8Btfy/zOKxMy32T1+FcIsaBxjNTWgTCjo5EBSDSLimqHppHTqAMIIDHYhKfFBDiY7NnA0SBbU3JaNaJNRJJF5Em0YEJFzL8odt9MP61aJFoNjQE2NgErFpSuIdj0KFhXpeJFzXNqXVB1g5DWWeRMq8lAU+zvpQSBEJYiKw0GwFlAYkJDoPOeUMXDJwANCYkwGI2JEkhUco20pTDPmdl8wVaXOH96hobAPFQerQ1vHzZ8T3puMj1fnBSv7FyzoPh8JvIryjnOcYYzPN49zqmtsyz2Lji47wZkkOwPHDBDvV2MA77fw6YAa7BQINIFm+SCJmpMNqVE9QktbItuQiW5EMOmCQvq7kU0oRqd2lOZ4tNjnWfOTqdMQmSy7zpGkjyGzabJ6FNen6LSewzbAMMCXcQmGHUxDUKL0X1RhNYzSTNKW2Hon1e16TY7EDQeFadOc85LZX1zipTC2p41amg8Qs0l7sXtJFqYF0uJiXj1k1ZUA9Hp3kwg10oQu78SaKvvEtUM+QC5CsX5xiC4GtSm7i4b5aoVJDkYYxmqXTWa26qdXDiD0cvR7Rc9hWr3sefry4e1ytKOob5PrCRGwwkxeUdjVbou03aZspiRu4JgTEQTk2WONslqlou9hhBY1l4FDJz7yTEoPD6D3/xq4bce+VtCnaOTNRO5jAb2M40R4vbOL2iHRt8jpoSEiPqOUWo2sAjJdpKx7+qr1NjYfcTAU5JACjYVSkWCxfwRIirVQtBDsGlbzOiu0QRbtmI1mpIYqAQDSfEWkuV9LZmIEOmiUFNCg9ge2XetBsDBaU8D20wgNMne3ACFBNib1exfQ/E3kiWY3ShoooRAjQOaBjppCXQM1yasLwrHtxLfe37AgY2Ouxd30zTNJeEDk8nkqtWvu6kYvVa6FOEaBsUrOTvtQVQq66xznrOss84WMxZ5wcbGFrUoK3vXLH2lNxL30nm1sG5rcRC2c0htesxe5WSpNAaABDPbLylTF89Etf82X15YgqGBrlA0LPv8CnYRnncdGxtbEAJ71tZIKbmytb8YmIcvEVloZaheK1WN6pRqk5xU3IZhk54VCuP9iYrU4OHb1X2URpkmAlkiXVWGYpNoLbYXXJ9O2Zq3XLcyoRmO6YrnoKowd7N7EqOGF0UYuLW/qLLIlRQDiUCpavRp9d1bhYxNjFRoHPiyJ+Kof/8CgNoesqplmCI+5RXL/SnqACpigOt7SVH13FODQO13uBjgVTff227QblOd1+ydF4qpSWsFiizTiWJqaNLAbmADHKXLlJxZtHPmF7K9lYqBJjaE2NA0je13e1AWp4ABKXBg/QFOnzzOeDJgmBwZYkJCpjYmqKlpgFDRMHCuuKBpiGiGwcgeLAY0DgxUg+8FNRt4SsQM/sn8oFFsJ+mg17mIB/fwaXQA02KTXIgm/AoGXgWQGA1kUfouZkWo/ceDi8XE9t85boNlBiTE5dRY/N+tVJqYjMVAzLerYbnvrxLNRqPBAdHefHSupM4KnQpbqsyl4fSicjIP+LdrY163/5XLa0bbtkv160MPPcR0Ol1WO11sFbmcya+UsmuT4vnz57nhhssLI/9GPdcsKF7JpLiTyqdK5fN8hs4TQWtWLmxtMq8da5NVhoMxOQgqm1RxGgn7w7QLk+2mTHVo6k9/c23UENiu0NsvqOJZpPZHiqtGS1BC3S4H7vdHwdNnQm+5qIG5ZkpXOL85Ze/qKtokF7l4sTDV6FLFeha1+n7Qnr/4FBhUaKsyxqhQvIS4VRj455Rt4U1Dnx1qwNf5zi9UzM+lSp4tODObMhhNuH7vPmqIzIvtSaWKW0D6GimYO7gFhK4adWhlwcFsFg468SKqFPAaJw/SdkA0m4eBZ1bI7iOU/uNqQFUILKoJMcQTZmIvd1FTkeaiSxpUgr0uf+ZtcOrHHqewBf/Z96+xmo3Pqqnwdg7HpH6RKBBTA9HecMkEy08thZIzZd6ytTm1nWqMpCaSQmKQGiRGklbe+thfQYAH85jQFvatDa0seDiyPeKgsa/TKVDpJ0MX0RAjklvqYORkbkBTtDADMZsGUqjB9oNQ6GKiNkYxFEkQjUHRICiJGgNoNVo02g6wRmNAqioEK8uuVdFoNGpRIIrRs+J0KqbYzjFSHCwNBE0dW4M/ZjB2RmiM5RDfwWNl2rVXW/eAKNYs0xHsa9DI3PfXHZFZiRzLiS0ivzK4jm9pLi1aHgysRm3//v3Lj5VSluEDjz/+OPfffz+lFMbj8SVAORwOn/J6tpuT4sbGBrfffnXpOt8o55oFxSs5OwHFQOAwL+O+eoStrQ0W847x6ioHxnupYr17KF7IaxdvCFSptgPCw6VdXSrqmaBikGkRbUawZgCxd6NmYhdygFBlOd1FNT/idu2TG+ax8e381gaztqApsH/vXqpEAnpR5JtNeMUjynKtlmxT7SIxqD4REskS3CoRlpmmi2p+SesilKWAJjid2NOyRZVWDchzFbpuwXRji2Y4Ym3PPgZiF7ouV5KX6M51m2q2ZBqbjrUW2irkUi/yHdr/R7HbV4VFtqzSnlJtXaxkqlU1Gq2a71D95xT9Z9Sn1LTZ9r1NNSFPT4tWFfsH3yX6XtSAtLd0+C+N9opPDCCr7xwv0nVJNZ8nWYjJbm+vyIFR/WF8Au3Vpo6viCSalAgDe9wAlJLpukxeFDa2ZpSi/NfTu9nXnmLeDBlJ5cE8ZDqHw2sOhEFsaqyZ2ti0aH7DgGg1Oj8oNYyQaDmx6rcDpUoiBDWgCgmlotKQm4acxEEq+bfEwKiGaPcNUOPApz5jVYqqTZEOevhj2J4vLBWzxd6dGCUrkRIMFKvfL0uEYFRrFdtzC5Y4U4MxLGiy/TGmKjWQtfSnSrJ0JgHVAQsxNmJeM7kq980bNuYN7ztwgMPNzurnYozs2bPnEn+gqjKbzdjc3GR9fZ3jx4+zWCyW9OvSKjKZUEpZFhpc7VlfX7+ihoxvxPMiKF7GaZrmWUPBSyksHu5YH2zS7B+x7+Ae+yMUfDK0q6HRR077YPVLWXS7hR0TFIhL7atPg/RimCoW2B0CqUBpWF7sK7Y/zE6ZGt3joh2s6mh9vsV8a0GzNub6les4tX4O3J/YB3QnFYpbMjp19ahWj2RTV5baxWNpIK+2m2vUJkvUNqE9+GXZvujPVa01XpSZKgMCRZUz6xsMQmR1z3VUcdoTq1lqHBDbCvjOsFWX2GN0ZosQs4lnUAsHVw8SEBVrwSgmgjHfJHRVfd7EkmZCIPteVFXcumHU6WLZkejgpEIoOI0mZA8/LT5tqvcnusXOVKc9AOLPAUtbhbpHUv37s9TkVMWu4ECAWranA8ddLgqOsx7Ni9JuRG132N8iakNqGmRgNPVad4G3bHyVnAYkVae1lce3Os7mIa+cmO0lBaUOPPtUgwljamcWjf73OyVECzUMQOxNUGmsSaMANTW2GyRQQ0NpjJqsobF1gtOXS4D03Z+K0agqULQ6IEYX1dgkWMRC1iXYbnG5VxRFg+0Mc79XFPH9o1hwhNO1YNNl9u8B0pj/UPxNEe41DGrdo9WmxtaV1lmNPTjbKQ/NhM3pkF+TZseA+HRHRJhMJkwmk0vozKeiXxeLBSsrK3Rdt9xXXimd+qL69Bo4V0OfPtWptXLixAkeeughDh06xFtufAufT/9EcX+e4DsOv0DYBBHMn7WM7IruOTR9qe0WLfQ6SwXfg3VA6i+uffuEF+GKR7Gli8BU3F4RNbC+mDHfmDEYD9l7YL+/q1a0BDqnRfsy3tYnMWujEEuGIZqlwOm9VpVhXxVVrUZqUI027XRbddpXWVmMWrA2Cg8gmFeLd1vf3GDaVvaurDIcjeiKgWDFugaTJ2bPvXYp1b5RwABPFRYqlLwdJtBWr5ZyRW4BavEJ0y0WXfUsVaetM9Zaoaokpz+rKl2NXh9lnkRVn85rL7gRat0W0YSLQIpqINnvEO02ulSY9r+NoV8gOgUs4P8vxNKLebxr8aKpUvzNxpJG7R9Pt0EVp5aXgXv9my4xpuG/u/CfaaSQB0NCmdPJmJWYWcRVhMKRdsKNzGmCZZMOYkAGQwaihOHA6q80UxsDx0ryjNRKmwa2k1elhsbyUzFQqwS0EXJqqGI/4+JqUl3Smfb7rNHArWCCFwnJd4u+XhA176zDcy+a6QU1lr4TyBLIsbEwfIQuQHXdsPq+PYtrvYOJbIoIaHRq3+vEVKgYYHYIuQaKGsPwyDxzrB0iJfJr7YAPpuDjAAAgAElEQVRDk5XLveTs+DwV/fq1r32N1dVVRITHHnvsEvq1nyjX1tYYDAbPej280i7Fb8RzzYIi8JSdis90ngoU+0LiBx54gIMHD3LnnXcuo6Bu0pt4RB519WFARE0EIGLqOqdS7TJqohdrve8vpZXixcBoXFoiwIYF29lFD9eGVszaYH5Gu3B2nkM6WyyYbkxJTcPe/deRghnmk1OY4qbikRvWk//RU+25BtqHeNvt+2mxUaNYg5odoKsw0sBcbbL1RDSCWKWUAZFaqwWWX7q1NSfP5wxWVxlWC9VuPR1GVVhUJXnH4KIoEswf2Cn+Wu0i1qpPbQ7auajv36zqKStmSHe7Sp912vQ7xqp0/djme0PFMkeNDjX1bOx/d6rSFSFnIQ2cxsREMD1dWsFtH1i/YbXXBAZuwXeI/U6T6iHk4KZ3lnRnKtD465WLPIniQKe9qtWf1xhD2RbS1ItuL6C5fxXwLYuv8fL8MPM4ZBgKM0Y0SclxhNkWI0UCpzWybzzgxnH23WthXlrajZYoFU0NqSixCcRmSIi2s9OlirSxZJuA7dxdANOGQJcSRpl6+o0LX4rbKkjJvZ6VgoWLV/Adovh7B4t3AMzK5GBp3l9Tp6oIXbBS7eK+YHsDmlzFajRoq5EgDUX6YHdbT9g06pF/KlBtaszV2I9Fhvu2Kmdyw4jI/75yCyfPnXxBSo337t37pOqtp6NfL46ze6L69UVLxovnKc8TQfHMmTMcOXKEtbU1Xv/61z+Jv38lL+dROctCO78I9CpS7I+zFiQ1FPcRutaRSiFqQghe9gvbU5/9onZSSdg+rCBG+fj+EbXk/UigXWTObm4SA6zu28swNMxxZSS6dD1WsVb3Iv00KsRq4GLxaqbCLE7d9orVCkuQXGCAlB2cRALzaqKYRXXBBVb8m4AL85bN6ZS10YTJvoP2hmHeMa/CMHp4eLUdKaL237637FR8knTg6gGx2uTWlu0pzS5Y/steoQQT3ZRqCTciBsLFKcYQHFj9sYpPUkm3my/a0gOv+MRoz1/F9sUVoyoVByaFUOx72dceRqBvxDCjPcsJcGnZcBbB6FVBSvA3UCzBUtRR1d9kCdv7RMUUpbA9kS6FOQ6ejXb8wOxvyc3QEnskIEGRoJSQGIZCS2IQKx0DTreBKQ237I1MQqaEVVbcy5cr5Lxgmgtd2UK1ILEhNYmQEnEwNA+j2GSmUdBa6IKJaWqEWqOLY+xLKoo1ZvSeQbW9YL9XrH5bxCb/KlhRs9s0NOACml65DZ0EWi9h7veF1T/fieU6IaaM1l5ZHOz3QSWZvV+D15vZzrpo4GTbcnwa2SgNN5J4/3W3Lqvinu9A7ad6zmejX/tC5179+qUvfYnTp08znU6fFdTvuusuPv7xj3PDDTfw5S9/GbDihR/7sR/j6NGj3HrrrXz0ox9l3759qCrvete7+MQnPsFkMuHf///svWusbdd13/cbc66193necx8UHyJlkRIl2pIcE5Jp0QliK1FoGxYiQWlrx0YhGSpg10WAFEWA6kPswqiDUN/aIP7Qh2rLMZpGgBGoBWRDBWGiTg3RVVKriWOLlClKl7wUJfI+zjn77L3XmnOMfhhjrnOuRIr3klekHHIC5L13n73XXvtx1pjjP/6P3/ot3v3udwPwqU99il//9V8H4B/+w3/IRz/60Rv5trzoer0oXsdqQcNXrlzh0UcfZTabPW8gcVuC8IP6dv44/Xu/wTLVh4TOjsMT3Bv8I5ZCZB675RDXq4SQG2Wgwas+7IdEUXeE4URnWGrl8sEhtcLm3habec44QbI5ZpleHHqCaq7GGmNmCaXZyNmkT5xFkepNWLcZIxZOMZDIUJVBjXnMH3MUiqJCD4ziJJpL+wvSfIOb9s5M3VSHMFii1yD5hLBe8C7YokNa4xenHr9t0HhP1CiWGGpiKwrYKC5/aJKKBo1OHaPFhRw3/U4iQZCwcEdJdLGdMXWyUNFjkmgKwo1Vv8CqeiFL6oVsFlDnVcVOj4tgc5jRKFQ5zmmCQO0YGk0qSIkiGuSgiZ1qMr1GEcIHl0kvOh1PvEjG2I7O4G+uH2Zblgx0bKTCOs2Y5SUDczY6o0jvGllJWOpIqXJFe5b7ys3bM85uCxr+pAmj3+zJZDakYhIbkDpwVJSyXKAULM3pZh3ZMin3aE6ULlFN3R1H2jyQ6Bpjzt70iETRlOx7AfEN2+i/VQ7bikekWRS21gUWS4yJKUmmRaqZOBpiMgNzOUZQrihRnJWeaj6zLDhMWiyxUuNLByNXxo5lTbwjzfj1m+6argWvRlG8HvH+C7Ffu67jD/7gD7h48SIf+tCHWCwWvO1tb+Mf/IN/8G2RVL/wC7/A3/t7f4+PfOQj020PPvgg73//+/n4xz/Ogw8+yIMPPsgnPvEJfu/3fo/HHnuMxx57jEceeYRf/uVf5pFHHuHixYv82q/9Gl/4whcQEd7znvfwwQ9+8BWFbl/TRfF64dNSCufPn+e5557jnnvuuSbX+DOc4lbewAWew8T7g4qi2dmSTR/mobDNUSZNvVwCT76wHLCoeaSQde6FKgkrJbSKfsG9tNhnWCsbu1vszeahsbKJpZkQl1uYhPUbLu0woY8uEEJyMcGjPsu0ENe3mU2Nudoal2QsMXYJo2+VIMKE+L4Urhwu6Els7e4xzw6XjRW6EEkrYc8WvqGZ6PQK9E2moQ4XgzjhJiDSQRKjeuyUqD+v4R2uEdZtSniRxMXRLCzo/HijRgCwyWQNR3SGGgxciY7Yu2LBNFFqiOFjvps5nqO2Amh4cWvwbJsvYw5ppoA5OQmPRiE1gBDYd/E5Owx6zGTV6AQFJmcd4/j5aEUU72IRuE2/zg/y56zzjD5V1jYjZ6PqjC4J9Amzjk5GBpmRU2WNk2fGlHniKPOcKm/c6+m89cWkjy7POz4wJPVsim+2WjTZqIW1DQzrFWuD5w723e911iN5Rk6RcJKD9AITOQeRE2L+FCQd3yyl5miD4KbcQaghoVIZxai5B2be3VrkhgazVNXnksoMI2aUGoxjiY2VeYrKoMaFZeXCAMO6Z1EzP951/Fc33XnVdeB7pVO8npVz5t577+WHfuiH+N3f/V3+8A//EFXly1/+8vMK+X/sx36MJ5544qrbPvOZz/Dwww8D8NGPfpT3ve99fOITn+Azn/kMH/nIRxAR7r//fi5fvszTTz/Nww8/zAMPPDAV5wceeIDf//3f5+d+7ude8uu43vWaLorXularFV/+8pe5cuUKu7u73Hvvvdf1+HfpXTybDln5PtQxIcmMpvTisosxKdnahj5E9pYmh5OW5i6WIybImY6WMqaFqsLicMFyNTLb3eDMzhZKisISxt5EwQut3hgXztFwzZ+2maHP6QrGzISlui2cw6TN5k2mWWIRyNU8Eqc4DXAwJou2WpTDwwWrWjm9tePuIObztbW1fESf5yV12LKqMMMhNtcxOkNyrdDH7UODOy3IMeF8016L4VmNJsYYxSibl5iRzKCOc3YxjxoDLhUTt4aziKmqFjCov5cts1EB04SNlTyLQghTV2kQnWPApXG+DU6dOkhk6tzE5ERXGSv+3WPkSrBq/clUndXaEFQhusp4aINLp07xRGJZMuUBewjNabK5EzI5V4axZ95XBpnRJWUlM8Bt14p29Lkw4rFO+ypcvjTjlt3Kue0mj/DiiGhYsHWQPFDaklu3mSX6vEm3lRgOLrNx6hSqxlAr47CmDCNVYzbZJ7rIYTRJJ+DTsHhL4QecXGiPOMO7dZeFRnTrArHpHJHBC6IT4jzBohXHgs8nXZbjZhOYa2sHg2+ujScXxlEVhppZlo7/eN7z0Zuu1iCCF6hXeqZ4LUYj17Ja4yAi5Jy55557rvmxzzzzzJRleeutt/LMM88AHrT+pjcdv0933HEHTz311Ave/kqu13RRfDHG1TAMPP7441y6dIm3vvWt3HHHHTz55JPX/TyZxLvszfw/8hd+wcBA/MKgYTHWip3/z9MgavJ/e+FRkuNEDm3iBUlRVqOyePYS8+1Nzp475bKHeO4m7Af3SG1F0E28CzPtqCiEqbJaE/oHUScYolOaRszOjGbo7XFMs4A+xYprvMzjjC4vjqjLJfPtHc7M5n5MdZLNKAmJ4de6+gwUhGWB3c67itG8mCi45tEcNhujUDipJk2v06FPmKt7nFaUGoSSxuZcq1/YfD5nVHXYc8Qh3ByM3lFbELG4000NKBPBiucT5shhnEUxm4CHgEa7YBrnNv+L7rDNIQ1nlE4V0Boj1LvFaQ6IzySTylT8pERnKsfwrGkcphXcgGXbc2PH/35v+gJnuysMNmeeC2vr6ZKxYgZdZZ1dXF+SUZjRp8JAR+pgkA4lk3Jl0DmSjAtL4evrjjfuKVub4t8pcZa1s0Y1urfsczxJQIcmRTtxB5s+0SePw5pFgRq1MJbKehgoZXTbtdyTZzNy35H6mX+/gyAz+jt+okt0mJSA20c1NGWH5M1dkBrzu1qarAtrQKRExFkN+c5iNB5fKvtrL4ar0lFq4r/YmvGBm25/3mvAq9Ep3qi1Wq3Y3Nx82ceR0Ax/r6/XdFF8oVVK4atf/Spf//rXueuuu7jnnnsQERaLxUuOj7rFTnMrZ3hKrkRWXZpE4YGgeeJE/KKaOVzXuIhKo5k3GrixXg5cOVqAwrmz5xiStekXBdfwOcTpV2pPvM9U0zBldsJAxtmqqZ6MjIosxugK19GtuauN+5em6CZdYhI+qArrEXQ9cPloyfbGJrt7ZxklkczZpp0ds0ObU46Zl+7B4rwtOlkDzFjhekgJiFMDki0czwwFWBtR/LOzQ9Uh8i65Ndta1XWHYu5G0+QxFl2geKpGad1ZY5vW+JxqdGfmpJkp+zAKWLNsawU4xeulwZytM9QTdbD93cRhzXRcuMyY2KKU6CLbObTZ4QlYNAcsa+Z46nQRimItaiSBs3KJe/t/y2BzJBtrmbn0JblOT1JhkBnzrAziht6r1GGWoRsYdUZKMKSOKkLXQak+i3viEOarjlv3jH6WoO+CMBMdXpuZ44WzmFJSx5izzxPxwGvB5+mp65htJJI4K5ok1GIMpbIcKmV14Ju2rvMi2fWeZ5n7sAwQikSUmRijKEVglIxIju+AopaD1eqFfEgGmifjiSvFOH8Elwdh1MwwJEYy49Dxj3Y73nPTG1/w9/9GBv6+0uvlME9vueUWnn76aW677Taefvrpidxz++23c/78+el+Tz75JLfffju33377BLe229/3vve9nNO/7vXK9vPfY+tbdy2qyle/+lUeeeQRuq7jR3/0R3njG9843e/lZirea3cyZ4bhs5ZiFtZtQbOLPEOLyVBEAoYcwA2SDShD4blLByxXa3Z2dqHvIaWJQGNR+PzC7lAk2oggzmD14GE7YcfmRaNGoSjqnaVZpmoYAKh3Yn6/5FR0dcH8So3OMuNYuXzpEuuxsLd3mu3NTUYS2aLbjHMaqvuBVpy4kqKDNBVE1W3RtInxM1Shi9ld0xWqeoEkRO3rcHDOzohgVQjv1EytxhDHTOpaxdGEdY1oKfXiNgxQKuQqzDQxU+/OVHEJQ3S5szZXbO1JFaS6lrAz11D26n9mEinuo6NgBbSAFcFKa28EiZmhFMjVCTE5bkvxHkn1v3carNIowlJA4li5QK9OgJJqpAqp+mNSFOT3bfxfSC7ULmFdFIi+Moqbc6+kR1Km5A5NCe2MIhnrxAtozmgHo7it20BH6TpsJgzScaUkvnwl8fjlnkVN1JQmF5maXYJRsmeAlpzcci07A3TMgqZMzR01J0qXGbtMyVC7REkJm8/otrfYPHWKnTPn2Dt3E5s7pyDPWI3KpcMjvnH5Es/u77O/WHKwGliqMuK5nGOaUaVj0BQEtMxo3hkOKqxIVE2sTbgyCn96Wfj3l+HiKrMeYFU6VtozDj2f3Jt/x4LYri2vJHzaQqJvxHo5RfGDH/wgn/rUpwBnlX7oQx+abv/t3/5tzIzPf/7z7O3tcdttt/GTP/mTfO5zn+PSpUtcunSJz33uc/zkT/7kDXkd17pe7xTxL9DTTz/NV77yFW699Vbe+973Pi9r61ocbb7T6kjcq3fwR/LVyPHzjkeCCq4hvfDl7UIN2A/zWcvR/oJahc29bebdjFqVqqt2bWSMTo5wmUmhL2wWZCWOVzA6bVILx9mKuq9qEdzRJC78o8gUzZRRRslkLGBSJ/HUYhxd2WdV4Q2nT6Mpk8Ut2ywGbC1yqrSuVz0MOEWxX08pFx6mPDP3nyzVO+hixqBhrm0StnD+usZ4/yTYoevqHViXckg0PA1D8C6rRLBwR/L5IVDG9jlF10hkH9bjgOEEkwWbmXeQRIFKMRtMFSQLlGO9oOH3bfO91CDNgDtbV9lg19C8E6RPT7HQkHSUY0bqBJ1y3F360LmRh5jOlfgO/JXNL3K2f47BNrxLtBnSGWPaCLZmpUgm98aaGTkZRdxv1NJIsQ26rAzMnBDWV4p1AbsnZ4cmF8SvRuHKxcTWhnJ2J7O16fdRkhNiEEYpURwdVq3iGzu3afPfBSWKcmgTE82IWyZyGn1H188RYCYSrGZhPYysy8h6tWZtSrbO9YVHK1LXk/JsglCrEpAqfHNlPH0kHIxhQ4gTu4r2LNY9e5b5Z7ecukoH+J3WKwkdvhpZij/3cz/Hww8/zLPPPssdd9zBr/3ar/Hxj3+cn/mZn+GTn/wkb37zm/n0pz8NwE//9E/z2c9+lrvvvputrS1+8zd/E4CzZ8/yK7/yK9x3330A/Oqv/upVjNhXYr2mi6KZTUa7Z86c4b777vvOvqY3IGj4Vtvjds7wKF93uE18TmhRDGvAbGaNMWdUNQ4OFqyKsr29yan55kQcEHGueOE4k6/BpcW8GDqMeuyjivq/m5VbsQgFNmML9yudkSnmGsMSgvlSnRg0VL/orA26olxaLtB1YXN7h/FoRUodxSy6v0pO2VMIcMPnMeQXtc0Fcdu2jAug15Y4pd41j/U4QHkwoSNisarjjCJMjNFOAIWxOgyZ8WSKoRmVIhFn1Uy7E318nKOHL9AHfAzeZTaZRGcg5rNcteg429ywRhBxg1INdGQS80/s1uj4BCY3mcYU9e/jsZRCIgTYiC4Q7xpzFfrOpg1Ag14bU9WCiSvt+cQ3VH5uxnbe551bf+qSoJTIqVKto+tG1proEgzWQ1cZsvN0JRWKdqTOGNh0SD7FvDm7yF0tkTqXxJhlUlcpkQWaOuPykLl0MTOfGXs7xumdyDEMWLXIjJLxLjDw4Gb1blMz7WaH4AxQYuboxLQUhviuvXWUOaPZYHPG3DaYkdgisVqvOTxaMmpmdbBmXQdMspsrpMzFMufy2LG07B7FMUYYas96yCw0cXff8T+/6ZZXnDxzretGFsXLly9fU1H85//8nz/v7Q899NC33SYi/MZv/Mbz3v9jH/sYH/vYx67vJG/gek0XxUuXLvGNb3yDe++994YMkq91/bDdzpNykQNdOelmKmcSBdJ/0awah8sj1suB7a0dNvY2nBGJaxaVFNZZx9IISDEzbMQZCxZqkxe0WWV7jAWLM6FWY3bZgoNdtCzRVSUyJTqzWpXlcsVwtGS2s8Pe1imOioIuGaJD9fy5zlM91JgDozllsgbhoQvafVWjFzcrt8hVKvGeODkmSCZJfC6IkDGqug6ua7IMbTNGJzGNxYtFJy56q0BWyMm7XyOSNGJ+KDGTa8LBmQlddGNmMQduUKZV6mJFlzNSIhNTU3Rxx8SeHibJBTAVUzS6wdb1wXFXRytuMhW8NruUGjZz0VEm9Zlz8qroEDStWPoLauzWv3bmD5l1a5b0dKKsmJGSMcgMy4kqlRrkF6wjdcIgfegBlaJe5NbM/HuaKlUzZKFkpWhPSsYomdrILs1PVGBlymI/89R+Ymsu7JwyNudGzZmSnS3a5olhvjc5zHhTHukchOQiCDZKm5+nq+Qb2khsyVBNjDiESj+n39gmbUE/Jp5eKhePYDE4sWddS9w/RPu1Y6gdK038rb7jv3nzy/Mw/W6vGxkw/FrKUoTXeFE8d+7cNWkNb/TKJH54vJ3/w/YxC4qIhTAZA62sjtYcHq2Zb885fe4mJx0YQYzxoxTC81OP/TSDhzFBZX5RSCgaF1yHgjKeUt/HZbemEdY2yTNKFNmShKxusp3wojUcrTg4OmJzvsmpM+fAXOOYiegeAqpVlz6PBimCeQcz5pI9dxFAvSj38fpKhOM1kfxMYtYYr6eqRVfoM86hKjmnKIgOSVO9kxoqkbPoySM1mJu9OGu0mgQ06vKPZpfWClZv7hiEOXmHmMf2BsPyiPVyyfb2NrPZJlptej0utVCHZdscL7pYLE3MYKtELmJ0pyGXaJ2jiM+CJSq1FHcdshMCfSmEVMTfH7+rhfF3GB4EEecdu/+W0/NLDOKEmdoJVTvoCrV2SFdZB4GmVCfFdNko5skWhYzmhCWl1uySEOmoKSM5OrQUPyejlpHkqSqW/XVbdIBGYn8Urlxygks24UyX2d6CNFMnXomPAEhy3PmhIZkIpxlk6ioVIomGgFj7SaNYa4Nle0Yz9gfj2Zq5vIZVEUb1/EM3BZ9jvaKq1DGzGhLLmlguF/zEsM+HNzb4yleO2N3dvWbv0Fd6vRqd4n8o6zVdFF/qF/lGDM1vlV3ecCmzPh0XMpzgslovOTxYsdnPOXv2DDWnqXMwTsKkgtB5Jyh+sanaOsBm4uZFaEZ0ggreSbq5G2oB38YFT4dwlmkxTw6dttnaOAxcPFyy3XXs7u4xS92kWRzUWaxrS+xUL7gWXcqobsdWIqxX1eHSGQ7BEejmYJAi53Cs7j2qGhCqOCw8mpBDWD8Ub7OSGqUmiio5CVaUw8VATj0bncc9lUiUmEXHXE3QgD1zkum8GvyZ5ViPaPWYIGTjmsPDBfN+xpnTN/nP1ed9E+M0tfsbiGLqEg7XEtqUm5hi+9BFhwonIVRxEwIsuk6HwqlGikgMieM1Rtax1jEda0MCot7pr/CW01/COmPU3mUV1pEyjNph2ZmgSkZyZagzZj0M4Q5j2bxLzN7taxZSB6VmLJwKinakBCQolpGuGdYnVDJkd2mqkn3zEX6kqomFwvJyxq4IqVP6LjGfZTY2YGOekM5NFkxcnqSRFtOsE93q0MC6ySRjjLHEwdo4KonFmDhcw2KYMeoMSS0FQyY/3mLm0WUmjKVnVRPrmhnXmX9ybov777jn27xDV6sV8/l8Mtje3d1lc3Nzur60nM1Xct3ITvHg4IC77rrrhhzrL8N6TRfFl7KuJVPxWo/zfReEr98554CR9TBy5XBB6hKnz5wmh/u/xsWthdYaDn+6tXhjpvpwpnWATU+Ymp4wOsTRlB4w62In7cVONHbZIZr3AFkJ6zZhXSr7BwvAOLu7R06+o3bo14ttg2drFIch5oZuXO7nUUzpzO3bomQyqneJBZdOZBNWlujDrWClFukLERcVGj4P9U3k5EzYoXqhzwV2d06zWo3UumIYK5YyOfds9B2S+zA88GLe5pBa2wXWz7tJLPC6htbK8vAAscTpndOe8l79PXb/US/cWINavcuz2AjQiqQFM7AdO+aUTXyf8AI7dYfxSYs53JsVUo7uT3FdqUJK0aE2vKBBtFEs73vj/03KlZIiaikrqh3kyqgdORtFe+iMSo+mQs2Jqj2SHe4mZTRV7y4DFi0kco4uLlIqvGvLUaGFavH3IOFUf5VYdnlMiW65pGYqL6wG4fI6oQce7ZQkkzKkDJK6IGsLRM6mmlsJVnP3oREYizGQUHVWtkoEQtfR8RYNY28TRlWqCOOYKChrywxj4qhkZtrxL2/ruPUNbjX2fN6h6/Wag4ODKRB4uVySc2Z3d5etrS3M7BVloN5oos3r8Onr6wVX3/c3pCim5CSQHzk4y+8OX6KIsLt7Cum7KHrQiqDQ5mtB4rCJWuGWaybh0+nMPnCo0lywFixR7466INpU/OI8EsJ0ywHBujNMAoaqHB0uWJfK3tY21vUkEuuwZfOurZmGB/En2HtVlSRpKo5DEEMUmwrhEMXOsKu63KEaOzgk22DTUqGq0iUXUJfqc0FRWFd1ZmY40/SpZ3N75rQeg3EAq2tsKOyPK8ahMkuZ2ayjzz059aTsEJ4n6cXGI4T5y8NDahnY3dpl3s+doVgCsmx6QQGr/hqay4wKk5k7tJkvE9vVV1THKGBeMKOoxQTNLee8Q8yapuILQh+fu1Un/jSEIBlI+L/d84Z/w8Z8gSVhNCfM1OgAPUQ3Y6m4x252Uo1lZx1bEiQL1XoQz5FEshdKiwBgUYpGx5hDtkPCUmyaCCPurNQqYfbtnW6LIVMRMGettgKGyVTwRrKzeYu/vpZ/aHGMxkStkXmpoqjNgqgWxt/xudbQHlpKVPWUmUrPqJVaYVU7Bs0cDom3pMw/u3vnRcN65/M58/mcm266abptHEcODw+5dOkSwzDwr//1v0ZE2N7enjrKnZ2d74p+8UZ2iq+lhAx4jRfFG52peD1rvV6zXC558t89xn3vup0/314xQniUEr/E3s9FpmwYhBtqSiI7nDRBq85ctbA3qzFj5ETRwqBKRsSlDS2aSQOCG2MeVhBWh0ccrFac2t5hvjWjk8RKmboa9FjgPqrRJ88uRL3opSiuajJduDIRtROzrlId2q3m80QvnuGGg2sSAYg5okh29mjx+V1Wh7wk2LCix8UyR6ckCvMk5LwBHdQZpB0QNepQGNaFdVlQx0qfoO9mdMkF4HUsrI4W7Mw32TjlFzurbU7o/7WZI+aMUY0OHYtfLjtmhwp+H2nwKk7IkZgR+msFItdRokhmwKpigyIz/05MUVMEimCNfXrMOEWFvc3nuOOmxzEyNRtaEyZKtY7UFdQ823C0WWhEeo9T6kaMGanzolZNkK5iGiHASaa5ostD8wQPq7k7jKXQtornIkoUP1OB7B2kilCSUc1N3VCHxosAACAASURBVM1acoYntlgURgu7Pos8zzY/1BrFrVoURu9Gtbb7uTGFqm8AKjBowdSRmFJd9jMWY6huEbceModjx9/eyPz6O166EXXf95w5c4b5fM7R0RHvete7qLWyWCw4ODjg6aef5vDwEFWdCmWDYFv83Etdr3eKL329posicEMyFa9nlVL4yle+wje/+U26ruNHfuRHEBGe40kusJjIEb7H911sCnKGtobCohuMzsMmV45g5lkU1ZghAgF0+tzNZ2VBwkCCYeoJGsvlmsuLI3bmG5w+dYY+ZU+aFz8zJcT7ko4v9NK6VbxYVr9QF5NIkRC0OCFmKC7rcDJQRE9p2KHFtZzaTATctHyImWOHF30x75DAcbeZ+oyxRKHo1fWWo/l9O3FIssRssJeYIXaZPs9dM5gESmUYCsvVmvHKIdmg6zJ1rKx1Sdf15NSRxTvCVngJaYQG/EnMBVNAoo1VOjFs6vG/Baag3xYiPUk74nGr5ZLl0YLNjS166UnVJvs3bQPZeGRucgxAkvKDd33eu64Mqj1k98vVYG1W866v1ExKAcnnRCke4Ev2ea0LJ7PPBgVnlCafPyqdB2anVqz8OUCp4pCohbbUuUbuWqPiDOqiPmskiqSqR0SZETPJHDU+ZrziGz2P6EqYelfYNl8lyF5Kx4hiAZ8WM7R2DNWNMNy2LXStmhiHzNIy65XwX5/q+fnvvzHd0ckClXPm1KlTVxH8VJWjo6MptumJJ56glMLGxsZVHeV8Pr/mjXwp5QXTe653Xbly5fWi+Pp64fVSi6Kqcv78eZ588kne9KY3cf/99/P5z39++pL/LW7lf+NrLKyQriqMMR+MmZtFV+BAZ0Td4EUH6SexuASkhB1DkxbZgBJFtZkFVKCMA8OqsGbk9Okz9OLyi9Gc/FLM8x5KnEuN4tTyG4sSCRoJrZ5EMRrMkkOoKcwKJKaja2UKMq4RKDyaYZrQAkNheh80xPspchJznEOp3hVlOSa75ChG1dq/vWDWSM7oA9L098zvm8Et7gqsj9ZIVW4+fZo+zyi1Mg6FMhYOV4fIUMkp0XeZPvXkPKPrOp/52Ym5IF7sWtMmrXMNmLnNRlvX2caA0xwwQVkPLA4XdF3H6VNnyWGV524lMcc0wDQea3Ec7/jf/tZ/Q7+xotSeJIWSenJXGWt2CLV2aPaKqSlBp1EMXVKRxX1xVRKSZJoFkhW1bkqoMM1+nOSQqtHuj88WCYG9eBdnEauldBiViov2s+SA0tNUeE3Vz4FWLJNb7YmhwaL16DRDR6EmiyIdCSaE+byEl60Jw5gpEmHDBqvaU4pwOGRyzfzOGzp+8C271/07/kLrxbq2lNIU7tuWmbFarTg4OHjeMODnI/Rcz3Nez9rf339Fo5te7fWaL4rf7U7RzHjmmWd4/PHHufnmm69yyxGRafi+Qcf7uIXf5+te2BDUNAy6W16ik1u8yUhT5yjB4LNkMX8K+rt656jhjEMStKqbHTdoqVSuHBySzEipY3tn12ElDW9QIMdcsiPmkubzxBleDMWcrdedKLQlyDqmFrNNqOpnXixRq/uwDqruvYqH/GaL+WmNCCDzgpUCKvNOj6g2Xlg7JCBbAmr0zs1zHYLFUb0g5tbpRtRTF3PXo8UBy9XIztYWm9sbLnupQrKOjdy5f7tB2vYTGcbCsB4Zjo4oZSSR6LueeZ+Z9T0inTN821fLYqYaMHLrBEVbFx+FXUC1sth3SPfUzg5d10fhFAjvUgk2rYjDyliTwnhRPHP2Kd7whguspYOME2yiIDdCTEmJnJWxuuRitOQdXqouws+CSOcoQTJqTe6uI5mqLaEi03I/RSQKWodFMT3JEkUcvlXcdN7U4dQRt7i3MOP2lykxFw9dblBraw0dojrErhKpKgDSezZmbAzqSVapSsh1hLV16JgYcqJUYVU6DleJO7vMv3hnz87Ojemw2nopBUpE2NzcZHNz85oJPQ163d7evqEzxaOjoxvWdf5lWK/5oni963qs3i5evMhjjz3G7u4u73nPe5jP51f9/FuZrHeyzTvY4//jShBQstvAmZP9vREIQgIE1hhzFFVEcsCn7nEKFrMq1yaK+n2TuID+8uEBOozMd3fYzHOWFy/F8cPyLbrTBsU2NxePfQqXkSr04s/nWXPZZ3vCpA1U81nTYC6JGA2y5umClsV39Vo9nLcWDxlOlXC/ces01y16d9WkI310ulTPW0zmvqo9x1mIpbokpIuZm1YvhtlgvV5ytFgw6zc5d+osWY5JQ0zzQyfgEF2mamYmmfnmHNmI20yxcaAU5ehoQS3O3e27jpQ7NroZOXfH5Jz4+CYIWv1CfnR0xPpoxfb2NvOtjWCZtrmlTXNiYErBaHKRpoWczdbc9dY/RaMbTJ0L16WDosnneZrbXoEacVFjze6QJEESSrgwXwjJhV/YDXWyTADqRm4c2YkYc7XhfeQequsXoTGoBUt+hEqaAqmnuWEQzkwErRLsUTzbUBStfv4m7oTkRBp/H0fzMcMghpVEEWe1qsFq2VGSUEpmOXqH+FMbmX/yw98dzfKN7Nq+E6Hn4OCA8+fPs1gsWK1WDMPAmTNnXhahpzUM36vOPd+N9XpRvM7VdR3r9fo73ufg4IBHH32UlBLvete72N7efsFjfSuT9a9zjqdtzTO2jl9y1/9pMDeN8BLFrbXU1GnpRtzPfUBMjkN+TR1+chNt42C5YHG0ZGNnh1NbpxjNqeqY6/mSSBzHZQ8TB8S88zIL0oSZkzowkJbu4d1i8EXcB9PMi6SKU/nV4dXSYF2DUoxZztTqIcF9WKyJen5gm9dlI+QO3gm2iJEe7yjVoFOJoCCJjjCIRvhVNRlYKRwc7iOS2ds5Q5LOLd8kWKQN8rTjuaAQsGzYrrVkDiEcc/oNZh0w2w7CTUCvQ+Ho6ACrPvXtu54+Z/quJ3UdglCGgaP9A/r5nJtOn52Kn8TrE2xit4rYNMNtl6pmD2cod7/rj6Evrj1NCaRSJJFEUeldaF8dOTBzwX0R16ymJBTNlND6WZBk2ubII8bEC2EyzDKmGcttjpcC1bCYAeZgozohRuMdUwu/X/OZdpUaHWJDQAI6N7DkXaMTt5xZKtZTcCLXqMSmzGeEGvNd7xRDA1u8oBZNLMZMzcJqyJQx84/OJP6Tv3Jt/qUvZX23sxQboeckxPnFL36RW2+9lWEYvo3Qc1JPea2Enu81c4Lv5nrNF8Xr/bC/E3y6Wq147LHHWC6XvP3tb3/R4XTOmVrrVbcJwge4hd/mAqNWxJqllV8wDCLPkCg+ceFpkUNxFFUftHlMTwb1GcWlwyO2NjY4d+acW6rRBOqGViiqdCnCWdWft1mFWRQ781DFKNQhTtcgfdgxE5CUGEuhD/cZggRR1SHEUsOZBi9cKVIsGJU6VpBMF16wqkTihHeMfTymmEOJfbxui9lhCgguBdmmdVGmytHBIZSRne1dUvaMxxQzvjQGb6U6NNnIMim6zNxgT7v6TyAYqF6wO/H5addlyHNkazt0hUodC3UcWS2OqENBHXNma3PbO8oT80Yn28T7357LBNFjj9MGmwpw291fYrZz6ISVOnMSjcw8lT45wGxSMHH2aa29k3AAlQ5NhdWokLxrNAuGqVoYdJsTdqJDUzUsCR4JFcSaaGbVnEhDikBsk/DldaJNNfeoHbVD6ZAkkx9wjU5SlQmVgIiCCj1igSDvpBDde0ds8R0pNTEmNz8YVFjVRK2Jo1VlZRuclcy/uDvz5tu/uxaPr0ZsVK2V06dPX1X0no/QM44jm5ubV8GvJwk9r4bxwKu9XvNF8XrX8xXFcRz5yle+wrPPPsvdd9/NG97whmv6Ir1Qgd2Rjgf0LP+7PTtd9GrMjCy6LdcaeseQku+iPXA2H+/Ug4k6jAP7+4f0uefMaTcGwAgCC14sxC+r1q6QMQN0XaT/5wkbhOm0z3poSRtIyC6CdSqJWvAuImBW5yMynbueIKLkBilW2Jltstg/pIwBe85cS7jZuwenmLNLW7FKEs9ffMPQCz5b9UFT5BnC4eERy+WSvc1tNnf2nOUaZJ0Ux9KQcbTj5tizmPrJTySaViijaJo6zGn4/SYiTCPaRIHLlph1MzTPWNYjqhVObe+SSYxl5OhggZbq89LZjFnu6buOrstBtiKSNSLxI4zME7B5+llO3/k4Y/GOr+bQGFYhZe/OPNi39ygnXBPrX5fOBeym5DQjdTkSudy5XOnCeyDm0/7s7k7jlkpR9EIfmwWlm76HarFhMj2xyQurNq2+OasdzdPUlDDEj+JpycOzFQatZHpUHNmo5s44o7eVjGYUc6u7UhNDjfzDMbEcE5eXS969ucOnf7R/2Xrja1m11lfkeb71Ob91pvhihJ79/X0uXLjAarViNpvx0EMPsbW1xfb29nUZD9x5553s7u6Sc6brOr7whS9w8eJFfvZnf5YnnniCO++8k09/+tOcOXMGM+Pv//2/z2c/+1m2trb4rd/6Ld797nff0Pfietdrvii+lE6xzRRVla997Ws89dRTfN/3fR/333//dcEk36nrfFve4V4G/l898M4lCpxp7OAIGDF21M0EO2lcLCVRxsJi/5BRhN3dPfrUOcmmOpPUu832p0UnRcBaAZU6cBc+nADuTJMlMVameaLEcVVd+9W3OVfMGhvUWqu5rVnMLJN5YcoBmaUKp7a3MN2ejLeH1cC4LiwWK4aidJLY7DKpn5OlY5a7ydWsM5lIK00TWIaB/SsH9P2Mm/fO0ZEm2UdSJ+5IvKkWDNGkx8Wv3Z5iMyAxq/VCF5FVENFQdhXsCkxG7M02bhzWHOwfMp/PObd7DgkTglk3R+ZER2nUMlLHwnK9oAwjIolZ7um6TJd7cu7jnAzpRm79K38ayRCZmsDTUHyjo0nREn6liDNB8Q6vUinrQuo7UreJ1pMeo9kLV2p2gyHLiS2OWuvWciAZ7XsasCs1pEMZDeF+NXePuTo0O0g22uaSAc2KM1qrBhvVJ8wM5lIR0+ToQ8KPjxtFmGXGKoyWGEZhbZnDdWI9JH5WLvHfvvcNr1ihupEzxWtd15qn+J0IPRcuXOChhx7i/PnzvOc972FnZ4d7772Xf/yP//GLxmX9wR/8wVVzzwcffJD3v//9fPzjH+fBBx/kwQcf5BOf+AS/93u/x2OPPcZjjz3GI488wi//8i/zyCOPvPQXfgPWa74oXu9qRfHChQtT/uL999//kr70Xdd9G3x6cr0/n+VpHXnK1r5r18heNKY5TZupNZhJxJmdlw8PGceBU9vbbM824gLp0GWaugOJi4/Paqo12Ydf2Gq7CAaUWBuxJ3SBzQZtsnELyNU80yrSN5wAITgJRnC7sqrHxBeqM0hVONYUBsQqJnTdnK1+w4t9AUzRUlivR9bDkiMtJOvpUsfGRo/kHiFTS2W1OMAUzuyeJktHNpk6LFHITUgfwvvOkUy/Xf1+U9eZ3IZOEk6RjTllCngzY5PsQswLZHPkMQWthf39A5LB2d09UnTsUoM1GuxaES/uXZojm3Oohmzhkpoysl4XVsslZTzweWnO3PX+P8XmK0rMATHzeSDi3qXixg0pGVplgkRLLc5+nW06zIlvqJQUQ+Rwj4nH0JxkODb4toBZVdx6om2qVI+LnbYCHUJ8h0hbwXOWdbbMiPfAziBVWlqGu9J4jNoYcWtqGp6nQhnieLi591qFWhNjTazGxGJInKqZf/muzPq5Vzbw99Uoii93zedzPvCBD/CmN72J5XLJ7/zO73DlyhX+5E/+5CUlCn3mM5/h4YcfBuCjH/0o73vf+/jEJz7BZz7zGT7ykY8gItx///1cvnyZp59+mttuu+0Gv6JrX68Xxetc+/v7XL58mZ2dnRfNX3yxlXN+UXnHf5Rv4n+qX+dAdZorHcNPYS8WFxgMDo+WLI6WzHe2ObWzDUFESE3gbW1O6DZmFiQWT17weSBB5ED9sqQk1/bF82v8DHOIq2gQYKrP74Yq9JkpBNfUcwAdPmTqDiUKVKeEkbeRXdCGqJFjFtW116neUXUpI7ljtjEnb/i8cBgqVkZ0NbIYl6zWhSzGZj9n1m8gKnQhq7BIrO/ESTMp3svcCiJAEH1SPbbaSzVmk8VddBpMPYUZVZcuiLb7+U5c1Dg6PGK1XLG7s8usm2HR1U8ifaKQYhEjJQHZOnGq+al23Yx5nvkmx8Cqcuqdf8787CGjKmMxUldRdS2iGUhK0JxlUPcftco4uoOPdM6o1eTwYwAQwTBNmAXDtDbz+YTVBCmHXtBpRy6j8A+rul7D9YBKFFFHNJT2PfLLT9WMtRBjw9M1cAPwURR3WwIknyDRQKWj1PBPRcMU3gvooImhJI6GxJUh8WPzzP/y1zs2Nzv+zTdf2aL4aswUb9Qc8GRCxt7eHj/+4z9+Tc/9Ez/xE4gIv/RLv8Qv/uIv8swzz0yF7tZbb+WZZ54B4KmnnuJNb3rT9Ng77riDp5566vWi+Gqua/3y7O/v8+ijj9J1HZubm/zAD/zAy37uk1DsC63NlPlwfxP/6/Csz0lCn+iZiTG/IbEcViwWa7bmc86ePhsBvjmIMeokiVYQmxWcudk2JL8tYKqcImVDXApSi19EVJ3D2o7XZo3Ogjgu2BIMVAuItDncOOvTC3GKoqQB+4q3KKHt8zDhrHhhwbtHJeZ8FsL9mHFSoSeTus7tuoaR0xvbbGzOGYfCOBTWh0sOq4bwfkYnPXneuWA8Ot6kcY5idCcu3lOxDBh0slNTl5BM5BcxpIibv0TRXx2tOVocMp9tcNOZcwGpSrzfx7BtA6tTQOTHon8nDdE2BvX43xjMb73I3j0XUPU8xJw6Tz0uGZXRUyyseGecHQkoYyXPhH4+D8aoAeEiEzpAl1iEWD5mp9BhpmEVF51gcqhUG3NUXHPYbHRMnPVZic86ebKLqQR7FMYipJxwU3v1WTku6VBxtxwX7/tzVxOqKWrCYCk8chOleEFclsRowmKd0VXiH92W+YX3Hm9eX0ljbvjus0+/db3cIPST66VYvP2rf/WvuP322/nGN77BAw88wPd///df9XMR+Z4m77zmi+KLreVyyWOPPcZ6vebtb387e3t7/NEf/dENOXbXdSyXyxe935vynL/Rn+L/XDut2qn4vvNelYErB4fUCmfPnCOnxBiMMYvuopqn1as5BIdlnzmEnrEG9Go0r8ogUyjekwZ0ax5XgEtA/Odtttk0aZjPDVPn3dQ0exMmP9bUilDM41IQY3wGGB0ScZs16YNMUggRJmJMFu8+6nrg6HCf3M04c+oMKWV6nK5eE+QNNyGopgyrkToOLJYLP1fr6HuXScz7ji7lIM7Ey20kIJobDlBbMYzzwd9PCws3HQqH+wck8VSNLncQLkAEzJqEENo5WUai+8sS3Sz4jLI9F8dOORjI1ordH/5zN8vG53oiyaUQ4sG/QodkRbNQ6hheodk/r6EgKbkPd3yePl92k+9qTZcoWM1YitxCCYmF+jwSBEJ3apoZW1JzmxFiIf0QVsUt3RRhGKsTQvp5sEnVO1XLYfPn0HvVCtIz1jYCyIwaMKsmxgaVWmI9CuuaOFgl7iLzWz+SueuNV1/mrnXedqPWKw2fllJuqJvN9ZqB33777QDcfPPNfPjDH+aP//iPueWWWyZY9Omnn57ml7fffjvnz5+fHvvkk09Oj3+11mu+KL7QL8c4jjz++ONcvHiRu+++m5tuuumG/yI9nyTjhdaPdLtcqCN/sl56l1cr+weHVBU2t3dYLdcOQRoOrSUvJU6pTpi14td0jN6JNHeRls+nlfCb9I6yCag1xNGpGrU6tFkDkm2pGgTDMFWfk2Vt8KFfzGv17pCATaUEqWXqDon4qROkFiPisLwAd9E9aXHCjmrhcH+BFWVvd48uz5y4wwmYVKPomNDVTO4zdJtkwgC9KmU1UtcDi6Mj6ljJqY88xkzfz3ynH8VK4nyoJ7WKEkxd4+jKEeNqxc7OKfqu959Wm0g8Zj77FTQ2AoYV/25lnKyUYp7rRZTjAkxrISs7f+3fYTMP6Z00h5JQq5OeUMXZneOgdP2MlLvp83bhu5txD9Uwq6ie2MWbgXSudUzHc8Hj2XJC6BgpYB70rGHjNk4hlG4D10hhmDAWYSwjqevoZxuoOVmGsOOr1UOx/VjOJq1VJylGrcZIQkvrGhNLFcbi88P1UeLvbGX+uw+8sAbvP+Si+HzM05e6rjchY7FYoKrs7u6yWCz43Oc+x6/+6q/ywQ9+kE996lN8/OMf51Of+hQf+tCHAPjgBz/IP/2n/5S/+3f/Lo888gh7e3uvKnQKrxfFb1u1Vr72ta9x4cIF3vzmN/P2t7/9eX+BbsRu83ot4z40O8OFofDl/UsM65HNU6fo0oxSS0BwiVKrw57BYGwxUw4/uTuOmU1yDtSlFT5X807R9ASJJuBSJGG1ohLznnoMl7ZwXcyLWQqtIGbHnp94wcsaBblCloSFr6lFp5hiBhlGeF54JjjSL2YWrNHl8pDhaM1se5vtnQ06864xx4yzyUkmHSKhXVSm/EEq9JbpZplutkFLfK5rZSwj47qwOlxipuTU0eWOWerpckfOeRLPi8F6uebw8ICdjW12T59zlxplIuK0+6FCEp+FtW7Ru/+Q2DQ2qwgtU7G9h15mYPOvfwnZWXpIbo0QXm2fdQ6o3KHk3CdyP8M6QN09BjE0IpwMJXWd2wS2mZ0qWhW1MRIvBGHmP09h00ZCVELaAYnOY7WcTkUxjRmiw3lWYF0C6eh71DKlHs+pR1PEunCz6amBiqh1FKth0eaM6aH6d3WtQi2JZU0MQ+L0kPgn78w88M7vHXjuL3uneOedd17z/Z955hk+/OEPT+fx8z//8/zUT/0U9913Hz/zMz/DJz/5Sd785jfz6U9/GoCf/umf5rOf/Sx33303W1tb/OZv/uYNOe+Xs14virHMjAsXLvDEE09w2223fUdGaStmLzfe5XqKYpN/fP+FC3zzrbcxbu05HCpCkuysPLWI5YlZjirNd1Sji0nSCiSIpTAAiI5EQ4wNx2Qciww9V1VjqcGgPjd0M27vPrP6XM7MvANSZ7lmdZhTQxNo4dtJzN0yEsSbgEkDdsWOC1w5MXsclisODg7Z3fD5qUV3asnt4Igi2Fimkzl3+zMINBKsW4lOVwSkhCuOdPR9B73DrgbooAzjSFmPLIcjTJU+Z3LKlGGky4mze2fpLIUfWcw8JzKNxdzUi2QysLgtE5/lySIosfmKEVGDkrt3nofbL1I1pAupddKeZuF5kwUV6LsNrHM3HAIZUDJmDlXqSdQgjCJMBE2GepQISYxR1dmvZlStkZri3yHrXIxfBJoNoaqEpAJQYSgj6zqQco/kjlEd2m2saQOqdQ7nx+tX3Oy7mqE1U4JwMwaZZizeIVrJlCPhvlnmf/qAsLGh1MqEwrQZloi8KnZlr/RM8dXsFN/ylrfwxS9+8dtuP3fuHA899NC33S4i/MZv/MbLOscbvV4visA3v/lNvvzlL3PmzJlrYpTeyKL4YvDpSUPxW265hR9/73t5B8b/cHiRpZpf5DUyEc1h02RK1eNEC+EY6rSARKtCFxT8lr5RrXU1GavFBfjVH0fIN1rRsSgmThSJLqhAnzr2rxxSuyVdnjPvOlKeIblBojFH5EQBTEAUTme5MhlgWxTITqEMI/tHB4glzp46w6xBgZUpKQNjKqoWUg+xyCRUjp1q4vhiQZYxm2abOUgwWYFgk/r8MtN3GXqQTTBVFoeHjKs1fZqhRdn/5mW6nF1wn3v6viORSWLT+wTHbjUSMpWpq27nofiJRsG26ObTHc+S3nkhzLIzZPf2JGzYai0MxdxEPDsb01DMPJHCzDWdmiNSbJLi+HmU+IwQJ8K4obqL/lVwYlYwfaqqF7TRqEVdKxgsZlJCUqaOMIwjpEzXbYOK6wvNZTngLkoaLNVivjGrMaeu5ibjWv3cxpoo5Vh/WMbE9irxK2/P/Kd/9Wr3lpZ23/4E5wioKqWUV7RQvpJF8UZ2igcHB6+p2Ch4vSgCcOnSJX7oh37omp3gb1TQ8ItJMi5fvsyXvvQltre3rzIUvxX4O5un+BeHBxTxnXUp5ubhEJpDnS7CTnSwKaTXi1+KnT+TXpEodKU6g1Ar04yphmtJs9YSosgRQveQtW3O5sxvmiNFWQ8j4zBS1kuOiiKpZ7P3YpFT7xmGAR9mPe4O25zRGmRYK4cHC8pQ2NvZpUseiNsccGQqon6+LUnCdSv+Xko8T4oC1Lo1KYC0rMZAT0N64V3lMTGoucmouZH48nDB9tY2p0+fmky5MahjYRxHxtXA8mABanQp0/U98y5E96l1y2GR0Oa6ZmEOLkHMYfKXlZ0l6a8+Hi+rQ3CNH/i8dxwUsj+PRpePeVKFSzK6gDK7KBZ+e+0UqV74PFrMv0PF3GS+EZ8Kilj2sN8IEtbshTX3oXENpGA9FGoZHJoPF6JS1Ek+tPxE73RVZxSrPr/WxGhO3ykljMFrZkTQ4sSa1drPuywT7+oS/+Pfztx87uoi0IpQKw5mxlNPPcX58+d5y1veMiXUAFdtTFNKr1pHeaPWq9kp/oewXvNFUUS45557rovGfKOK4gt1ikdHRzz66KPUWnnHO97B7u63Z7v94GyD5zYrnz08nOaGCYdRa0giUszqSmNzBqOPEybiTWfnRs2JUiD1oNVnpmrNbcaCMUqQeY4hytatpuk5oZNMmmW/WM7c1HsYKlpGxvWao/UCMUVSx1bXk7IXjESeiqEYLA8XrJdHzOc7nDl9ys+1elc6zRppszpIxUiZSeIBLuFomLCYTV2qR0M5hClRkKweF0GR49fYgoHLMHK4f0jfd9y0dw5J6ercRIWUO3rpSDPCgciwMeKmFmtKPfS5aU50uXcrt74PMpBvOlJscMy8MdO+Yn/zMUqvLptw0BtDGIrvJFKeQ3fCE9R8AqlRZK0Rr0zCqN3F9RHeFgAAIABJREFU9D4f9u9Qaa+dENmboZqR0Iw6lO6PgURFA4WInMQklKqMYyJ3c2biEGhRRasw1kqNwklSSnJmdJ2clTy9wixRaAJ8oRTvGEsVZExsrhK/dHfiF//Gi1/Cjo6O+LM/+zO2t7e57777rioY39pRtr+3ThL+8hXKGxkb9VrLUoTXiyLAdWcqXk981PU870nG69ve9rarbJKeb71vc5tnivKF1QDBCmwXKMHjoVwvaAjN5FvD8SQSytVc3hHzPLU2h3TmIlrRlNCqZEsOc7bOaRKnuwNM1NooWMczRPCOa5Y9zd26LWTTjzOsR6gj66M1q7pw0b509CmxWg7M+zlnT58DdU/WaUYXc0Nt/yYKZMzniJln8yiVMDBvYcCIhTylxR67cYCCz0MDNW6G31ocKi1F2dvedVjUgvQTc8EmlTA7ftwEMeeOXjI23yCZw7NuDF4YViuODhYQZJ5Z39N1/meyhCVFH/hzdGv0AkXEhRWP6JU0QzqJrEJo9m41uuWaAnxVD4GWmA237EsNUbyYt9/V3ElGFWpy03epnVu54XNag+iM3XZNMdZaGVeDU6+6GcXprWHB1vkcNfcOzaqjG2OBwVwf6WHa3q0qiTI6ZFo1YcWLIUfCO2eJ//5DmTfe/J2LlKry1a9+lW984xvcc889zwsDfmtH2R4H3nGdLJRtA2tm5JyvmlV+L60b2Sm+FJ3iX/b1elF8CetGdYptqSrnz5/n/Pnz3HnnnS/IeH2+9bO7u1yp+3zd3I5NJSNuXBqekiDJNXIWsxqZdvwOqWq1oNlDLeL46ol5omjMw9rFPi78HX5bFpluS5FZJ1WOO6gmcG9kGbyAJWCWe3I3Y6Mn5oOF/cv7DFqY5Y46jFx57jI5dfTRTXa5cxNvjUKYmKzShPb3q89XTKbZoOBm0cnkOOTXHDJNtUGoeMdpsFosOVos2d7cYu+Uu+OImv8XGY52ojBn4rXEPJMTUKhjk35+s9yB9GzONgPeNUqt1GFkOBo4KkcOvf7tp5G9Q0Q7l17UxFAGpGZktumwtsXGR9UlHWRK0pgnR+dXPYdQSFDF46ECxqwhjLQibvJt4MxjiW7RvysV/Pk0IPb43Gv1IpfzVugamxWboSUM4ONYFZyEE+4FGd+AFYNVSdTRWOlIHZ34lDVjmthYbvJf/mDiY3/zxedlV65c4Utf+hI33XQT991333V1ee2+Jx/TCqW/v1fPKf1X6bhQnnzsjRTSX+sqpbCxsXFDjjWO4ytuZv5qr9eL4ktYN6oomhnjOPL5z3+em2++mfvvv/8l7fD+s71d/uxrzpDM5pR7KxrOkTLNERUnSjTB/mQKbvg8UTLVRmdkGlOKhqpA8SLRIpxCsu3F5gSEKeqdWCI0hdpIJaE7bPNHvBBF/YaiHB0uKMOane1dNmYbXrAMsqoLvQePWxrHQqow793vtJ/NXB6hKRx7iJlnnJ8FWSZu8+6QKd3CvWBDJB/s2yzCuBxYHB4w62acO33GL961WbE1Nx2JYu+wpDUSjR0zR6XNN839Ua295tggtNeJCLPUIbMMc//p8r3nWb91TTV3FKordROFnJEGNUekGCqYdD7r0+Rm7xoNnTkrtMYzeyGMiK/qhRCc2GQkCjWMFJzQo9QgxkQsmaR4DmWxXvv7n2aMwWAmNh5FXBvrx/DAZzWPeTJz4b2Yw/ZVU8gxEr3N6C0hVRkWS97KJf7z73+WM5uFL35xi1OnTk3/nSS8lVL4i7/4Cw4PD3nnO9/5glmm17uut1C2LrM95pV00blREpBXo6B/L6zXiyK8pKSMa3Gi+U6r7WRLKbz3ve99WTu7LMKHFlf4w1tu45ulYNWzCCWIJxowahK/6LQLuuFp97USYnyhFGvNTJAM09T9SPh1tY4qiYSAPTxMW+dIKxShOzxRKIj5IwHB5grjesXh4pCt+Sanz5yjagpJR/yXEhspYRs9ee6PoxhWCsO6sDpYclAGsiX6gB/nXT8ZbmcJAkwrsjDlI4qBVJvmeZhQS+Xg4IBajb1Te8xS5510g2a19aQ2pWZ4R93kFEGeCXiyeZwSMobUdhSN7KP4TNBad+sbmdU7v8HyB56DCP2txZBZJtFRRGEIXepQsOxzwUTGOochlWPLNRVAU4j1JQrp/8/em0fZdZVn3r99hnurbk2qKsmaSpJlDSXJsy3JBmIgDe4EYoYQPqYAIVkhTrAbhyFAmia2Q3CgHbrB8DVJHBJIJyt8kNVJ3IF4BToxNMGWByDGqAZJpVmluerO955z9t7fH3vvU7fkklTDLakw9aylZUsq3XvuqVPnOe/7Ps/zTgittHZb780xaEISqVK1MDpAC8/s+LSz6SiJkBoCrxUwKTNC2vVQNkEJZdrYsVZoQlSikHjEGrQ0885E28DvRJjKMDGVrKx7dMqQu25u4U239QLr0VpTqVQoFAqcPn2a/fv3E8cxuVwO3/cZHx+nr69vRt2W2eJiRBlFEfv27aOtrQ0pZUqUnudN+W+bhWbOFGHm98efdCyS4iwwl0qxWq0yPDxMHMds3bqV3bt3N+UCzvkev9XVymfHaoxraW5ktmUKnqnOlGcHQmbzgMJUSr5LsEGkCkpl7QwoU0GZvYtmV6FWRrzizPEB2JadJR09safQGfGVrcjSWDcJKokoFUoEfsjSrh7ctltfYS0QRt/iCNUl42CrI8/PEGYziBbXulQkdUkSR1QqZWKZ4ClBGGTIhAGhn8Hz/FStarPPTfVoZ6+1UoVarUZbro2WTNbYE2xb2NlF0BDYDRieMmukXIsU93piQsVqBErm73XqCQXERKSdj2s/G86vbzhLZfsJIqVRdYX0NWGYNdmi0jdtcs8zFpyWABR2HyJEcYKQiTXmJwgRGA+kp0H6NllOG5LE2HWEvR6UNq10U8UHSGHi/Uy7XaB1SJJI6kmC52cMycaY1VRSoEhIhIenfKRShuyUCZ+P0QjlEWnbAtaA9Gw0nocfC4gFIvGg4vGyTsHv/6pHY7EnhKCtrY22trY0+aRer7N7925qtRq9vb2cPn2a0dFRcrkcHR0daUV5KdqAjuTOnDnD3r17WbduHStXrnyePcQRpCOcRiHPXImyWZVivV7/qWudwiIpArOrFGdKiq6tc+bMGTZt2sSyZcsmvdZcidH3fdrR3L20nU8fL1CUtvXhGVWgkgqtErQXYMdX6MTOFJW2LS8PraRJhYGUIANbaXlYc76aqKp8idmSYEnQjVk8ZYjUiTLShcAK0AmlfAktFR3tHYR+xmzZcKTnBLlighA9a69wwh5Xaaa7CxF4eHiBT4sX4rfY4IFEk0QJMoopRUW0TPDxCf2QMBMa8Q8+cRxRLpbIZjL0LOkx3kU10X51ohtlCRIMcXhCm60ZztPXcDyAPdaGyli4KD5Tcbv2Ke7foqmvKjF221GSRCK1SslcabM0FyyZCtAqNGpT5ZkFzJgZsud5tj2ZGLGVFsQ6xksUwjeCGax3VFlyVtoGwCtNbA4FjYlok0qTKEkUm4eqwM8iEx80xMKIrozP0DwMRFqSYFq2WmH8kgpibaw+WlrBjxSE0q4ji3yismBNKPjdV3nccs2Fr3mtNaOjoxw8eDBd7t34d9VqlUKhwNmzZzl48CBRFNHa2pqSpNsy30xEUcTQ0BBKKW666ab09adSr56rfHVCnrmGDjSrUszn83R2ds75dX7SsEiKs8BM1KdKKY4cOcLhw4dZu3YtmzZtmnSBN9PzKKVkeUsLd1/RwWdOVCgn0laLVlHqB5BAYhWmRmGoJ2aKGAKJazGtQQYEaUuPBBPybQlJCPNU71mywLZMRQMhePZrXfC3QlMtVYirFdpyHbS0tZgWrGogNzuDUzb8O3DkoZ7fhjXiGDPfS72KbkZoKzIPQUuYMQECOWO50FIho4Q4jigWKiSRRHia1kwLGZGBROEJszVeaBO9Zh4SzD5FE3Bg1Zt2Xig8YbeF6HS+Oom4sb+3lXI6zyTlQjyg2lvh1MsPEcURngjxgowRTCllFcRMiGCs2MWNfqQlJIRRmZqoN+MrFCLEJwAPlLTEF0uUkibZSBgVrRYeHgEIY59wuxBjJVGJxvdbTFVpBVVSm7ansjNEpW1L3s4epVUyx9LYK7QCT5pz5UnPPGQlAlH3yNQFb+n3uOeXLn69X8hmAeb6zOVy5HI5VqxYYU5/A1GOjY1NIsrGinI2RKm15uTJk4yMjHDVVVexfPnyi/6b8ylfpwodmAlRNqtS/GlUnsIiKc4K0yEyrXWalLNs2TJuueWWKZ/eZhIKfiE0vs7abMBdV+T4zGiJmgKEjxE+2BRKqzjUVlWoXU6pB7lsjqhapVovIYBMEBJ6IZkgJCA0N3lr5J+Yj1m/X0NbVShsDJypQnStxni+TEtrhp6epcbeYb2GykWzOaLwbPsU8+fazepcFJyeIFzXnkzJUDvCcaHaE9Wm8SWa6iUIM8g4Bqnp7uzE931UnJBEMbVKBRWZHXgZq3YVmRAfoxByQpmggfiISbdluF2HE8k5pl3paZNwIIQRmRhy1Wm0W7mrytH/sA8twPOtqlQ5tbBzV9pAbitKEcKzymFrkNcSlNlkYZS/Jo/UHIFvDPJag/DxPfM91xpEYiw6sUrMdaQEwvNBa+JEon3wRcasZxIglCG+xHYZTMZsgJSKxMbFycTtVTQVoWlBm7mhUAIiY7XQZcGOLo//8k5YPlHsTYnp2CzOh/MRZa1WS/ekHjp0iCiKaGlpmVRRXmjmX6/XGRwcxPd9tm/fPqekq2YQZTNJcbFS/ClFs9unhUKBoaEhWlpauOmmmy74AzVfQQAbWgLuWtHG50fLRFIZsYWhMUuMzsNolwwr0xbsyOXMjT9nSEYmMXEtploqU4xN6zEbhoR+QMbPghfYCDVDQq796QuTqamSmOJYkUB49HYtMfv+nH3Bfq3XMC90lgrTQrWtS0vEbnbpu0pNm/amI8hGYvXdtzRpqBzt+0W1OsViibbWVpZ29xhhixL4QQChnaMCMpYksUnlqZcqaCkJgwDfczPKEE94E+uk3CxOgS9sVeoZAnHEp23cnueGidpDaMW4n+f4K49Bqw/CRq8JEMqa74WHtMQqhULYGD8tBD4+sVTW4+eb+DeFXedl2pQmjcYzlZ9N/dYIG/OmrUrVwxchXmDarrWaRCttlg1LqCuz3trlowptogGlBpRn9iNK43lMpBXz2MrQt2QYSA+VmEoxKQtWeh4fei3ccsPFr/G52CzOByEEra2ttLa2ptVdI1Hm83kOHz5MvV5PidJVlZlMhuPHj3Pw4MFp+Ypni4sRpfsFpn2bJObBZq4xduPj44uV4iKmh/NVd427F/v7+6f1lDWd/NPpHtO55LqlNeQ9y9v5f49XzWoepdBK2s0GE3YMF6vme8Z6AWYe6AG+H5LNZaBlgoDiep0kSihW8qjItBpbgoDQDwhDs8BXSUk5XyKJYjrbOggzLSYdRrkZ2kQIgMCKTCBdzptWjUzML408yFWLprpNl/xacY6NUZ0g2wYPoUokpUIBoQW9Xd0Enm8CAByZnkPOnvAJQ5/WjEC0GDLTSUISJ8S1OpWoDFLbitJ8duEFZi2Uq5ilq8SNSlVo+1RvP1O9XmfMO8v4GwvoDh8T0m0rQaHwlF36KzHZptoGbaPQ2jf+PxvsHdvWtbSVsRJ+Gv2ntW+SjTSgQ6SnUbEJexfavG9iBVpJEhMlgsAP8cLAWjbMAFRKRZxgEma0hgQkZtuGsd0Y20oIqMQjUOb3vhT4iWmVUhW0So833gDv/sWLX9tuHl8sFptqszgfLkSUxWKRQqHAoUOHKBQKhGHI8uXLUUpRq9XIZrOXRK05FVGeOXOG4eFh1q5dCzw/hMAl80yXKBfbpz/FmOlFfO7XJ0nCyMgIp0+fTp8Yp/uaF8s/nS7OR9Tb2gLuXtnCZw9XqeGBcEZqnd6c3c3MiT8coRgl6YRNwFRc0JJtgUCk5KRkQlKLUUlCsZxHxxIlNS2ZkPa2dsIgtKZ5+0uYdp1zJTixiRCknj7teqM2qs0djwvyxm29t3/WmCaT/t7NOLWmXCoR1SI62jvIBFnjXXQzQG1UpAY69Tm6Jcc0BAMgAsJMQC7MQqv5miSOkUlMVKlRqceAxvdCMmFgsk6DEB9hI9/su0hFsVCk1hkx9vYKqtVErAnloTxl5oFqIlrN0y6hxton7PEpF+OmPRJh1J0gSLRZCCbtOdESEB6JNiW2Qph2qt1+obQygphIoz2PIAhNGESijB/R9IBNwowlds/ODKUyW078JEbbHFuVCHzPhyRDRpsQAK8u0DXBf+iD9/8KTIfbTp8+zd69ey+ZzeJ8cETZ0tJCvV4nSRKuv/562traKBQKFAoFjh49mhLjua3X+TxuKSV79+6lVCpxww030Nramv7dXGaUi0KbRcwYjSKaNWvWcOutt864VdFsoc1U2JoL+e0++MzhKpXImPoRphXp2pVuVgfWlI5Nf3HzOedfkxNf49SWgQgIswGRrpOoiEw2S2umjSROSGoR1biMiDRB4BP6GTKZkIyfMb4/c69N/X/C2hpcpWV2D2q0NOupPJtPKqzoJs1fxQpQbJUpnMWiVqVcKtHWkqN3Sa8hXu22exgi9JzgRQiENO/vbCPOZ+L2Q7r5qVLCRNuhreE+hFBDm7Ef6DihHsfUyjXipIDQmK0ZmQAZK+IoIlyd4fQbKsRZk0JjskjN1gutBUL71l/oUxcKT/lmO70EhW1DC9/MEe2cOFH2GtBZpJXxauWbOLjEzBXNsl5pWqYYVWscS2Il8DyBUKaq1IlAE9g4QNtm156ZUSrbik0glIIWIRAqMOdeGtO9qkeIqEZcT4gqCf1tNX7rl3yu7e8kl2uz37Wp0ajivPHGG5uuEp0NyuUyAwMDdHZ2smPHjrRKa2lpSTfJa62p1+sUCgWKxeIkomwU8zSLKMfHxxkcHGT16tVTPjTMZkbp/l0+n6evr2/Ox/iThkVSnAUak2iWLl16XhHNdNCMIAA4Pym6i35DBt63KsN/OxxRSkjbkWbRrUijx4w5X1jRjG1TWguAb1WWaYya3TChEkk5X0ArTVfHEgJCPAEZL4MXGiL1hEbZ+VxcrVGtF9EKY43wQjKhCcYWDcRnbBqGDLFCm0bSM7ZKI2wx80SnZNXIRJLPFwmFYOmSXnzhTVSOjZYOGqpUNWG4NyoW8BLbNrUVs28fDHyBmVcyIQTS1oaiERCGBF4ILY7cFdVqjWqhZJSdvZKRV58hFgo/NjcsIQLb7vRsnJqLCgPwiRSgA5NFameBaDPzUxIQtpVqP6fWZhuGIdYQra3yWAIiAARRIonqGk94+L6Hkh6xjfeT1pKS2AAHaTenoAWBEnZWCIE0Wbp+Yr7OSzzQPplahnpZsDEHd/5yzOarTOtxZGSESqWC7/uTkmnclhpns9iwYUNKNpcTbpfpiRMn2LJlywW3RgghaGlpmUSUQEqUhUKBY8eOUavVyGQykz7/TIhSSpm2lK+//vpJ1eHFMB2iHB0d5a/+6q9497vfPe3XfaFAzDDKZ0Zf/JOEer0+ra9zIppiscjOnTtpb2+f0/ueOnWK8fFxNm3aNKfXGR0dpVarsX79eoBJQcbOJCyE4EhN8akDEWN1ZaPPJgQyOrHtSOfR80z70ksm0msmckQBqamUS0S1Gh2tHbRkcxNmeNtydOpLYYvhtB2pwdOapJ4gLVnKOEEoyIYZAj8kGwQEXmCN7464batTi4Zj0Wl7VUtNqVgiiWO6OjpMRWoTZJwalcTNDK3fzwpkzIBVpJ5I8290Or906TM2Fc5Uqtq0IoX1UGJbsYbaNChzwymVSkgl6WzroLoyYvg/niQOFVIrpLSGbmVSh+yEFzAKUK0VSI9EKASBrfCUNdUrtDL+xFhphLZh79Yob/TGHkliiFTZyWwiJXGsjT/QN0k4Stl9mxjbBrY9qq3P0UXiBdJDx+bch9K23u3MUEmBFwlqZViZgXf9Atx+29TXbBzHKVEUi0VKpRL1ep3W1lbWrFnDkiVLyOVylzVRpVgsMjAwQG9vL+vXr29qAk0jURaLRarVakqUrqpsbW193ud31eGqVatYs2ZNU8+P1pr/9b/+Fw8++CCf+MQnuOOOO15IiTbT+iCLlaLFxTZl1Go19uzZQ61WY/Pmzezdu7cpBtlmzhTd6zgjsCPDxh/kvhaPe6/K8IcjEadr5qaNaztiZwxMEIZvSUAn1l+HIaOoVqFcqtCWydLRvQxPe+nC4FT9qZg0j3OE6HJH0YJMkEEEIaLFEkqiiaIIFSVUKmVkIvERpu0aBASeiW9zs0XXbpVaE9dqVMoVcpkcXUs6J1WUTojTCJHotC3s/INaNbZXLREI0nzSxhxV7drNaQU6MTj1tTm3lWqVarlEe1sH2UwLY31Vhn/2NNIXaOWDNknonvAQ9mRLpWxGqETKGKUFngjwhY/SErwA7apIFVi/orkWE+mUpAK0j1QSszLKbKlItCKuSxIpCDwPfA8ldSrCMXmm7mHBkL1vU2cCjFAmUIYwfes3RNpAg5pAVaEzhF/7WXjjay58zYZhSG9vL93d3Rw6dIhqtcq1116LEIJCocDevXupVCqEYTipopqKKJoNpRT79+/nzJkzbN26dcr1bXNFNptl2bJlk0IH6vV6KuY5fvx4SpQdHR20t7czNjZGpVLhuuuum/b+1+nizJkzfOADH8D3ff71X/+V3t7epr7+TwoWSfEiSJKE/fv3c+rUqTQ1QwjRVCtFM2eKUsp0RnC+tTZLMx4f35jhU3vrHCybP9PKzvXkRHXnpZWPMJsoNMh6nVKxRIBHz5JuhA6MitS1IG0H18z0LFnY13MCnlRxaiutNCDAVkotYRbht1jFqyGbej0yHsKoblJpdEAQBmR8Hw+PaqlC4Pn0dvaYNJcGlauJcTMtwcbVU676EcKEEOCqWWEN5k4tqhs/i0hbr8Zn6Nk2NKnS1ROaqB5TKhbJZDL0di3F8zxGNxTZ96IxYs9Ucdqeb23NlkqZKk8TolD4eChPEygPpSW1RKGlINGRdSx6KK0I/MA8CCkB2qhVVaJMipFtk0qpUYmiHpvkHz8wBCglSG1Sj0z1b7sH0n5uKQgS02IXkfE+BtKIhrzYqkljSCrQHcJrXgJve+P0r9tCocDg4ODzbBaNN+QoitJq6sSJE1QqlUmtx46OjqYSZT6fZ3BwkOXLl7N9+/ZLukcxm82SzWYn2TuiKGJ0dJQ9e/aksWu7d++eVFHOpaLWWvPoo49y//3389GPfpQ3velNL6TqcMZYJEWLcytFrTVHjhzh0KFDU4pomrVTsRmWDLe25tSpUwRBQFdXF52dnResZHOBx+/1t/DQvojnxg0RSFvFaVs1OP+dmZtJimdLKJmwpL0D38+mQeDYysmluPi2zenyPJ2Yx1kdnpfywoQZH6UnRb25uLnWMIsXZBGtGEuJTIiqkUnIqSUEoY+nBbVyhSAwYQPm9q5tULeZhWo3e3RtXiu8wYp3nBUEMFWQh7FVpO1a3ITTkAZ6IjvV05AoCuUiMk5Y0tFJ4GfQaA5cO87+68tGVZv4ae6sFnaXIBpPGXWoBrAh2QKfWCvQJmUGHzIuYUaZ4INSTbsLAenav56HM/THShHXLJEFhjTjxG7AsGToaVv5aXN+hW2l+4lTJpsKUSXmgUEoATWoV6ArhDtugXf88vSvWaeYnI7NIpPJsHTp0ucRhWs9jo6OTmo9zlbM4uZ0hUKBa665Zt6tH9OBCyvI5/Ps2LEjrQ6jKEorypMnT6YVdaOYZzpEWSgU+MhHPsLY2Bj//M//nIYa/DRjcaZoEcdxWmG5JJqenh6uuuqqKRMqhoeH6e7untT6mA3q9To/+tGP2L59+4z/7blzQ7c9wP3SWtPe3p6SZHt7+5RPvX9zsM5jJ5iY29kIOBSIRFCtlKkVy3S1tdOSbbPiFGdXsNMvNdEedW1Lpwj1XPWYzhKZUHgmZoImNBOzPmcDceRsiQ076/ME1MoVKuUy7bl2WjM50/qLE2OPqEfEcYKS2ngnvYBsJkMYZNJqGA9ILGmak2lUpS4MANIFwalPUgs8oaywxs4ZlYmaE1JTrVcplyp05NrItrTadqzmuRePcXx9FW2FTVKbxc6J0qYaVBrtYuKsId5qaEyUnF3YbCLctP2+mzVQOlYgfFOIS0Viv87YJJRJt1ESLUJC3wY4KGE3n4h076VLm0GDJz2QEFgyRIKQpjIUGqhDrQxXhPBzPwO//OaZXbeNNovVq1c3rSppnNEVCoUZ2SPGxsYYGhqalzndbOEq1hUrVrB27dqLHlMjURaLxeeJmTo6Omhra0sLgO985zt85CMf4bd/+7f5lV/5lUtaEV8mTOubukiKFnEcp4kZYRiyefPmCyq6RkZGaG1tTZP6ZwspJU899RS33nrrjP7duXPDqX5glFLpD0k+n6dUKk36Ienq6krbTt85GfP/HdAkMbZiEiTVGqVCkWyYpT3XZW6cYIgA8OKJGaSwUW2+8wc6q4StONPMVPd7eyUJJQxpuooxtWLY17CrqlwVKuOYwlieTJilPddGgI9W2lo0dKqgtbHZyCghsoHgMorR2iw2DvyATMYoRIWdpzqDfVotCVtBWoIyX2PbsPZrtfKQMqY0nscPAtrbOsx8UEMSKp556VnOLo+wI0awDxvaej6lVfsiQXkCJSUCn0RqhPBsdq0x7msp0Nasr7FLpbXR5iZS2XYqCO2RSE21ZsK7fREY1aiSIDUBJt/UFx6++yyxN0GOWuAnhlx9WzHqGHRk2qS9LfCGn4PX3jGjS3aSzaK/v79pi3AvhEbD/VREmcvlOHz4MNVqla1bt85IxTlfUEoxMjLC2NgY27Ztm1PF6sRM7hx87Wtf45vf/Cbt7e0UCgU+85nP8LM/+7NNiYX7CcAiKc4Ee/bs4fjx4/T3919Qcu1w6NAhhBD+nmHXAAAgAElEQVSsWbNmTu+rtebxxx/nxS9+8bS/vjHW6XyEeD7EcUyxWCSfz1MoFKhUKulNYjxYwleOdzBeU5TG84TCozPXie+HpopQrtKzlZyc8DK6Ks4HK8qwM0PVkGnq5ouO8BrsFWlrUoHGSP1TkY4CFStKxSJaSro6OvG9wLZcjWrUVHH2tdEgPXts5qFBSZego4hio3RN6jFJHCMQhEFo4uu8DIHwEb4V8TCZpM03wRyr0mYxclyLWdLVQeDbdrUWFLrqPP3SPNU2t6UCtM2bhQZFJ8JuKDEuEE97KKt0TXNKlW92FAJu64S0W02U1iAxrVZtyDOumUAF3w8JsA8aluyExG7+1ehEomNp90n6ZBIPIQIj6ontBpQEVA1kFda2w//zRnjpy6Z9uaXX7EKxWTT6CE+cOJGOHBo7KhfLOp1PFAoFBgYGWL58OevWrWt6xbpr1y4+9KEP8ZKXvITVq1fz/e9/n6GhIR544AF+/ud/vqnvtQCxqD6dCfr6+mbUNgmCYNo2jgthuu83VzJ0CMOQnp4eenp60j9zOY9q/Cz/sT7E340vA91OkM2SyMQQkLY5nknDPM59Bki3PXjOP2hJSNsKL9BOvYltiU6QrGvZ0mC1MB8aY/uoVKiVq3S2t5PNtKStVdfidTmrJK5itCIZTMWH0PjCCWE8WvwseCEimzMLc5VGxjFJPaIcF1FJAnhkgwxhYJYWB56PTqtRqNbqVEolWrOtdPV0AyYE3FNwZH2NH9yURwYgE1OBKYHZHWhFOSjTqjKGeAHS+A1jqVDCJMCYfFIfibFkgEkeVXb/oEIgE0OkUgOJIIoSQi+gVWQRrupXJplI2/mgZ32qngpMYhECUZcgE2QUoeI6MlZGaVpP2LzM4x13tXPt1TOvJiqVCoODg7S2ts45LLsZEEKk83cpJS95yUvIZDIpUU6VdTqX7RnTRWN1OB/zzHq9zgMPPMCuXbv467/+azZv3jzp72dYHL2gsUiKFplMZkYq0DAMKZVK83hEBufzGzYT2WyWJEk4c+YM2zas45WrVvE3Q1W+fSihXqtRLpQQShN6oamovAzZIJsqMV22qBOg+Pa/NFaHDUQnbMUiXFXnPH12869vK6koqlPOF8hmWunt7sUTbtGvsCIebXc4ugxUz6pMTVsTrW07tKHaS4OxPYRREBFiFhELL4tudRmskihKSKI6hXLVJOp44PkBsh7j+x69XT346RzGsP4zO4ocWFc3v41NBWdmi8Iemkh9jUoLk38gQXseZh29yTM1mee+2V6hTZZqokxrVOmG865MILmMJUho9TOEtuUplG1/CpHudfQT60tNBDqxDxQxeCoAGZCpQVSHFhTXrS/xC68tE2RM+23XLiYJOc43o4bJhveZbrOYT5w8eZJ9+/axfv16li9fnv4snWu4P9/2DLePsaOjg66urqYs4W2sDrdv3970n+9nn32W//Sf/hNvfOMb+Zd/+ZcpBXgLYYa6ULDYPrWQUs6IFMfGxhgdHWXbtm1zfu/vfe97U7ZPpzM3nCvGx8cZHh6mq6vreaKi3Wckfz0IpbohsaReR0YRSZwgaxECjxa7WiobZAhEJt2d6DeIbpwZ3tkhUC4tx8ywXBC3a6GqWFHK5xFa0NHRQUCQzvxQJvFmInnGeCKdvQMnHrGzQZOPastT7aUzSrt6wlpDRGrSN2pV89rOriG02W5RLhSp1eq0hKH5viiJ5wVk/AxRt8/Tt1UZ60xSpSqY3YIaQ+Ruq4dURjikPQ8VK7RnlkAL7aHxkEqZTFltCF9bcYxSpmUtBHjKiGF0DCqOyXgZQr9BKSqxwilBoEzlbkIZnK/Qfl8ijBUlgqgCy9vgZ18Gb3gLnHvvlFJSKpXS1nupVMLzvEnVVFtbG8VikcHBwXkxvM8W9XqdoaEhhBD09/fPiswaidL9OndxsdueMR04L+TZs2fZunXrnINAzkUcx/z3//7fefTRR/nTP/1Trrvuuqa+/k8gFtunM0Gz10fN9L2VUunNo1mt0guhVquxd+9e4jg+ryR+W6/Pf9mp+PKPYfAMZMIsItNiEm46BEolyGpMVK8TVYskkSSwW+1bMxlCPwPattwccbmPIbAePUOIzjhfLpSo1+p0tLWTDVsmhDZgBD7a2iOYmBfaYdtEqDlGdelhqmzzlhNVJLZliG2tBlYVin0PG3Fq808V9XqNSrFES0sbS5d2mh8aZav4JGH36irPXlci8jS6bL2KPoZ0vQDtniXtpgtjNzFiGKQR0KB9tBQoqyw1Jn7w8az3U5CxM1jzbyCpJ/hK0Oq34CsjjhF2fqglE8RoI/M8aY5bxeBL8CKIquYmsHU1vOG9cPUF7pu+79PV1TVp5p4kSSriGBkZ4ezZs2itWbZsGblcjmq1ellTabTWHD9+nAMHDqQ+49nifNsz3OLis2fPpouLc7ncJNXnuUTpknKWLVvGzTff3PQHh8HBQe6++25e8YpX8J3vfKcpFe1PCxZJcZZoJim61wrDcN7JUEqZtrU2bNhw0ZtEa+jxmzfA9w4rvr4Hqgloz9xYwWyMyAateDkniDEtxziKqNbLqEQTej5hYBJpsn6GQJgqx6XmaG12HJYKeVoyrfT29KYeSFc9NprxTVCAi18jTZnRrh2rvAl1q3btVp3+v4cJ7XYzRw3WjmFnnELjaxOSXSqUQCmWdPea49agMWQch5pv31rhZG9MrAN8KZBCI4RNlpFGFWpWYHlm6a8AXACAMpFrUprPoqyX0lTUHtm0ehaputdTAhVLRCRpD1qM6ElaFa9T+EpBaP2HOjafLZCmWtQxeDWQdejOwc+8FF7/Dpit6DIIArq7u1FKMTo6yoYNG1i+fHlaUTZ66JziuZmB2BdCrVZjYGCAbDY7b/PM8y0urlQqFItFzpw5w/79+4njmFwuR0dHB+VymUqlwtVXX9306lBKyR//8R/zla98hS984Qvs3Lmzqa//04BFUpwlgiBoinkfzBN4HMcmiWWeWqVaa06ePMn+/ftZsWIFO3funNHT6YvXeFy3XPM3P4LhEyB8MwvzbAHmCCsIAnw/wM+AyGGW1EpJUo3MaqWkiEjA9wJawiy+51MrV/C0oKu7h0CHqUI1bZlqk+np+dq2RpmYqdl0AA8BngnlNhWknjDkA9ht99oaAIU15bvYOgT4bn2U1hRLZeqVKp1tXWRbsjbWTVsxkWBodZ1n+qvUfQmx8RpKBMSG+FwMm2fXPLnFu1JpkAqltVG5YhbwBna/pdfQUvbd2iy7tV4lIOs1fOGT8XIEkWsbN/gNpbHFKAmB/a+IgTrEVWgPYdtGeM0vQ3//3K4pMDaL4eFhpJTccMMNqWrzXDHXVGb7RmuEE7I047rXWnP06FGOHDnC5s2bJx3HpYAQgra2Ntra2iYR5alTpxgeHiaTyeB5Hj/60Y9oa2ubVFHOhbgPHjzIXXfdxQ033MB3v/vdBWEv+UnE4kzRQmuTuTmTr5+JleJCr/PjH/+Yer1OT08PXV1ddHR0NLWdUiwWGR4eprW1lQ0bNsxZRffsqOaR3ZAvi3Q2KJRb/8SEiEbbG7P7e/tnWmpkFFMplahXY0LPw1OeVXpmyPoZfGGUkWmCjDLVnss7NfsSzVopsxcRsFmmjkzNyivTMhU24Nos+rWzR2tY10KZSkwI4nqd4liBTC5LZ64DrHpTmPgaxrMJ3726zonO2ChJFamAxhnusUTo5nh2mQhoM+sTbj6oBTrREJm+prFO+AR+QGCDvT0FvhREcR1fajJ+qzkvNnNUaKvyVeAl9r8SE8AegaxAIGDDcrj9tfCi2+f0rU/RDJtFo4cwn88/T/E5GyFLpVJhYGCA9vZ2Nm7cuCD8d0opDhw4wOnTpyflqE4VuJEkySSivFgylXv9L3/5yzz88MN89rOf5WUvm6Fn5qcHiz7FmWCmpAjnF8hM9/1cq1QpRblcnpSYfz6T/UwQRREjIyOUSiU2b97c1IWhiYJ/GoSnDkGtZiwXuBuyAN+tVUpIU2zMyilNvVqnXCySy+boaGkzrUsJMolIKokJBE8UgYDQy9ASBvhBlsA2VSc2VFhjYuIbotMCT2nchvs0mNyKfrRuCCNvXF4MEEsKxSJITWenEffg6ZSUpdY8vqHG8IqIuiVImQqE7AJhO7JUkKbD+LY96ysXsC7SGDlPWRVtIgzRKdCJRiqJiBP7gGE+U+j7hLQYBandO+nZhw+tgdiEKZCYijAD9HXDi18Or3wjNHOkVK1WGRgYoLW1lY0bNzatLXk+IUvjfK6zs3PK99Nac+jQIUZHR9myZcuCUbuWSiV2797N0qVLufLKKy/6sKu1ft69QEr5vIrSEeXo6Ch3330369at48EHH5yX4PIXEBZJcaaYqe9wNqQ4XRFNkiTpE3Q+n0/bTU7ocCGVm1t+fPToUa688kpWrFgxb/ObsSp8/ccwcBQjIElN8oYg000VgKpFlApFAhHQmWvHF3aPYENl6akJ0tNKEdfqyEgSRTE6kQTCziftHsYg8NJ/41ZLufBu54MMhF2R5OLmNOBsClJRKVeoVat0tHeQC1tQDfNKjea55XV+0BdRClQ6lxQaEu2M8Q2zS+2yRG116+aUTiRjbRISU0X7ljyFbZl60ngKkQpZj+xreqhEEyhpqmENSJ9ACIh9kAG6Cllg1TJDhP/hF6HZ/nNns3AhF93d3c19gynQKGRprKYaidLzPPbs2UN3dzfr169fMNXhwYMHOXXq1Jy3bCilJlWUg4OD3HvvvaxYsYJ9+/bxvve9j9/8zd9cJMSLY5EUZ4rZkOKLXvSiaRHOXP2GLonDyeHz+XzaamlM4hgfH2fv3r3pk+mlukGM5uEbz8HBk6Zt6DvRRwLohOJ4kaQe0d3ZSei3ppYNF//mg6l2rCrV7TAMrL/PhVVLmRDXIlQcE9VjtIQMAWEmpDUM8fzQCGLc+2uTnUoqSJlQs8a1iGK+QCZsob29zZCRNpYHpGJgWcwzq2IKGYmUE8IfFzzguZxUS3hKQmi0r+bPpY1MsySvtEjPi6caPJey4YFAQRLVIJbGC6qCdOOIF5u5p6xHJHVJpRrjU2VVZ5kbX1rjFb/QkV4Hzf6+u5vxQrBZuGoqn89z9OhRSqVS+sDYWE1dLnIslUoMDAzQ09MzL+fq1KlTvO997yMIAm677TaGhob4wQ9+wE033cTnPve5pr7XCwyLpDhTRFE0o2SHJ554gh07dlz0h2++/IZaa0qlEoVCgTNnznDmzBmEECxdujSdT15qOfzoOHxzN+w/ZlqOtUKJSrlEe0s7uWy7CZnWVkQTm6vUrZ5Kl/sq0mrLCG5E2pp1YhQXI6clqFgS1SKSKDKLitFkgiwZP0Pg+4RBaOeddu4nFaViAR1LOjs6CYMw3cqB0Ozujnl2WUw+Y4Q1gVO2anCbNNLgdHucUgpTuUk3O3XK2MZ9ixPE7NvPRCysJQRUEiErEZkgJCREJ4bc/cS0oZO6OQe9Obj6WnjpL8GmzUxqv7uMW611arTv6uqira1tVjdntzkin8/Pi5dutnAkvWzZMtatWwfwvLajUmpS2ECzZ/XnwlWHJ0+eZNu2bU2v3LTWfP3rX+fjH/849957L7/0S7806Wfb3V9mi1/7tV/jH//xH7niiit47rnnALjvvvt4+OGHU5X6Aw88wKtf/WoA/vAP/5AvfvGL+L7PQw89xM/93M/N4dNdEiyS4kwxU1J85plnuPrqq8+bk3gp/IZu3+PZs2fZtGkTXV1d6c2xUChQLpfJZDKT2q7zGVflcOREka995wTHiktoaekiEKERntjq0Xn9RDIx9/PVROWok4n9jgLTZlRWkSpcGEBjIEDDaie0RtZi6lEdWUuQUiIQZIIQpCKK6nS2ddGSbbVrsaDuK37UE7OvU1IK7NosTVrNBQIbSmBELp6YaJEKV9ky8fl8N7d0FaVTy7rjtp5BlCEeWY8QUtPmBZBkENLYJnQCXS2wbg3c/DJ48c9Nbz44ldHe9/00iWU6q4XOnDnDnj17WL16NX19fQsi9URKycjICOPj4xcl6cZAfHcOhBC0t7fP+WHhXJTLZXbv3j1v1eH4+Dgf/vCHKZfLfOELX0h9ks3Ed77zHdrb23nnO985iRTb29v54Ac/OOlrd+/ezVvf+laefPJJjh07xitf+UqGh4cXROv6Alg0788U5+5UvBjO51W8FGTYqP5bs2YNO3fuTN+ju7t70rzHtV3z+XwaV+UG9+4G2ayLOY5j9u3bR6lU4td/vp9cWwfP7IMfHoDRM8bf6AUQW0LzfZCNHkSrnQlsVpwQhhiUMDYKy3mGzDxDVMIjnSc6gvSzIW1hiOgwJBXV65QLBbSGwPcpVgqUKxXOdofsXxZyqh0S39gfwsgmv1iRjS9N6zO0s1IXhO5hW6bapfIY07xQDVmv9v9dK1lhZqh+AkJp4qiOriVktUAlbWht/IN9q+H6nfCS18xuNng+o70jiH379k251b6lpYU4jhkeHiZJkkk2i8uN8fFxBgcHWbly5bTi0DzPe945kFKmRHnw4MHzpvLMJJP44MGDnDhxgq1btzZVzOZe/1//9V/5z//5P/PBD36Qt7/97fNW7b70pS/lwIED0/raf/iHf+Atb3kL2WyW9evXs3HjRp588kle9KIXzcuxXUoskuIccC4pXoqcUjB71oaHh+ns7JyWKTmbzXLFFVdMynWsVCrk83mOHz/Onj170nabu4nM5MbgXvPYsWMcOnSIK6+8kv7+/vTf79xkfuUr8PQI7D0Mp8dBxiYXW0TgZ5moJK2C07UsvcD8ndZABH7Q0LIQhpg8oc3CDAS+UOnORJ1oxosFVJywpGcJgeczmlEczMBoi6aCRCYxoqRx+w4D4ePrgNAzy4A9OSGkCWxr1Mw5SeeWwkanCczx+K79a4VHKrGVcGT+PqpXqJerdLQoNi/vZs3GgBtfBjfeMqNLYUYIguC8/sF8Ps+xY8colUrEcczSpUtZvXr1gohok1KyZ88eyuUy1113XbpodzbwfZ8lS5ZMUqc2PiyMjIxQLpcJgmASUU5VVbvqsLu7mx07djT9XJVKJT72sY9x4MABvv71r895I89s8fnPf56//Mu/ZPv27Xz605+mu7ubo0ePTlp319fXx9GjRy/L8TUbi6TYgLlEvZ07N5yPm0m9XmfPnj1EUTSn+U6juXjVqlXA5Cfo/fv3pzcGV0l2dXWdt2JwJN3V1cWOHTvO66vqysErrjG/SjUYOAQjo3DipCFMaduVwrgh8L0JFanGVGaegESaClM70Uxgo0wVCE8jhanC6tUKpWIF3d7OeHsHPw7hrKdJPJvPqgQtygMdYjZ7CISSqCQGmZDIurV5gBA+ofARBGYriKsAnUjGVn9gSE9ZewQSoghyIbT50NMtaW07xLL1p3ntm7bQ23t5FYNuq31bWxv5fJ6enh7Wrl1LpVKZFISdy+UmCVku5p1rFlwLt6+vb9KDVjMx1cOC20PYuNk+k8mks8lSqZT6Dqezam6m+N73vsfv/M7v8Bu/8Rt84QtfuGwPJ7/1W7/Fxz72MYQQfOxjH+MDH/gAf/7nf35ZjuVSYZEU5wCXaiOlnNdWqRvgu2i2pUuXNv09pnqCbqwijh49Sr1eT6XwjiQPHDhArVabMUm3t8COzeYXQLkK+0ZNi/X0GRgvQrUE5QpkhCVIjIg0NNGg6MhljBoSjRSUgHy9zsl6hVomIFnTTYxvxDAKwsS0NDMYCaqzg3iY/9c6ABVMzC6lRMsYmSi0lChZR0iFwLczRh8lsyRVQSYDOR96stDeA1eug5Ub4PoXQ2+vUQ26jfN9fc3fhjAbnM9m0dHRMSnf01kCTp06xb59+5BSprO5+VB7xnHMnj17qNfrl6WFG4Yhvb299Pb2pn9Wr9c5deoUe/bsSVdQ7du373nt57mgVqvxB3/wB3z/+9/na1/7Ghs3bpzrR5kTGmeX7373u7njDrNZevXq1Rw+fDj9uyNHjrB69epLfnzzgUVSnCW01gRBwLFjx9Bas2TJkqYLWFw01MjIyKyi2eYKV0UsXbo0PZ5qtcr4+Dj79u2jUCiQzWZZsmQJ4+PjKKUuuE7oQmhrheuuMr8aUavBkdNweBROnobTRRgfhzNjUIyhKqEcQVlBrBOSpABANsjiJ22EeZs4gzG0J9JUeEKRWidEgz3EdcN1DGgI8Gn1fQIfQqDDh1xbTBgWaGmpkG0r0NVdYuO1IStWTFTVjR7SWq3Gv//7EJ7ncdNNN10SodN00GizuNC11dhZWLlyJTCheHVt12KxCNAUxat7eJhvj+1MoLXmxIkTHDt2jOuuu46urq5JC4sLhQJHjhyhXq/PemvGD37wA+655x7e8pa38OCDDy4I0cro6Gj6Pf+7v/s7rrnmGgBe+9rX8ra3vY33v//9HDt2jD179rxgclYX1acNmM76qMaZYZIkjI2NpdWUE7C4udxcnp5LpRLDw8Nks1k2bty4YG6kY2NjDA8Pp341IcSkqC6ncnQ3RldRzueNzeW6joyM0N61hrb21Zw+KyiXoB5DrQq1ug39tlFoWkMQQhgY9WuuzYRit7VCdw90dMJ0IzMbb45O7emuBaVUmih0OTfON2K+bBaNLfhGxevFZnMOURQxNDSE1potW7YsmM0OlUqF3bt3p+vVLvQzPZtUnjiOefDBB/mXf/kXHn74Ya6++upL8bGeh7e+9a089thjnD59muXLl3P//ffz2GOP8cMf/hAhBFdeeSV/8id/kpLkJz7xCf78z/+cIAj4zGc+w6te9arLctwzwKIlY6a4GClezG/YaCrO5/MUi0WEEJMI4mJxbU69WSwW2bx587zMK2aDWq3Gnj17SJKE/v7+C4od3DzGnYdarUZLS8skW0izosHK5TJDQ0O0tLSwcePGBXMjHR8fZ2BggJaWFrLZbOodbG9vT8/BbKvqueBS2yziOKZYLKYPC42zucaq2oXVzzZHdT6gtebw4cMcO3ZsTtFxU2Wcjo6O8hd/8Rds2LCBxx57jNe//vXcf//987LJYxEpFklxplBKTbn5Yi4Wi8a4NndTcKHHjiTCMERrzZEjRzhy5MiCahs1zpyms2pqKjQ+PTuibJxJzSYEXUqZ+jM3b968YLIukyRh7969lEoltmzZMqkKa/TNnVtVzyXjdjpw2yySJGHLli2X1WYRRVH68+A6LUEQsHLlSrq7u2fUcpwvuGDxjo4ONmzY0PRWZr1e54EHHuDf/u3f6OvrS1uvd9xxB/fff39T32sRKRZJcaY4lxTnw2/YGNfmftXrdZIkobOzk/Xr19PV1bUgpPCuqli+fDnr1q1r6jE1zqRcVe38Yhczlzuxx6pVq+jr61sQ56qxhbt27VpWrVo1rWtlqkrKrVRqRthC45Ldq666iiuuuGJBPGw1Wng2bdpEW1vb81qOM90W0azjcrnB8xUsPjIywl133cUtt9zC7//+76cPKHEcc/z48TlZL6ZKpTl79ixvfvObOXDgAFdeeSVf/epX6e7uRmvNPffcwze+8Q1yuRxf+tKXuOmmm5ryGRcoFklxpnCk2Dg3nE+/YbVaZXh4GK01q1atSsnSVRCN7cZLsZT13OMSQrB58+ZLVlVMVVU3Zlpms1lGRkbwfZ/NmzcvmDlrtVplcHCQTCbDpk2b5lzlNM6k3Kx6NpYIt82ipaWFTZs2LZjWnDuuXC7Hxo0bp/wsjV7axm0Rjd2F9vb2plZw1WqV3bt3z9vaKaUUX/ziF/nSl77EQw89xG233dbU14epU2k+9KEP0dPTw0c+8hE++clPMjY2xqc+9Sm+8Y1v8LnPfY5vfOMb7Nq1i3vuuYddu3Y1/ZgWEBZJcaZwVdx85JQ2wrX+zpw5w8aNGyfJvh3cXG58fJxCoUCtVqO1tXUSUTb7yVlKme5927Rp0yVfzjoVarUa4+PjHD58mGKxmM6jmiFmmisaW8vzucy2cSblSKIx19MRhKuYlVIcPnyY0dHRS7bNYjponNHN5ricaKkx39TFtrmfidkoXhurw/k6X0ePHuWuu+5i48aNPPjgg7S1tTX9PRwOHDjAHXfckZJif38/jz32GCtXrmR0dJSXv/zlDA0Nceedd/Lyl7+ct771rc/7uhcoFmPeZoqvfvWr/P3f/z3bt29n586dXHvttU2dbTS2svr6+i6YgnGuT8rZIfL5PCdPnmTv3r1zTqFpPC5n/Vi5cuW8pHPMFuVymQMHDrBixQpuvvlmhBBTWgEa242zPQ8zwfj4OENDQyxbtmzerTLns0S4bFP3wOB5Hq2treTzeXp7e9m+ffslM9lfDOVymYGBgTTgYTYPMo1xbA6NitcDBw5QLpdnpHh1VWtbW9usj+tCUErxla98hYceeog/+qM/4vbbb7/k7esTJ06k182KFSs4ceIEYIi6sVXrUmlewKQ4LSyMn5gFgje84Q1s2LCBJ554gj/90z/lueeeo62tje3bt7Njxw527tw57VnRuSgUCgwNDdHR0cHNN988Y7IVQpDL5cjlculF23hDGBkZmZRl6YjyYi3GcrnM8PAwYRhy4403LpiWZK1WS1vL119/Pa2trenftbe3097enpqFpZTPi+k6t6Js1udypvJarca11147p8ixueBcgpBSsnfvXs6ePcuyZcuo1Wo89dRTaRh8s8zlM0Xj5ogtW7Y0XU09VejEhdJoGlvxR48e5ciRI/NWHZ48eZJ77rmHJUuW8O1vf3tBVOzz1f16IWGRFBsQhiHbt29n+/bt3H333WitOXv2LLt27eLxxx/ny1/+MqOjo2zcuJEdO3awY8cObrzxxgsqBuv1Onv37p1V6svF0HhDWLt2LTCh7HMVxPm8k267xtjY2IJSbza2/jZu3JgGB1wIvu9PGYLu2o1O2efmcu48zKSKaqzyF5I6GCbbLDZv3jzpuM53Hi62yb4ZKBaLDAwM0Nvbe0aFJGIAAB/9SURBVEm7D+dLo3FEeejQIQqFAmEYsnLlSqSURFHUtK6Q1ppHHnmEBx54gI9//OO87nWvu6zXyvLly1MT/ujoaGp5eSGn0swFizPFGUJKydDQEI8//jhPPvkk3//+9wG46aab0rbrhg0biKKIRx55hLVr13LVVVexbNmyy/KDMZV3Mo5jkiRh2bJlXHnllZd85+L54IIB5mNB8rnCjUKh8Lydg+3t7VOeh0qlwuDg4IITrMzGZtHYhnfnYa72mHOhlGJkZISxsbEFtYPRKV4PHz7Mpk2byOVyk+a0cRzPWfE6NjbGBz/4QZIk4X/8j/8xKwvTXHHuTPF3fud36O3tTYU2Z8+e5b/+1//K17/+dT7/+c+nQpv3vve9PPnkk5f8eC8hFoU2lwJu0e/TTz/N448/zq5du/j3f/93oijilltu4V3vehc7duygq6vrshNPqVRKje7Lli1LSaJR5dnonbxUiKIoDTq/WDBAM+F8g+6BoXE7giOH0dFRTp8+TX9//4Kppptts2i0xzQKWBp3L053TpvP5xkcHGT58uWsXbt2wcyma7Uau3fvvqjitXFR8bmCpgstKtZa861vfYuPfexjfPjDH+Ztb3vbZfl5nyqV5vWvfz1vetObOHToEOvWreOrX/0qPT09aK25++67efTRR8nlcvzFX/wF27dvv+THfAmxSIqXGidPnuRXf/VX6enp4T3veQ8HDhzgiSee4KmnnqJarXLttdembddt27ZdMhFEHMeMjIxQKBTo7++fcudbrVab5J101YMjyflIX2kMLHDBAJf7wSGO43Sl1smTJ1PRxpIlS1KyvJziFWf/yGaz81q1Ns5p3QNDGIaTtqZks9n0++Wi4wqFAlu3bp1XdeVM0OiH7O/vn7FCuFHx6uxS7oFhfHwc3/fZtGkT9957L8eOHePhhx9ebEEuXCyS4qVGFEX88Ic/nDIYt16v84Mf/IAnnniCJ554goGBAZYsWZKS5I4dO1i+fHlTSaFxEfG6detYuXLltF/fVQ/OElIsFpuaaZrP5xkaGqK7u/uieZKXEq4lGcdx2pKsVquT2mxSyvPaIeYLWmsOHTrE6OjovNo/LoTGJJpGm1AYhoyNjdHX18e6desu+4ONQ61Wm+TTbNbDjBO4fetb3+KLX/xi+rN8xx13sHPnTl760pemK9kWsaCwSIoLGS4B5Yknnkjnk6dPn2bz5s2p0vX666+ftVrQqV07Ozu56qqrmlJRTJVpOlPvZBzH7N27l0qlQn9//4KbNx06dOiiLclGO8RUDwydnZ1NjWsrFosMDg7S3d3N+vXrF8wDRBzHDA4OUiqV6OrqolwuI6VMhV3zsVJqOmh8GNy8efOUPuC5olqtcv/99/Pcc8/xxS9+kSVLlvDMM8/w1FNPcd111/ELv/ALTXuvK6+8Mj2PQRDw9NNPnzelZhEXxCIp/qQhSRJ2797N448/zhNPPMGzzz5LGIbcfPPNaTV5sbi1KIrYu3cv1Wp13kmnUbRxrqm8se0qhJh0o1po6s1SqcTg4GCaZDKbiqLRBpDP56lWq1Nm3M4EUkpGRkYYHx9fUIIVgNOnT7Nnz57ndSBch8Gdh0YfaeNKqfn63rvqMJvNsnnz5nlpdT/99NO8733v4+1vfzvvfe975530r7zySp5++ulJSuzzpdQs4oJYJMWfdGityefzPPXUU6mIx5GKqyZvuukm2tvbkVLy7W9/m1wud1kzLs8Vr5RKJTzPI4oi2tvb2bRp04K5uTeGim/ZsmXKWetscW7GbaFQIEmS59ljzveAc/bsWYaHh1m1ahVr1qxZMA8QcRwzNDSElJItW7ZMy//p2o3uPJwraGpGjGGj+Gi+qsMoivjkJz/Jd7/7XR5++GG2bt3a9PeYClOR4vlSahZxQSyS4gsRSin27duXVpPf//73GRsbo16vc8stt/CBD3yArVu3LogWW5Ik7Nu3j/HxcVauXJnOpJq5d3K2OH36NHv37r2kpHM+lWdjFRWGIXv27Jk001woOHHiBCMjI1x11VWTNrLPBlEUTaqs3XqxRqKcrm+wXq8zMDBAGIZs3rx5XsRHP/7xj7n77rt5zWtew4c//OFLqs5ev3493d3dCCG48847+Y3f+I10sTeYB4Lu7u7094s4LxZJ8YWO06dP8/73v59Tp07xjne8g6NHj7Jr1y6Gh4e54oorUt/k9u3b6e3tvWTVhttSvn///ik3RjR6Bh1BAJNajfPlnazX6+ki2/7+/stOOkmSUCwWGR8f5+TJk5RKJXK5HMuWLUvPxeVeo1Sv1xkcHEyD2OfjeM5dL+Yq63MDFxofnhqrw02bNk0r6GGmSJKEhx56iEceeYQ/+ZM/4cYbb2z6e1wMR48eZfXq1Zw8eZLbb7+dz33uc7z2ta+dRILd3d2MjY1d8mP7CcMiKb7QMTY2xr/9279xxx13TPpzJxpx1eSuXbsoFAps27Ytbbtec80183Jzm+3SX2cBcER5rndyrjv2Gu0fGzduvCym6vPhXJuFUmrSA0NjZe2qyktRWTfOgS/HOWsMnnCVtQtcyOVynDlzhmw2S39//7xUbnv27OHuu+/mZ37mZ7jvvvsWRATifffdR3t7Ow8//PBi+3TmWCTFRUwgjmOeffbZdDb53HPPkcvl0mpyLrmuMHk+19/f35SMS+eddGSZJMmsklecenPJkiULyv7RuDXiQjaLRlO5E680Kwz+fJgvO8NckSQJhw4d4vDhw7S1tZEkSVOtQmCu5T/7sz/jf/7P/8nnP/95XvziFzfxE8wM5XI5Fa+Vy2Vuv/12fu/3fo//83/+z5QpNYu4IBZJcRHnx7m5rrt27ZpxrqvDyZMn2bdvH319ffT19c1bm3aqxcQXuiEmScLIyAj5fJ4tW7bQ0dExL8c1G8zVZtEoXnGVdWMY/GzDvxvXKF0uP+T5EEURAwMD+L4/qTqcSvnbuIdzJi3ow4cP8573vIdt27bxqU996rIFvjuMjIzwi7/4i4C5nt/2trfx0Y9+lDNnzkyZUrOIC2KRFBcxMyil0lzXXbt2nTfX1VVno6OjHD9+nDAM2bRp02VpL03lnWxpaSEIAsbHx1m7di1r165dMOrNRptFs4m60Vyfz+cnhX9Px0daqVQYGBiYtyW7c4ET+Uy3jXvufNK1oBu7DI3nQinFX/3VX/GFL3yBT3/607ziFa9YMNfMIpqGRVJcxNxwbq7rk08+yb59+1ixYgVBELBv3z7+9m//lg0bNiyYG4jbnu5i6kql0nm9k5cal9pmMdVyYq31pPg+13Y9ePAgJ06cmJf1TnNBFEUMDg7ied6cZodTBcIXi0X+7M/+jG3btvG9732P9evX89nPfnZBff5FNBWLpLiI5uPRRx/lgx/8INdffz09PT0888wzaa6rqyYvZa6rQ+PKqU2bNk3yqZ0v+LtRxDOfKtQ4jhkeHiaKIrZs2TJpN+SlRuO5aIxra2trY+3atSxZsmTOM7lmwVWHGzZsSNcdNRP1ep3Pfe5z/NM//RO5XI7x8XFyuRy33XYbDzzwQNPfbxGXHdO6qBfG9HyB42tf+xr33XcfAwMDPPnkk2mS/IEDB9i6dSv9/f0A3HrrrfzxH/8xAM888wzvete7qFarvPrVr+azn/3sgrjRzAVaa7773e/y6KOP0tfXl/55vV7nhz/8IY8//jif+cxnLkmuayNcjmpPT8+U29M9z0sJ0KGx1TjVvsVmKDwbrSnr16+f13MwXbhz0dHRwf79+6lUKtx4442p4vX48eNpC7rxXFzqrSmDg4MIIWa1kHs6OHPmDB/4wAfwfZ///b//d2rnyOfz7Nmzp+nv14hHH32Ue+65Byklv/7rv85HPvKReX2/RcwMi5XiNDAwMIDnedx555380R/90SRSbNxb1oidO3fy0EMPccstt/DqV7+a9773vbzqVa+61Id+WdCY6+osIc3MdXWI45h9+/ZRLpfZsmXLnDYznOuddArP2Xonnc0ik8nMm6F8tigUCgwMDHDFFVdMGRvoPIONtpDGrSkXWqE0VzjRVjMCAqaC1ppHH32U++67j49+9KO8+c1vvqQPKlJKNm/ezDe/+U36+vrYsWMHf/M3f8O2bdsu2TH8FGOxUmwWZhrnNDo6SqFQ4NZbbwXgne98J3//93//U0OKQgiWL1/O6173Ol73utcBk3Nd//Iv/3JSrqtru14s19XBke7IyAjr1q2jv79/zjc2IQRtbW20tbWlGw4avZOOfLPZ7CSiPLeKma7N4nLAiXzy+TzXXHPNeR8ihBC0trbS2trKihUrgMkrlI4cOUKxWMTzvEnnYi4h6C5cXGs9b9VhoVDgd3/3dzl9+jT//M//zMqVK5v+HhfDk08+ycaNG7nqqqsAeMtb3sI//MM/LJLiAsIiKc4R+/fv58Ybb6Szs5M/+IM/4LbbbuPo0aOT2ot9fX0cPXr0Mh7l5UcQBFx33XVcd9113Hnnnc/Ldf3bv/3b8+a6Nt5oGyuw+bp5Ovi+T3d396TtAy7PdHx8nIMHD07KMw3DkMOHD9Pd3T1lG/dyYmxsjKGhIVatWsXNN988Y/JyBNjZ2Zle20mSpA8NJ0+enHXgwqWoDv/v//2/fPjDH+aee+7hXe9612Vbfnz06FHWrFmT/r6vr49du3ZdlmNZxNRYJEWLV77ylRw/fvx5f/6JT3wirXbOxcqVKzl06BC9vb0888wzvP71r+fHP/7xfB/qCwJCCJYsWcLtt9/O7bffDkzkuj7xxBP84z/+I/fffz9RFHHDDTdw00038eyzz1IqlXjwwQcv25qcbDbLFVdckQo/tNYUCgX27dtHsVhMdwvu3bs3XU58OYUrSZKkq7quv/76pop8giCgp6dnUjXs2q5jY2McOHBg0kPDuaukGsPF5+sBp1KpcO+99zI0NMQjjzzCunXrmv4ei3hhYZEULb71rW/N+N9ks9nUm3fzzTezYcMGhoeHWb16NUeOHEm/7siRI4vbuKcBz/PYtGkTmzZt4h3veAdgbmpf+tKX+OQnP8nq1aupVP7/9u49pqnzjQP4twW8MgQxKmmZCNVSRQFpURZ1/MKMgrOZY9nAzBuLugqaiW7TOLW6TDDTzXiLoJLBEmVz6gjxggNFZ2IhMusUJxwtyAakwrRy0VSs/f2h56yFFrm0tMDzSUy0ZTsHoz7ve/o83/cJFi9ejLCwMISHh0Mmk/Vormtrjx494sYsQkNDwePxzHZQZWVl3DFSPd248u+//4JhGPj6+trkEXNHDBo0CIMGDeJ2fKZRbTU1NVwIupubGxobG/Hmm2/a7WDi4uJiJCcnY+nSpdi7d6/DdoemBAIB/v77b+7X9G+D86Gi2A11dXUYPnw4XFxcoNFowDAM/P39MXz4cHh4eEClUmHq1KnIysrCqlWrOvz/tdbtCgApKSk4cuQIXFxcsGfPHsyePRtA3+1o02g0yM3NxcWLFxEQEGCW68p2u/ZUrqsp0zGL1juw1jso08aV+vp6aDQaGAyGNjFttvpHm723lpYWhISEODT0nMfjwd3dHe7u7hAIBGhpacFff/0FvV4PgUCAhoYGqFQqDBgwwCyBpjtBEHq9Htu3b0dRURGOHTvGdYc7A5lMBoZhUFFRAYFAgOzsbBw9etTRt0VMUPdpB5w6dQqrVq1CXV0dPD09ERISgry8PJw4cQKbN2+Gm5sb+Hw+tm7dinnz5gF4eRApO5IRHR2NvXv3dng1bK3b9fbt24iPj0dxcTFqamrwzjvvoLy8HAD6dEeb0Whs9/eOzXVlu11tneva+l5sMWbBzguyO8qmpiazMwbZyLrOYj+fc5YREFPswcSW7k2v15slE7VOoOnoiMyff/6JVatW4f3338fnn3/uNJmtps6cOYPPPvsMBoMBCQkJ2Lhxo6Nvqb+g4f3eLjIy0qwopqSkAAA2bNgAAJg9ezaUSiWAl+n5eXl5Fr+uv2md68ouIkQiEaRSKWQyGaZMmdLpbkk2JNteYxbsGYNsYejM7KTpbJ9YLHb4cVOmTHeuEomkQ7tASwk0piHoHh4eZk1YLS0t2L17N86ePYv09HRMnjzZ3t8W6X1oJKOvqa6u5sY8APOuVupo+w+Px4O3tzdiYmIQExMDwDzX9cSJE9zqnM11lclkEIlEFh9h9tSYxYABAzBixAhukNy0MGi1WjAM0+Z0jCFDhkCr1aKystLpjsQCXn6uWV5eDj8/P4wePbrDixBrIzLs7rqyshJarRZbtmzB+PHjcevWLURFReHSpUtOccQT6b2oKDpIV7pdSdfx+XxIJBJIJBIkJCRwDSBsrqtSqcTdu3chEAi4Bh6pVIry8nJcuHABsbGxPT5m0V5hYJNXHj16BFdXV26e8NmzZ06xS3z+/DnKy8uh1+sxZcoUmxQqFxcXeHp6wtPTEwAwYcIEqNVq5Ofn46233sLdu3cRHh4OsViMn376yakeHZPeg4qig3Sl27W9zjXqaOsctgEkMjISkZGRAF7uJquqqnD16lXk5eUhMTERADB9+nRcunQJT58+dUiuqykXFxcMGzYMTU1N0Ov1CA4Ohru7Ozc7WVVVhWfPnpmFftsrfcYadnc4ZswY+Pj42KU43b9/H4mJiQgODkZeXh7X6GQ0GlFdXW3XgqhUKnHo0CFuV759+3buiYS1RjjSe1BR7EXkcjkWLFiA5ORk1NTUgGEYhIeHw2g0UkebDfD5fPj5+eHFixfYuXMnkpOTsWLFCty8edNhua6tsaeADB06FFKplCvQrWcnm5ubodPpUF1dzY1B2Cp9xhrT3WFoaKhdul5fvHiBzMxMpKenY/fu3fjf//5n9j6PxzMLzrCXNWvWYN26dWav3b59G9nZ2SgtLTVrhHOmEAfyelQUnZBpt+vcuXO5bteJEyfiww8/5HYr+/fv5/7C7du3D7Nnz+Y62iZOnNjl6/f3lbBQKMTJkye5Qe+pU6di6tSpAP6LmGObeNLS0lBXVwexWGzTXNfWTD/XDAwM5B4hWmI6BsEynZ3UarU2n53sid1hbW0tkpKS4OvriytXrjjVodEAkJOTg7i4OAwcOBBjx46FSCRCcXExIiIiHH1rpBOo+5S0oVQq4e7ubnElbGkkpL+vhE1zXYuKinDjxg24ublhypQpXKHsaK6rJU1NTdwO1d/f3ya/3+zsZENDA3Q6HRf6/cYbb3A7Snd399fe8/Pnz8EwDPdo2R67Q6PRiOPHj+O7775DamoqoqOjHfp5oVKpxA8//AAPDw9IpVLs2rULXl5eSEpKwrRp0/Dxxx8DAD755BNER0fjgw8+cNi9EjPUfUpsi1bClnU211UmkyEsLOy1hx2/ePEC9+/fR11dHQIDA+Hh4WGzezYN/WbTZ9jQ78ePH6OqqgpNTU3cZ5iWZifZQ5N9fX0RGBhol0JVV1eH5ORkDB48GIWFhT0WsN5eI5xCocCmTZvA4/GwadMmrF27FhkZGT1yX8T+qCgSi/bt24esrCyzlXB7IyHkP9ZyXTUaDa5evYrTp09j27ZtXK4rWyjFYjG3C9RoNKivr8eIESMglUp7pFHGNPSbHfFpaWnh5iarq6uh1+sxePBgPHv2DEajEZMmTerWkV3WGI1GnD59Gl9//TW2bNmC2NjYHt0ddrQRbtmyZXj33XcBUIRbX0FFsZ+ilXDP4vP5EIlEEIlEZrmuJSUlKCoqwo4dO1BWVgZvb2/w+XxotVpkZ2dj7NixDn1U6ObmZjY7+fDhQ9y5cwfDhg0Dn89HaWlpm9nJoUOHduuedTodvvzySzQ2NiI/P98uJ2d0R21tLXfs1KlTpxAUFATAeiMc6V2oKPZTtBJ2vCFDhmDGjBmYMWMGAODq1atQKBQIDg5GcHAwVq5cicePH3O5rjKZDJMmTXLIHKLBYADDMHjy5AlCQ0PNsl5NZyc1Gg2am5u5LFNr505aYjQaUVhYiA0bNmDt2rVYuHChU4R4t/bFF19ArVaDx+PBz88PaWlpANBuIxzpPajRhrRhuhL+/vvvUVRUxLWaL1iwgGu0iYqKAsMwNvmL31cDzTvjq6++wpIlSyASibjXWlpauJEQlUqFmzdvcuMYts51tYY9i1EoFEIgEHToWuy5k2zHK5tlajo7afrnprm5GZs2bUJFRQUOHz5sltBEiI1Q9inpmoULF7ZZCbNF8ptvvkFGRgZcXV2xe/duREdHd/t6BoOhTwea2xKb61pcXMx1u9bU1CAgIIDbTXYl19USg8GAu3fvoqmpCRMmTOjWWYymR0g9fvwYjY2NKC4uxvXr1zFmzBjk5uYiMTERCoXCKXeHpE+gokh6BzZmjQLNu8Y017WoqAjXr1+H0WjsUK6rNTqdDnfu3IFAIIBQKLTLTvTBgwfYtm0bbt68CS8vL2i1WgiFQsTFxXGfuxJiQx36Q0xLMuJw1dXVbQLNqau149hc14SEBKSlpaGoqAgXL15EfHw86uvroVQqERERgfnz5yMlJQX5+fnQ6XSwtCA2GAwoLy/HvXv3MHnyZPj6+tqlIF6/fh2xsbGQSCRQqVQ4f/481Go1Dh48iJCQEJtf7/jx45g4cSL4fD6uXbtm9l5KSgpEIhHEYjG3MANePtIXi8UQiURITU21+T0R50SNNoT0Me3luqpUKly4cAE7duzAkydPEBQUxAUMaLVa5ObmYt26dRg3bpxdimFLSwu+/fZbFBQUIDMzk+vcZO/b19fXLp8nBgUF4eTJk1ixYoXZ69ai2QAgMTHR7JG+XC6nR/r9ABVF4nDU1Wp/bK6rn58f4uLiALxshlGr1bh8+TKWLl2KhoYGBAYG4vDhw1wjjy1zXW/fvo2kpCTMmTMHly9ftvl5lO2RSCQWX7cWSAEAIpEI/v7+AIC4uDjk5ORQUewHqCgSh5PJZD0WaO7n58d1Prq6uuLatWt4+PAhPvroI1RWVsLPzw8///wzvLy87HJ9ZzJw4EBIJBIkJSVBoVAgMTER9fX1XK5renq6TXJdDQYD9u/fj19++QUHDx7kDs12BnRGKWmNiiJxOFdXV5sGmr/OxYsXuWF0AEhNTUVUVBTWr1+P1NRUpKamYseOHXa7vjPx8PBAbm4udx7jqFGjIJfLIZfLAfyX66pSqfDjjz9i3bp1ncp1raiowMqVKxEeHo4rV67YJRuVRWeUElugokicQkxMDHcSR0/LyclBYWEhAGDx4sWIjIzsN0URAFcQLTHNdV2+fDmMRiMaGhq4kZATJ07g/v37GDNmDPfINSwsDEOHDkVGRgYyMjKwZ88ezJw50+7fB51RSmyBRjJIvzJ27Fh4eXmBx+NhxYoVWL58OTw9PaHT6QC8nKfz8vLifk1ezzTXVaVSoaSkBPfu3cO8efOwZ88esyOsHC0yMhI7d+7kHuFaC6QwGo0YP348CgoKIBAIIJPJcPToUbs+wSB2R6dkENLalStXIBAI8ODBA8yaNQuBgYFm7/N4PIdmjfZGlnJd6+vrMXz4cKcZxHf0GaWk96CdIum32HMjDx06hMLCQvj4+KC2thaRkZEoKytz9O0RQmyLhvcJMdXc3IzGxkbu5+fPn0dQUBDkcjkyMzMBAJmZmdSUQUg/RkWR9BtarRbTp09HcHAwwsPDMXfuXMyZMwfr16/Hb7/9hnHjxiE/P98mYeQJCQkYOXKk2XD6w4cPMWvWLIwbNw6zZs3Co0ePALz8HHP16tUQiUSYPHky/vjjj25fnxDSNVQUSb/h7++PGzdu4MaNGygtLcXGjRsBAN7e3igoKADDMMjPz7fJ6e5LlizBuXPnzF5jRz8YhkFUVBQXHXb27FkwDAOGYZCeng6FQtHt6/cl1iLaKisrMXjwYISEhCAkJASffvop915JSQkmTZoEkUiE1atXW4y0I8QSKoqE2MHMmTPbFNecnBwsXrwYwMvRj19//ZV7fdGiReDxeJg2bRp0Oh1qa2t7/J6dFRvRZmmsIyAgAGq1mstNZSkUChw6dIhbbLReoBBiDRVFQnqIVqvljuAaPXo0tFotAApEfx2JRAKxWNzhr6+trUVDQwOmTZsGHo+HRYsWcQsQQl6HiiIhDkCjH7ZRUVGB0NBQvP322/j9998BvFxkCIVC7mtokUE6g+YUCekho0aNQm1tLTf6MXLkSAAUiA50LaLNx8cHVVVV8Pb2RklJCd577z2Ulpba+1ZJH0c7RUJ6iLXRD7lcjqysLBiNRqhUKgwbNox7zNoZljpelUolBAIB14xy5swZ7j1r5wg6Qn5+Pm7dutXmR3vjMQMHDoS3tzcAICwsDAEBASgvL4dAIMA///zDfV1/XGSQrqOiSIgdxMfHIyIiAmVlZRAKhThy5IjV0Y+YmBj4+/tDJBJh2bJlOHDgQJeuaanjFQDWrFnDNaOw+bKm5wieO3cOK1euhMFg6Po37AB1dXXcPWs0GjAMA39/f/j4+MDDwwMqlQpGoxFZWVk0e0o6jB6fEmIHx44ds/h6QUFBm9d4PB7279/f7WvOnDkTlZWVHfpaa+cIRkREdPs+bM1aRNvly5exefNmuLm5gc/n4+DBg1zH74EDB7BkyRI8ffoU0dHRiI6OdvB3QXoLKoqE9HH79u1DVlYWpFIpdu3aBS8vr3bPEXQ28+fPx/z589u8Hhsbi9jYWIv/jVQqxa1bt+x9a6QPosenhPRhCoUC9+7dg1qtho+PD9auXevoWyLEqVFRJKQPGzVqFFxcXMDn87Fs2TIUFxcDoI5XQqyhokhIH2aajHPq1CmuM1UulyM7Oxt6vR4VFRVgGAbh4eGOuk1CnAZ9pkhIHxEfH4/CwkLU19dDKBRi69atKCwshFqtBo/Hg5+fH9LS0gCg3XMECenPOnueIiGEENJn0eNTQggh5BUqioQQQsgrVBQJIYSQV6goEkIIIa9QUSSEEEJeoaJICCGEvEJFkRBCCHmFiiIhhBDyChVFQggh5BUqioQQQsgr/weoiuo4CYOleQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = Network(13)\n",
    "losses = []\n",
    "#只画出参数w5和w9在区间[-160, 160]的曲线部分，以及包含损失函数的极值\n",
    "w5 = np.arange(-160.0, 160.0, 1.0)\n",
    "w9 = np.arange(-160.0, 160.0, 1.0)\n",
    "losses = np.zeros([len(w5), len(w9)])\n",
    "\n",
    "#计算设定区域内每个参数取值所对应的Loss\n",
    "for i in range(len(w5)):\n",
    "    for j in range(len(w9)):\n",
    "        net.w[5] = w5[i]\n",
    "        net.w[9] = w9[j]\n",
    "        z = net.forward(x)\n",
    "        loss = net.loss(z, y)\n",
    "        losses[i, j] = loss\n",
    "\n",
    "#使用matplotlib将两个变量和对应的Loss作3D图\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "w5, w9 = np.meshgrid(w5, w9)\n",
    "\n",
    "ax.plot_surface(w5, w9, losses, rstride=1, cstride=1, cmap='rainbow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "从图中可以明显观察到有些区域的函数值比周围的点小。需要说明的是：为什么选择$w_5$和$w_9$来画图呢？这是因为选择这两个参数的时候，可比较直观的从损失函数的曲面图上发现极值点的存在。其他参数组合，从图形上观测损失函数的极值点不够直观。\n",
    "\n",
    "观察上述曲线呈现出“圆滑”的坡度，这正是我们选择以均方误差作为损失函数的原因之一。**图6** 呈现了只有一个参数维度时，均方误差和绝对值误差（只将每个样本的误差累加，不做平方处理）的损失函数曲线图。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/99487dca6520441db5073d1c154b5d2fb1174b5cf4d946c29f9d80a209bc2687\" width=\"700\" hegiht=\"40\" ></center>\n",
    "<center><br>图6：均方误差和绝对值误差损失函数曲线图</br></center>\n",
    "<br></br>\n",
    "\n",
    "由此可见，均方误差表现的“圆滑”的坡度有两个好处：\n",
    "\n",
    "* 曲线的最低点是可导的。\n",
    "* 越接近最低点，曲线的坡度逐渐放缓，有助于通过当前的梯度来判断接近最低点的程度（是否逐渐减少步长，以免错过最低点）。\n",
    "\n",
    "而绝对值误差是不具备这两个特性的，这也是损失函数的设计不仅仅要考虑“合理性”，还要追求“易解性”的原因。\n",
    "\n",
    "现在我们要找出一组$[w_5, w_9]$的值，使得损失函数最小，实现梯度下降法的方案如下：\n",
    "\n",
    "- 步骤1：随机的选一组初始值，例如：$[w_5, w_9] = [-100.0, -100.0]$\n",
    "- 步骤2：选取下一个点$[w_5^{'} , w_9^{'}]$，使得$L(w_5^{'} , w_9^{'}) < L(w_5, w_9)$\n",
    "- 步骤3：重复步骤2，直到损失函数几乎不再下降。\n",
    "\n",
    "如何选择$[w_5^{'} , w_9^{'}]$是至关重要的，第一要保证$L$是下降的，第二要使得下降的趋势尽可能的快。微积分的基础知识告诉我们：沿着梯度的反方向，是函数值下降最快的方向，如 **图7** 所示。简单理解，函数在某一个点的梯度方向是曲线斜率最大的方向，但梯度方向是向上的，所以下降最快的是梯度的反方向。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/5f8322f6172542dab0f78684b70efe45d819895332af4cabb7c536217ab0bb26\" width=\"400\" hegiht=\"40\" ></center>\n",
    "<center><br>图7：梯度下降方向示意图</br></center>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.4.2 梯度计算\n",
    "\n",
    "上文已经介绍了损失函数的计算方法，这里稍微改写。为了使梯度计算更加简洁，引入因子$\\frac{1}{2}$，定义损失函数如下：\n",
    "\n",
    "$$L= \\frac{1}{2N}\\sum_{i=1}^N{(y_i - z_i)^2} (公式14)$$\n",
    "\n",
    "其中$z_i$是网络对第$i$个样本的预测值：\n",
    "\n",
    "$$z_i = \\sum_{j=0}^{12}{x_i^{j}\\cdot w_j} + b (公式15)$$\n",
    "\n",
    "梯度的定义：\n",
    "\n",
    "$$𝑔𝑟𝑎𝑑𝑖𝑒𝑛𝑡 = (\\frac{\\partial{L}}{\\partial{w_0}},\\frac{\\partial{L}}{\\partial{w_1}}, ... ,\\frac{\\partial{L}}{\\partial{w_{12}}} ,\\frac{\\partial{L}}{\\partial{b}}) (公式16)$$\n",
    "\n",
    "可以计算出$L$对$w$和$b$的偏导数：\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{w_j}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)\\frac{\\partial{z_i}}{\\partial{w_j}}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)x_i^{j}} (公式17)$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{b}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)\\frac{\\partial{z_i}}{\\partial{b}}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)} (公式18)$$\n",
    "\n",
    "从导数的计算过程可以看出，因子$\\frac{1}{2}$被消掉了，这是因为二次函数求导的时候会产生因子$2$，这也是我们将损失函数改写的原因。\n",
    "\n",
    "下面我们考虑只有一个样本的情况下，计算梯度：\n",
    "\n",
    "$$L= \\frac{1}{2}{(y_i - z_i)^2} (公式19)$$\n",
    "\n",
    "$$z_1 = {x_1^{0}\\cdot w_0} + {x_1^{1}\\cdot w_1} + ...  + {x_1^{12}\\cdot w_{12}} + b (公式20)$$\n",
    "\n",
    "可以计算出：\n",
    "\n",
    "$$L= \\frac{1}{2}{({x_1^{0}\\cdot w_0} + {x_1^{1}\\cdot w_1} + ...  + {x_1^{12}\\cdot w_{12}} + b - y_1)^2} (公式21)$$\n",
    "\n",
    "可以计算出$L$对$w$和$b$的偏导数：\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{w_0}} = ({x_1^{0}\\cdot w_0} + {x_1^{1}\\cdot w_1} + ...  + {x_1^{12}\\cdot w_12} + b - y_1)\\cdot x_1^{0}=({z_1} - {y_1})\\cdot x_1^{0} (公式22)$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{b}} = ({x_1^{0}\\cdot w_0} + {x_1^{1}\\cdot w_1} + ...  + {x_1^{12}\\cdot w_{12}} + b - y_1)\\cdot 1 = ({z_1} - {y_1}) (公式23)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\n",
    "可以通过具体的程序查看每个变量的数据和维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 [-0.02146321  0.03767327 -0.28552309 -0.08663366  0.01289726  0.04634817\n",
      "  0.00795597 -0.00765794 -0.25172191 -0.11881188 -0.29002528  0.0519112\n",
      " -0.17590923], shape (13,)\n",
      "y1 [-0.00390539], shape (1,)\n",
      "z1 [-12.05947643], shape (1,)\n"
     ]
    }
   ],
   "source": [
    "x1 = x[0]\n",
    "y1 = y[0]\n",
    "z1 = net.forward(x1)\n",
    "print('x1 {}, shape {}'.format(x1, x1.shape))\n",
    "print('y1 {}, shape {}'.format(y1, y1.shape))\n",
    "print('z1 {}, shape {}'.format(z1, z1.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "按上面的公式，当只有一个样本时，可以计算某个$w_j$，比如$w_0$的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w0 [0.25875126]\n"
     ]
    }
   ],
   "source": [
    "gradient_w0 = (z1 - y1) * x1[0]\n",
    "print('gradient_w0 {}'.format(gradient_w0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "同样我们可以计算$w_1$的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w1 [-0.45417275]\n"
     ]
    }
   ],
   "source": [
    "gradient_w1 = (z1 - y1) * x1[1]\n",
    "print('gradient_w1 {}'.format(gradient_w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "依次计算$w_2$的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w1 [3.44214394]\n"
     ]
    }
   ],
   "source": [
    "gradient_w2= (z1 - y1) * x1[2]\n",
    "print('gradient_w1 {}'.format(gradient_w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.25875126 -0.45417275  3.44214394  1.04441828 -0.15548386 -0.55875363\n",
      " -0.09591377  0.09232085  3.03465138  1.43234507  3.49642036 -0.62581917\n",
      "  2.12068622]\n"
     ]
    }
   ],
   "source": [
    "gradient_w=(z1-y1)*x1\n",
    "print(gradient_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "聪明的读者可能已经想到，写一个for循环即可计算从$w_0$到$w_{12}$的所有权重的梯度，该方法读者可以自行实现。\n",
    "\n",
    "#### 2.4.3 使用NumPy进行梯度计算\n",
    "\n",
    "基于NumPy广播机制（对向量和矩阵计算如同对1个单一变量计算一样），可以更快速的实现梯度计算。计算梯度的代码中直接用$(z_1 - y_1) \\cdot x_1$，得到的是一个13维的向量，每个分量分别代表该维度的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w_by_sample1 [ 0.25875126 -0.45417275  3.44214394  1.04441828 -0.15548386 -0.55875363\n",
      " -0.09591377  0.09232085  3.03465138  1.43234507  3.49642036 -0.62581917\n",
      "  2.12068622], gradient.shape (13,)\n"
     ]
    }
   ],
   "source": [
    "gradient_w = (z1 - y1) * x1\n",
    "print('gradient_w_by_sample1 {}, gradient.shape {}'.format(gradient_w, gradient_w.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "输入数据中有多个样本，每个样本都对梯度有贡献。如上代码计算了只有样本1时的梯度值，同样的计算方法也可以计算样本2和样本3对梯度的贡献。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w_by_sample2 [ 0.7329239   4.91417754  3.33394253  2.9912385   4.45673435 -0.58146277\n",
      " -5.14623287 -2.4894594   7.19011988  7.99471607  0.83100061 -1.79236081\n",
      "  2.11028056], gradient.shape (13,)\n"
     ]
    }
   ],
   "source": [
    "x2 = x[1]\n",
    "y2 = y[1]\n",
    "z2 = net.forward(x2)\n",
    "gradient_w = (z2 - y2) * x2\n",
    "print('gradient_w_by_sample2 {}, gradient.shape {}'.format(gradient_w, gradient_w.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w_by_sample3 [ 0.25138584  1.68549775  1.14349809  1.02595515  1.5286008  -1.93302947\n",
      "  0.4058236  -0.85385157  2.46611579  2.74208162  0.28502219 -0.46695229\n",
      "  2.39363651], gradient.shape (13,)\n"
     ]
    }
   ],
   "source": [
    "x3 = x[2]\n",
    "y3 = y[2]\n",
    "z3 = net.forward(x3)\n",
    "gradient_w = (z3 - y3) * x3\n",
    "print('gradient_w_by_sample3 {}, gradient.shape {}'.format(gradient_w, gradient_w.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "可能有的读者再次想到可以使用`for`循环把每个样本对梯度的贡献都计算出来，然后再作平均。但是我们不需要这么做，仍然可以使用NumPy的矩阵操作来简化运算，如3个样本的情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x [[-0.02146321  0.03767327 -0.28552309 -0.08663366  0.01289726  0.04634817\n",
      "   0.00795597 -0.00765794 -0.25172191 -0.11881188 -0.29002528  0.0519112\n",
      "  -0.17590923]\n",
      " [-0.02122729 -0.14232673 -0.09655922 -0.08663366 -0.12907805  0.0168406\n",
      "   0.14904763  0.0721009  -0.20824365 -0.23154675 -0.02406783  0.0519112\n",
      "  -0.06111894]\n",
      " [-0.02122751 -0.14232673 -0.09655922 -0.08663366 -0.12907805  0.1632288\n",
      "  -0.03426854  0.0721009  -0.20824365 -0.23154675 -0.02406783  0.03943037\n",
      "  -0.20212336]], shape (3, 13)\n",
      "y [[-0.00390539]\n",
      " [-0.05723872]\n",
      " [ 0.23387239]], shape (3, 1)\n",
      "z [[-12.05947643]\n",
      " [-34.58467747]\n",
      " [-11.60858134]], shape (3, 1)\n"
     ]
    }
   ],
   "source": [
    "# 注意这里是一次取出3个样本的数据，不是取出第3个样本\n",
    "x3samples = x[0:3]\n",
    "y3samples = y[0:3]\n",
    "z3samples = net.forward(x3samples)\n",
    "\n",
    "print('x {}, shape {}'.format(x3samples, x3samples.shape))\n",
    "print('y {}, shape {}'.format(y3samples, y3samples.shape))\n",
    "print('z {}, shape {}'.format(z3samples, z3samples.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "$x3 samples$、 $y3samples$ 和 $z3samples$的第一维大小均为3，表示有3个样本,下面计算这3个样本对梯度的贡献。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w [[ 0.25875126 -0.45417275  3.44214394  1.04441828 -0.15548386 -0.55875363\n",
      "  -0.09591377  0.09232085  3.03465138  1.43234507  3.49642036 -0.62581917\n",
      "   2.12068622]\n",
      " [ 0.7329239   4.91417754  3.33394253  2.9912385   4.45673435 -0.58146277\n",
      "  -5.14623287 -2.4894594   7.19011988  7.99471607  0.83100061 -1.79236081\n",
      "   2.11028056]\n",
      " [ 0.25138584  1.68549775  1.14349809  1.02595515  1.5286008  -1.93302947\n",
      "   0.4058236  -0.85385157  2.46611579  2.74208162  0.28502219 -0.46695229\n",
      "   2.39363651]], gradient.shape (3, 13)\n"
     ]
    }
   ],
   "source": [
    "gradient_w = (z3samples - y3samples) * x3samples\n",
    "print('gradient_w {}, gradient.shape {}'.format(gradient_w, gradient_w.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "此处可见，计算梯度`gradient_w`的维度是$3 \\times 13$，并且其第1行与上面第1个样本计算的梯度gradient_w_by_sample1一致，第2行与上面第2个样本计算的梯度gradient_w_by_sample2一致，第3行与上面第3个样本计算的梯度gradient_w_by_sample3一致。这里使用矩阵操作，可以更加方便的对3个样本分别计算各自对梯度的贡献。\n",
    "\n",
    "那么对于有N个样本的情形，我们可以直接使用如下方式计算出所有样本对梯度的贡献，这就是使用NumPy库广播功能带来的便捷。\n",
    "小结一下这里使用NumPy库的广播功能：\n",
    "- 一方面可以扩展参数的维度，代替for循环来计算1个样本对从$w_0$到$w_12$的所有参数的梯度。\n",
    "- 另一方面可以扩展样本的维度，代替for循环来计算样本0到样本403对参数的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w shape (404, 13)\n",
      "[[  0.25875126  -0.45417275   3.44214394 ...   3.49642036  -0.62581917\n",
      "    2.12068622]\n",
      " [  0.7329239    4.91417754   3.33394253 ...   0.83100061  -1.79236081\n",
      "    2.11028056]\n",
      " [  0.25138584   1.68549775   1.14349809 ...   0.28502219  -0.46695229\n",
      "    2.39363651]\n",
      " ...\n",
      " [ 14.70025543 -15.10890735  36.23258734 ...  24.54882966   5.51071122\n",
      "   26.26098922]\n",
      " [  9.29832217 -15.33146159  36.76629344 ...  24.91043398  -1.27564923\n",
      "   26.61808955]\n",
      " [ 19.55115919 -10.8177237   25.94192351 ...  17.5765494    3.94557661\n",
      "   17.64891012]]\n"
     ]
    }
   ],
   "source": [
    "z = net.forward(x)\n",
    "gradient_w = (z - y) * x\n",
    "print('gradient_w shape {}'.format(gradient_w.shape))\n",
    "print(gradient_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "上面gradient_w的每一行代表了一个样本对梯度的贡献。根据梯度的计算公式，总梯度是对每个样本对梯度贡献的平均值。\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{w_j}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)\\frac{\\partial{z_i}}{\\partial{w_j}}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)x_i^{j}} (公式24)$$\n",
    "\n",
    "可以使用NumPy的均值函数来完成此过程，代码实现如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w  (13,)\n",
      "w  (13, 1)\n",
      "[ 1.59697064 -0.92928123  4.72726926  1.65712204  4.96176389  1.18068454\n",
      "  4.55846519 -3.37770889  9.57465893 10.29870662  1.3900257  -0.30152215\n",
      "  1.09276043]\n",
      "[[ 1.76405235e+00]\n",
      " [ 4.00157208e-01]\n",
      " [ 9.78737984e-01]\n",
      " [ 2.24089320e+00]\n",
      " [ 1.86755799e+00]\n",
      " [ 1.59000000e+02]\n",
      " [ 9.50088418e-01]\n",
      " [-1.51357208e-01]\n",
      " [-1.03218852e-01]\n",
      " [ 1.59000000e+02]\n",
      " [ 1.44043571e-01]\n",
      " [ 1.45427351e+00]\n",
      " [ 7.61037725e-01]]\n"
     ]
    }
   ],
   "source": [
    "# axis = 0 表示把每一行做相加然后再除以总的行数\n",
    "gradient_w = np.mean(gradient_w, axis=0)\n",
    "print('gradient_w ', gradient_w.shape)\n",
    "print('w ', net.w.shape)\n",
    "print(gradient_w)\n",
    "print(net.w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "使用NumPy的矩阵操作方便地完成了gradient的计算，但引入了一个问题，`gradient_w`的形状是(13,)，而$w$的维度是(13, 1)。导致该问题的原因是使用`np.mean`函数时消除了第0维。为了加减乘除等计算方便，`gradient_w`和$w$必须保持一致的形状。因此我们将`gradient_w`的维度也设置为(13,1)，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_w shape (13, 1)\n"
     ]
    }
   ],
   "source": [
    "gradient_w = gradient_w[:, np.newaxis]\n",
    "print('gradient_w shape', gradient_w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "综合上面的剖析，计算梯度的代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.59697064],\n",
       "       [-0.92928123],\n",
       "       [ 4.72726926],\n",
       "       [ 1.65712204],\n",
       "       [ 4.96176389],\n",
       "       [ 1.18068454],\n",
       "       [ 4.55846519],\n",
       "       [-3.37770889],\n",
       "       [ 9.57465893],\n",
       "       [10.29870662],\n",
       "       [ 1.3900257 ],\n",
       "       [-0.30152215],\n",
       "       [ 1.09276043]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = net.forward(x)\n",
    "gradient_w = (z - y) * x\n",
    "gradient_w = np.mean(gradient_w, axis=0)\n",
    "gradient_w = gradient_w[:, np.newaxis]\n",
    "gradient_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "上述代码非常简洁地完成了$w$的梯度计算。同样，计算$b$的梯度的代码也是类似的原理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0918438870293816e-13"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_b = (z - y)\n",
    "gradient_b = np.mean(gradient_b)\n",
    "# 此处b是一个数值，所以可以直接用np.mean得到一个标量\n",
    "gradient_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "将上面计算$w$和$b$的梯度的过程，写成Network类的`gradient`函数，实现方法如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        num_samples = error.shape[0]\n",
    "        cost = error * error\n",
    "        cost = np.sum(cost) / num_samples\n",
    "        return cost\n",
    "    \n",
    "    def gradient(self, x, y):\n",
    "        z = self.forward(x)\n",
    "        gradient_w = (z-y)*x\n",
    "        gradient_w = np.mean(gradient_w, axis=0)\n",
    "        gradient_w = gradient_w[:, np.newaxis]\n",
    "        gradient_b = (z - y)\n",
    "        gradient_b = np.mean(gradient_b)\n",
    "        \n",
    "        return gradient_w, gradient_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point [-100.0, -100.0], loss 686.3005008179159\n",
      "gradient [-0.850073323995813, -6.138412364807849]\n"
     ]
    }
   ],
   "source": [
    "# 调用上面定义的gradient函数，计算梯度\n",
    "# 初始化网络\n",
    "net = Network(13)\n",
    "# 设置[w5, w9] = [-100., -100.]\n",
    "net.w[5] = -100.0\n",
    "net.w[9] = -100.0\n",
    "\n",
    "z = net.forward(x)\n",
    "loss = net.loss(z, y)\n",
    "gradient_w, gradient_b = net.gradient(x, y)\n",
    "gradient_w5 = gradient_w[5][0]\n",
    "gradient_w9 = gradient_w[9][0]\n",
    "print('point {}, loss {}'.format([net.w[5][0], net.w[9][0]], loss))\n",
    "print('gradient {}'.format([gradient_w5, gradient_w9]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.4.4 梯度更新\n",
    "\n",
    "下面研究更新梯度的方法，确定损失函数更小的点。首先沿着梯度的反方向移动一小步，找到下一个点P1，观察损失函数的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point [-99.91499266760042, -99.38615876351922], loss 678.6472185028845\n",
      "gradient [-0.8556356178645292, -6.0932268634065805]\n"
     ]
    }
   ],
   "source": [
    "# 在[w5, w9]平面上，沿着梯度的反方向移动到下一个点P1\n",
    "# 定义移动步长 eta\n",
    "eta = 0.1\n",
    "# 更新参数w5和w9\n",
    "net.w[5] = net.w[5] - eta * gradient_w5\n",
    "net.w[9] = net.w[9] - eta * gradient_w9\n",
    "# 重新计算z和loss\n",
    "z = net.forward(x)\n",
    "loss = net.loss(z, y)\n",
    "gradient_w, gradient_b = net.gradient(x, y)\n",
    "gradient_w5 = gradient_w[5][0]\n",
    "gradient_w9 = gradient_w[9][0]\n",
    "print('point {}, loss {}'.format([net.w[5][0], net.w[9][0]], loss))\n",
    "print('gradient {}'.format([gradient_w5, gradient_w9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "运行上面的代码，可以发现沿着梯度反方向走一小步，下一个点的损失函数的确减少了。感兴趣的话，大家可以尝试不停的点击上面的代码块，观察损失函数是否一直在变小。\n",
    "\n",
    "在上述代码中，每次更新参数使用的语句：\n",
    "`net.w[5] = net.w[5] - eta * gradient_w5`\n",
    "\n",
    "* 相减：参数需要向梯度的反方向移动。\n",
    "* eta：控制每次参数值沿着梯度反方向变动的大小，即每次移动的步长，又称为学习率。\n",
    "\n",
    "大家可以思考下，为什么之前我们要做输入特征的归一化，保持尺度一致？这是为了让统一的步长更加合适，使训练更加高效。\n",
    "\n",
    "如 **图8** 所示，特征输入归一化后，不同参数输出的Loss是一个比较规整的曲线，学习率可以设置成统一的值 ；特征输入未归一化时，不同特征对应的参数所需的步长不一致，尺度较大的参数需要大步长，尺寸较小的参数需要小步长，导致无法设置统一的学习率。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/903f552bc55b4a5eba71caa7dd86fd2d7b71b8ebb6cb4500a5f5711f465707f3\" width=\"300\" hegiht=\"40\" ></center>\n",
    "<center><br>图8：未归一化的特征，会导致不同特征维度的理想步长不同</br></center>\n",
    "<br></br>\n",
    "\n",
    "#### 2.4.5 封装Train函数\n",
    "\n",
    "将上面的循环计算过程封装在`train`和`update`函数中，实现方法如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, point [-99.99149926676004, -99.93861587635192], loss 686.3005008179159\n",
      "iter 50, point [-99.55950291459486, -96.92630620094545], loss 649.2144359737484\n",
      "iter 100, point [-99.1143836791974, -94.0227231684414], loss 614.6583805120641\n",
      "iter 150, point [-98.65689507661327, -91.22377715643974], loss 582.4474015283215\n",
      "iter 200, point [-98.18775974718623, -88.52553386435612], loss 552.4103147331971\n",
      "iter 250, point [-97.70767065020402, -85.92420840138672], loss 524.388658297806\n",
      "iter 300, point [-97.21729221288223, -83.41615959961561], loss 498.23574335719394\n",
      "iter 350, point [-96.71726143542814, -80.99788454368958], loss 473.81577544420406\n",
      "iter 400, point [-96.20818895385739, -78.66601330881193], loss 451.0030415538293\n",
      "iter 450, point [-95.69066006217508, -76.41730389912176], loss 429.6811579341994\n",
      "iter 500, point [-95.16523569546983, -74.24863737882666], loss 409.74237406676986\n",
      "iter 550, point [-94.63245337541025, -72.1570131887469], loss 391.0869286373302\n",
      "iter 600, point [-94.09282811957833, -70.13954464121011], loss 373.62245361317457\n",
      "iter 650, point [-93.5468533160177, -68.19345458650403], loss 357.26342283205247\n",
      "iter 700, point [-92.99500156432298, -66.31607124435298], loss 341.9306417770852\n",
      "iter 750, point [-92.43772548454575, -64.50482419413264], loss 327.55077546036\n",
      "iter 800, point [-91.87545849514491, -62.757240517778385], loss 314.05591156785766\n",
      "iter 850, point [-91.30861556116066, -61.07094108957052], loss 301.3831562311184\n",
      "iter 900, point [-90.73759391374827, -59.44363700720379], loss 289.47425998793403\n",
      "iter 950, point [-90.16277374216338, -57.873126158759185], loss 278.2752716764845\n",
      "iter 1000, point [-89.58451885924933, -56.35728992040244], loss 267.73621817589355\n",
      "iter 1050, point [-89.00317734143711, -54.894089979830845], loss 257.8108080621227\n",
      "iter 1100, point [-88.41908214422965, -53.481565280678495], loss 248.45615739241018\n",
      "iter 1150, point [-87.83255169410579, -52.11782908327363], loss 239.63253596498384\n",
      "iter 1200, point [-87.24389045774262, -50.801066137316646], loss 231.30313252430267\n",
      "iter 1250, point [-86.65338948942181, -49.52952996221624], loss 223.43383749639403\n",
      "iter 1300, point [-86.06132695745193, -48.301540230983434], loss 215.9930419446124\n",
      "iter 1350, point [-85.46796865040659, -47.11548025373949], loss 208.95145153400557\n",
      "iter 1400, point [-84.87356846394941, -45.969794557044004], loss 202.28191438302676\n",
      "iter 1450, point [-84.27836886898521, -44.86298655539359], loss 195.95926176510483\n",
      "iter 1500, point [-83.68260136185022, -43.793616311381186], loss 189.96016070011396\n",
      "iter 1550, point [-83.08648689722693, -42.76029838113894], loss 184.26297754750686\n",
      "iter 1600, point [-82.49023630444216, -41.76169974181719], loss 178.84765177925001\n",
      "iter 1650, point [-81.89405068778211, -40.796537797974494], loss 173.6955791720998\n",
      "iter 1700, point [-81.29812181143511, -39.86357846387417], loss 168.789503715592\n",
      "iter 1750, point [-80.70263246964815, -38.961634318795845], loss 164.11341758467972\n",
      "iter 1800, point [-80.10775684266093, -38.0895628325817], loss 159.65246857460858\n",
      "iter 1850, point [-79.51366083896096, -37.246264658742334], loss 155.39287444062649\n",
      "iter 1900, point [-78.92050242438167, -36.43068199254937], loss 151.32184362677512\n",
      "iter 1950, point [-78.32843193854492, -35.64179699163965], loss 147.42750190654235\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights,1)\n",
    "        self.w[5] = -100.\n",
    "        self.w[9] = -100.\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        num_samples = error.shape[0]\n",
    "        cost = error * error\n",
    "        cost = np.sum(cost) / num_samples\n",
    "        return cost\n",
    "    \n",
    "    def gradient(self, x, y):\n",
    "        z = self.forward(x)\n",
    "        gradient_w = (z-y)*x\n",
    "        gradient_w = np.mean(gradient_w, axis=0)\n",
    "        gradient_w = gradient_w[:, np.newaxis]\n",
    "        gradient_b = (z - y)\n",
    "        gradient_b = np.mean(gradient_b)        \n",
    "        return gradient_w, gradient_b\n",
    "    \n",
    "    def update(self, gradient_w5, gradient_w9, eta=0.01):\n",
    "        net.w[5] = net.w[5] - eta * gradient_w5\n",
    "        net.w[9] = net.w[9] - eta * gradient_w9\n",
    "        \n",
    "    def train(self, x, y, iterations=100, eta=0.01):\n",
    "        points = []\n",
    "        losses = []\n",
    "        for i in range(iterations):\n",
    "            points.append([net.w[5][0], net.w[9][0]])\n",
    "            z = self.forward(x)\n",
    "            L = self.loss(z, y)\n",
    "            gradient_w, gradient_b = self.gradient(x, y)\n",
    "            gradient_w5 = gradient_w[5][0]\n",
    "            gradient_w9 = gradient_w[9][0]\n",
    "            self.update(gradient_w5, gradient_w9, eta)\n",
    "            losses.append(L)\n",
    "            if i % 50 == 0:\n",
    "                print('iter {}, point {}, loss {}'.format(i, [net.w[5][0], net.w[9][0]], L))\n",
    "        return points, losses\n",
    "\n",
    "# 获取数据\n",
    "train_data, test_data = load_data()\n",
    "x = train_data[:, :-1]\n",
    "y = train_data[:, -1:]\n",
    "# 创建网络\n",
    "net = Network(13)\n",
    "num_iterations=2000\n",
    "# 启动训练\n",
    "points, losses = net.train(x, y, iterations=num_iterations, eta=0.01)\n",
    "\n",
    "# 画出损失函数的变化趋势\n",
    "plot_x = np.arange(num_iterations)\n",
    "plot_y = np.array(losses)\n",
    "plt.plot(plot_x, plot_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.4.6 训练过程扩展到全部参数\n",
    "\n",
    "为了能给读者直观的感受，上文演示的梯度下降的过程仅包含$w_5$和$w_9$两个参数。但房价预测的模型必须要对所有参数$w$和$b$进行求解，这需要将Network中的`update`和`train`函数进行修改。由于不再限定参与计算的参数（所有参数均参与计算），修改之后的代码反而更加简洁。\n",
    "\n",
    "实现逻辑：“前向计算输出、根据输出和真实值计算Loss、基于Loss和输入计算梯度、根据梯度更新参数值”四个部分反复执行，直到到损失函数最小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 9, loss 1.8984947314576224\n",
      "iter 19, loss 1.8031783384598725\n",
      "iter 29, loss 1.7135517565541092\n",
      "iter 39, loss 1.6292649416831264\n",
      "iter 49, loss 1.5499895293373231\n",
      "iter 59, loss 1.4754174896452612\n",
      "iter 69, loss 1.4052598659324693\n",
      "iter 79, loss 1.3392455915676864\n",
      "iter 89, loss 1.2771203802372915\n",
      "iter 99, loss 1.218645685090292\n",
      "iter 109, loss 1.1635977224791534\n",
      "iter 119, loss 1.111766556287068\n",
      "iter 129, loss 1.0629552390811503\n",
      "iter 139, loss 1.0169790065644477\n",
      "iter 149, loss 0.9736645220185994\n",
      "iter 159, loss 0.9328491676343147\n",
      "iter 169, loss 0.8943803798194307\n",
      "iter 179, loss 0.8581150257549611\n",
      "iter 189, loss 0.8239188186389669\n",
      "iter 199, loss 0.7916657692169988\n",
      "iter 209, loss 0.761237671346902\n",
      "iter 219, loss 0.7325236194855752\n",
      "iter 229, loss 0.7054195561163928\n",
      "iter 239, loss 0.6798278472589763\n",
      "iter 249, loss 0.6556568843183528\n",
      "iter 259, loss 0.6328207106387195\n",
      "iter 269, loss 0.6112386712285092\n",
      "iter 279, loss 0.59083508421862\n",
      "iter 289, loss 0.5715389327049418\n",
      "iter 299, loss 0.5532835757100347\n",
      "iter 309, loss 0.5360064770773406\n",
      "iter 319, loss 0.5196489511849665\n",
      "iter 329, loss 0.5041559244351539\n",
      "iter 339, loss 0.48947571154034963\n",
      "iter 349, loss 0.47555980568755696\n",
      "iter 359, loss 0.46236268171965056\n",
      "iter 369, loss 0.44984161152579916\n",
      "iter 379, loss 0.43795649088328303\n",
      "iter 389, loss 0.4266696770400226\n",
      "iter 399, loss 0.41594583637124666\n",
      "iter 409, loss 0.4057518014851036\n",
      "iter 419, loss 0.3960564371908221\n",
      "iter 429, loss 0.38683051477942226\n",
      "iter 439, loss 0.3780465941011246\n",
      "iter 449, loss 0.3696789129556087\n",
      "iter 459, loss 0.3617032833413179\n",
      "iter 469, loss 0.3540969941381648\n",
      "iter 479, loss 0.3468387198244131\n",
      "iter 489, loss 0.3399084348532937\n",
      "iter 499, loss 0.33328733333814486\n",
      "iter 509, loss 0.3269577537166779\n",
      "iter 519, loss 0.32090310808539985\n",
      "iter 529, loss 0.3151078159144129\n",
      "iter 539, loss 0.30955724187078903\n",
      "iter 549, loss 0.3042376374955925\n",
      "iter 559, loss 0.2991360864954391\n",
      "iter 569, loss 0.2942404534243286\n",
      "iter 579, loss 0.2895393355454012\n",
      "iter 589, loss 0.28502201767532415\n",
      "iter 599, loss 0.28067842982626157\n",
      "iter 609, loss 0.27649910747186535\n",
      "iter 619, loss 0.2724751542744919\n",
      "iter 629, loss 0.2685982071209627\n",
      "iter 639, loss 0.26486040332365085\n",
      "iter 649, loss 0.2612543498525749\n",
      "iter 659, loss 0.2577730944725093\n",
      "iter 669, loss 0.2544100986669443\n",
      "iter 679, loss 0.2511592122380609\n",
      "iter 689, loss 0.2480146494787638\n",
      "iter 699, loss 0.24497096681926708\n",
      "iter 709, loss 0.2420230418567802\n",
      "iter 719, loss 0.23916605368251415\n",
      "iter 729, loss 0.23639546442555454\n",
      "iter 739, loss 0.23370700193813704\n",
      "iter 749, loss 0.2310966435515475\n",
      "iter 759, loss 0.2285606008362593\n",
      "iter 769, loss 0.22609530530403904\n",
      "iter 779, loss 0.22369739499361888\n",
      "iter 789, loss 0.2213637018851542\n",
      "iter 799, loss 0.21909124009208833\n",
      "iter 809, loss 0.21687719478222933\n",
      "iter 819, loss 0.21471891178284025\n",
      "iter 829, loss 0.21261388782734392\n",
      "iter 839, loss 0.2105597614038757\n",
      "iter 849, loss 0.20855430416838638\n",
      "iter 859, loss 0.20659541288730932\n",
      "iter 869, loss 0.20468110187697833\n",
      "iter 879, loss 0.2028094959090178\n",
      "iter 889, loss 0.20097882355283644\n",
      "iter 899, loss 0.19918741092814596\n",
      "iter 909, loss 0.19743367584210875\n",
      "iter 919, loss 0.1957161222872899\n",
      "iter 929, loss 0.19403333527807176\n",
      "iter 939, loss 0.19238397600456975\n",
      "iter 949, loss 0.19076677728439412\n",
      "iter 959, loss 0.1891805392938162\n",
      "iter 969, loss 0.18762412556104593\n",
      "iter 979, loss 0.18609645920539716\n",
      "iter 989, loss 0.18459651940712488\n",
      "iter 999, loss 0.18312333809366155\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_137/501705500.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[0mplot_x\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum_iterations\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[0mplot_y\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlosses\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 58\u001B[0;31m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mplot_x\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mplot_y\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     59\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        num_samples = error.shape[0]\n",
    "        cost = error * error\n",
    "        cost = np.sum(cost) / num_samples\n",
    "        return cost\n",
    "    \n",
    "    def gradient(self, x, y):\n",
    "        z = self.forward(x)\n",
    "        gradient_w = (z-y)*x\n",
    "        gradient_w = np.mean(gradient_w, axis=0)\n",
    "        gradient_w = gradient_w[:, np.newaxis]\n",
    "        gradient_b = (z - y)\n",
    "        gradient_b = np.mean(gradient_b)        \n",
    "        return gradient_w, gradient_b\n",
    "    \n",
    "    def update(self, gradient_w, gradient_b, eta = 0.01):\n",
    "        self.w = self.w - eta * gradient_w\n",
    "        self.b = self.b - eta * gradient_b\n",
    "        \n",
    "    def train(self, x, y, iterations=100, eta=0.01):\n",
    "        losses = []\n",
    "        for i in range(iterations):\n",
    "            z = self.forward(x)\n",
    "            L = self.loss(z, y)\n",
    "            gradient_w, gradient_b = self.gradient(x, y)\n",
    "            self.update(gradient_w, gradient_b, eta)\n",
    "            losses.append(L)\n",
    "            if (i+1) % 10 == 0:\n",
    "                print('iter {}, loss {}'.format(i, L))\n",
    "        return losses\n",
    "\n",
    "# 获取数据\n",
    "train_data, test_data = load_data()\n",
    "x = train_data[:, :-1]\n",
    "y = train_data[:, -1:]\n",
    "# 创建网络\n",
    "net = Network(13)\n",
    "num_iterations=1000\n",
    "# 启动训练\n",
    "losses = net.train(x,y, iterations=num_iterations, eta=0.01)\n",
    "\n",
    "# 画出损失函数的变化趋势\n",
    "plot_x = np.arange(num_iterations)\n",
    "plot_y = np.array(losses)\n",
    "plt.plot(plot_x, plot_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.4.7  随机梯度下降法（ Stochastic Gradient Descent）\n",
    "\n",
    "在上述程序中，每次损失函数和梯度计算都是基于数据集中的全量数据。对于波士顿房价预测任务数据集而言，样本数比较少，只有404个。但在实际问题中，数据集往往非常大，如果每次都使用全量数据进行计算，效率非常低，通俗地说就是“杀鸡焉用牛刀”。由于参数每次只沿着梯度反方向更新一点点，因此方向并不需要那么精确。一个合理的解决方案是每次从总的数据集中随机抽取出小部分数据来代表整体，基于这部分数据计算梯度和损失来更新参数，这种方法被称作随机梯度下降法（Stochastic Gradient Descent，SGD），核心概念如下：\n",
    "\n",
    "* mini-batch：每次迭代时抽取出来的一批数据被称为一个mini-batch。\n",
    "* batch_size：一个mini-batch所包含的样本数目称为batch_size。\n",
    "* epoch：当程序迭代的时候，按mini-batch逐渐抽取出样本，当把整个数据集都遍历到了的时候，则完成了一轮训练，也叫一个epoch。启动训练时，可以将训练的轮数num_epochs和batch_size作为参数传入。\n",
    "\n",
    "下面结合程序介绍具体的实现过程，涉及到数据处理和训练过程两部分代码的修改。\n",
    "\n",
    "* **数据处理代码修改**\n",
    "\n",
    "数据处理需要实现拆分数据批次和样本乱序（为了实现随机抽样的效果）两个功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 14)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取数据\n",
    "train_data, test_data = load_data()\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "train_data中一共包含404条数据，如果batch_size=10，即取前0-9号样本作为第一个mini-batch，命名train_data1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 14)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data1 = train_data[0:10]\n",
    "train_data1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "使用train_data1的数据（0-9号样本）计算梯度并更新网络参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9001866101467375]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network(13)\n",
    "x = train_data1[:, :-1]\n",
    "y = train_data1[:, -1:]\n",
    "loss = net.train(x, y, iterations=1, eta=0.01)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "再取出10-19号样本作为第二个mini-batch，计算梯度并更新网络参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1171681170130832]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data2 = train_data[10:20]\n",
    "x = train_data2[:, :-1]\n",
    "y = train_data2[:, -1:]\n",
    "loss = net.train(x, y, iterations=1, eta=0.01)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "按此方法不断的取出新的mini-batch，并逐渐更新网络参数。\n",
    "\n",
    "接下来，将train_data分成大小为batch_size的多个mini_batch，如下代码所示：将train_data分成 $\\frac{404}{10} + 1 = 41$ 个 mini_batch，其中前40个mini_batch，每个均含有10个样本，最后一个mini_batch只含有4个样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of mini_batches is  41\n",
      "first mini_batch shape  (10, 14)\n",
      "last mini_batch shape  (4, 14)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "n = len(train_data)\n",
    "mini_batches = [train_data[k:k+batch_size] for k in range(0, n, batch_size)]\n",
    "print('total number of mini_batches is ', len(mini_batches))\n",
    "print('first mini_batch shape ', mini_batches[0].shape)\n",
    "print('last mini_batch shape ', mini_batches[-1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "另外，这里是按顺序读取mini_batch，而SGD里面是随机抽取一部分样本代表总体。为了实现随机抽样的效果，我们先将train_data里面的样本顺序随机打乱，然后再抽取mini_batch。随机打乱样本顺序，需要用到`np.random.shuffle`函数，下面先介绍它的用法。\n",
    "\n",
    "------\n",
    "**说明：**\n",
    "\n",
    "通过大量实验发现，模型受训练后期的影响更大，类似于人脑总是对近期发生的事情记忆的更加清晰。为了避免数据样本集合的顺序干扰模型的训练效果，需要进行样本乱序操作。当然，如果训练样本的顺序就是样本产生的顺序，而我们期望模型更重视近期产生的样本（预测样本会和近期的训练样本分布更接近），则不需要乱序这个步骤。\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before shuffle [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "after shuffle [ 2 10  1  6 12  3  9  7  4  8  5 11]\n"
     ]
    }
   ],
   "source": [
    "# 新建一个array\n",
    "a = np.array([1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "print('before shuffle', a)\n",
    "np.random.shuffle(a)\n",
    "print('after shuffle', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "多次运行上面的代码，可以发现每次执行shuffle函数后的数字顺序均不同。\n",
    "上面举的是一个1维数组乱序的案例，我们再观察下2维数组乱序后的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before shuffle\n",
      " [[ 1  2]\n",
      " [ 3  4]\n",
      " [ 5  6]\n",
      " [ 7  8]\n",
      " [ 9 10]\n",
      " [11 12]]\n",
      "after shuffle\n",
      " [[ 7  8]\n",
      " [ 5  6]\n",
      " [ 3  4]\n",
      " [11 12]\n",
      " [ 1  2]\n",
      " [ 9 10]]\n"
     ]
    }
   ],
   "source": [
    "# 新建一个array\n",
    "a = np.array([1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "a = a.reshape([6, 2])\n",
    "print('before shuffle\\n', a)\n",
    "np.random.shuffle(a)\n",
    "print('after shuffle\\n', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "观察运行结果可发现，数组的元素在第0维被随机打乱，但第1维的顺序保持不变。例如数字2仍然紧挨在数字1的后面，数字8仍然紧挨在数字7的后面，而第二维的[3, 4]并不排在[1, 2]的后面。将这部分实现SGD算法的代码集成到Network类中的`train`函数中，最终的完整代码如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_137/3056455039.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;31m# 创建网络\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 13\u001B[0;31m \u001B[0mnet\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mNetwork\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m13\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;31m# 依次使用每个mini_batch的数据\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'Network' is not defined"
     ]
    }
   ],
   "source": [
    "# 获取数据\n",
    "train_data, test_data = load_data()\n",
    "\n",
    "# 打乱样本顺序\n",
    "np.random.shuffle(train_data)\n",
    "\n",
    "# 将train_data分成多个mini_batch\n",
    "batch_size = 10\n",
    "n = len(train_data)\n",
    "mini_batches = [train_data[k:k+batch_size] for k in range(0, n, batch_size)]\n",
    "\n",
    "# 创建网络\n",
    "net = Network(13)\n",
    "\n",
    "# 依次使用每个mini_batch的数据\n",
    "for mini_batch in mini_batches:\n",
    "    x = mini_batch[:, :-1]\n",
    "    y = mini_batch[:, -1:]\n",
    "    loss = net.train(x, y, iterations=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*  **训练过程代码修改**\n",
    "\n",
    "将每个随机抽取的mini-batch数据输入到模型中用于参数训练。训练过程的核心是两层循环：\n",
    "\n",
    "1. 第一层循环，代表样本集合要被训练遍历几次，称为“epoch”，代码如下：\n",
    "\n",
    "`for epoch_id in range(num_epochs):`\n",
    "\n",
    "2. 第二层循环，代表每次遍历时，样本集合被拆分成的多个批次，需要全部执行训练，称为“iter (iteration)”，代码如下：\n",
    "\n",
    "`for iter_id,mini_batch in emumerate(mini_batches):`\n",
    "\n",
    "在两层循环的内部是经典的四步训练流程：前向计算->计算损失->计算梯度->更新参数，这与大家之前所学是一致的，代码如下：\n",
    "\n",
    "                x = mini_batch[:, :-1]\n",
    "                y = mini_batch[:, -1:]\n",
    "                a = self.forward(x)  #前向计算\n",
    "                loss = self.loss(a, y)  #计算损失\n",
    "                gradient_w, gradient_b = self.gradient(x, y)  #计算梯度\n",
    "                self.update(gradient_w, gradient_b, eta)  #更新参数\n",
    "\n",
    "\n",
    "将两部分改写的代码集成到Network类中的`train`函数中，最终的实现如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 / iter   0, loss = 0.6273\n",
      "Epoch   0 / iter   1, loss = 0.4835\n",
      "Epoch   0 / iter   2, loss = 0.5830\n",
      "Epoch   0 / iter   3, loss = 0.5466\n",
      "Epoch   0 / iter   4, loss = 0.2147\n",
      "Epoch   1 / iter   0, loss = 0.6645\n",
      "Epoch   1 / iter   1, loss = 0.4875\n",
      "Epoch   1 / iter   2, loss = 0.4707\n",
      "Epoch   1 / iter   3, loss = 0.4153\n",
      "Epoch   1 / iter   4, loss = 0.1402\n",
      "Epoch   2 / iter   0, loss = 0.5897\n",
      "Epoch   2 / iter   1, loss = 0.4373\n",
      "Epoch   2 / iter   2, loss = 0.4631\n",
      "Epoch   2 / iter   3, loss = 0.3960\n",
      "Epoch   2 / iter   4, loss = 0.2340\n",
      "Epoch   3 / iter   0, loss = 0.4139\n",
      "Epoch   3 / iter   1, loss = 0.5635\n",
      "Epoch   3 / iter   2, loss = 0.3807\n",
      "Epoch   3 / iter   3, loss = 0.3975\n",
      "Epoch   3 / iter   4, loss = 0.1207\n",
      "Epoch   4 / iter   0, loss = 0.3786\n",
      "Epoch   4 / iter   1, loss = 0.4474\n",
      "Epoch   4 / iter   2, loss = 0.4019\n",
      "Epoch   4 / iter   3, loss = 0.4352\n",
      "Epoch   4 / iter   4, loss = 0.0435\n",
      "Epoch   5 / iter   0, loss = 0.4387\n",
      "Epoch   5 / iter   1, loss = 0.3886\n",
      "Epoch   5 / iter   2, loss = 0.3182\n",
      "Epoch   5 / iter   3, loss = 0.4189\n",
      "Epoch   5 / iter   4, loss = 0.1741\n",
      "Epoch   6 / iter   0, loss = 0.3191\n",
      "Epoch   6 / iter   1, loss = 0.3601\n",
      "Epoch   6 / iter   2, loss = 0.4199\n",
      "Epoch   6 / iter   3, loss = 0.3289\n",
      "Epoch   6 / iter   4, loss = 1.2691\n",
      "Epoch   7 / iter   0, loss = 0.3202\n",
      "Epoch   7 / iter   1, loss = 0.2855\n",
      "Epoch   7 / iter   2, loss = 0.4129\n",
      "Epoch   7 / iter   3, loss = 0.3331\n",
      "Epoch   7 / iter   4, loss = 0.2218\n",
      "Epoch   8 / iter   0, loss = 0.2368\n",
      "Epoch   8 / iter   1, loss = 0.3457\n",
      "Epoch   8 / iter   2, loss = 0.3339\n",
      "Epoch   8 / iter   3, loss = 0.3812\n",
      "Epoch   8 / iter   4, loss = 0.0534\n",
      "Epoch   9 / iter   0, loss = 0.3567\n",
      "Epoch   9 / iter   1, loss = 0.4033\n",
      "Epoch   9 / iter   2, loss = 0.1926\n",
      "Epoch   9 / iter   3, loss = 0.2803\n",
      "Epoch   9 / iter   4, loss = 0.1557\n",
      "Epoch  10 / iter   0, loss = 0.3435\n",
      "Epoch  10 / iter   1, loss = 0.2790\n",
      "Epoch  10 / iter   2, loss = 0.3456\n",
      "Epoch  10 / iter   3, loss = 0.2076\n",
      "Epoch  10 / iter   4, loss = 0.0935\n",
      "Epoch  11 / iter   0, loss = 0.3024\n",
      "Epoch  11 / iter   1, loss = 0.2517\n",
      "Epoch  11 / iter   2, loss = 0.2797\n",
      "Epoch  11 / iter   3, loss = 0.2989\n",
      "Epoch  11 / iter   4, loss = 0.0301\n",
      "Epoch  12 / iter   0, loss = 0.2507\n",
      "Epoch  12 / iter   1, loss = 0.2563\n",
      "Epoch  12 / iter   2, loss = 0.2971\n",
      "Epoch  12 / iter   3, loss = 0.2833\n",
      "Epoch  12 / iter   4, loss = 0.0597\n",
      "Epoch  13 / iter   0, loss = 0.2827\n",
      "Epoch  13 / iter   1, loss = 0.2094\n",
      "Epoch  13 / iter   2, loss = 0.2417\n",
      "Epoch  13 / iter   3, loss = 0.2985\n",
      "Epoch  13 / iter   4, loss = 0.4036\n",
      "Epoch  14 / iter   0, loss = 0.3085\n",
      "Epoch  14 / iter   1, loss = 0.2015\n",
      "Epoch  14 / iter   2, loss = 0.1830\n",
      "Epoch  14 / iter   3, loss = 0.2978\n",
      "Epoch  14 / iter   4, loss = 0.0630\n",
      "Epoch  15 / iter   0, loss = 0.2342\n",
      "Epoch  15 / iter   1, loss = 0.2780\n",
      "Epoch  15 / iter   2, loss = 0.2571\n",
      "Epoch  15 / iter   3, loss = 0.1838\n",
      "Epoch  15 / iter   4, loss = 0.0627\n",
      "Epoch  16 / iter   0, loss = 0.1896\n",
      "Epoch  16 / iter   1, loss = 0.1966\n",
      "Epoch  16 / iter   2, loss = 0.2018\n",
      "Epoch  16 / iter   3, loss = 0.3257\n",
      "Epoch  16 / iter   4, loss = 0.1268\n",
      "Epoch  17 / iter   0, loss = 0.1990\n",
      "Epoch  17 / iter   1, loss = 0.2031\n",
      "Epoch  17 / iter   2, loss = 0.2662\n",
      "Epoch  17 / iter   3, loss = 0.2128\n",
      "Epoch  17 / iter   4, loss = 0.0133\n",
      "Epoch  18 / iter   0, loss = 0.1780\n",
      "Epoch  18 / iter   1, loss = 0.1575\n",
      "Epoch  18 / iter   2, loss = 0.2547\n",
      "Epoch  18 / iter   3, loss = 0.2544\n",
      "Epoch  18 / iter   4, loss = 0.2007\n",
      "Epoch  19 / iter   0, loss = 0.1657\n",
      "Epoch  19 / iter   1, loss = 0.2000\n",
      "Epoch  19 / iter   2, loss = 0.2045\n",
      "Epoch  19 / iter   3, loss = 0.2524\n",
      "Epoch  19 / iter   4, loss = 0.0632\n",
      "Epoch  20 / iter   0, loss = 0.1629\n",
      "Epoch  20 / iter   1, loss = 0.1895\n",
      "Epoch  20 / iter   2, loss = 0.2523\n",
      "Epoch  20 / iter   3, loss = 0.1896\n",
      "Epoch  20 / iter   4, loss = 0.0918\n",
      "Epoch  21 / iter   0, loss = 0.1583\n",
      "Epoch  21 / iter   1, loss = 0.2322\n",
      "Epoch  21 / iter   2, loss = 0.1567\n",
      "Epoch  21 / iter   3, loss = 0.2089\n",
      "Epoch  21 / iter   4, loss = 0.2035\n",
      "Epoch  22 / iter   0, loss = 0.2273\n",
      "Epoch  22 / iter   1, loss = 0.1427\n",
      "Epoch  22 / iter   2, loss = 0.1712\n",
      "Epoch  22 / iter   3, loss = 0.1826\n",
      "Epoch  22 / iter   4, loss = 0.2878\n",
      "Epoch  23 / iter   0, loss = 0.1685\n",
      "Epoch  23 / iter   1, loss = 0.1622\n",
      "Epoch  23 / iter   2, loss = 0.1499\n",
      "Epoch  23 / iter   3, loss = 0.2329\n",
      "Epoch  23 / iter   4, loss = 0.1486\n",
      "Epoch  24 / iter   0, loss = 0.1617\n",
      "Epoch  24 / iter   1, loss = 0.2083\n",
      "Epoch  24 / iter   2, loss = 0.1442\n",
      "Epoch  24 / iter   3, loss = 0.1740\n",
      "Epoch  24 / iter   4, loss = 0.1641\n",
      "Epoch  25 / iter   0, loss = 0.1159\n",
      "Epoch  25 / iter   1, loss = 0.2064\n",
      "Epoch  25 / iter   2, loss = 0.1690\n",
      "Epoch  25 / iter   3, loss = 0.1778\n",
      "Epoch  25 / iter   4, loss = 0.0159\n",
      "Epoch  26 / iter   0, loss = 0.1730\n",
      "Epoch  26 / iter   1, loss = 0.1861\n",
      "Epoch  26 / iter   2, loss = 0.1387\n",
      "Epoch  26 / iter   3, loss = 0.1486\n",
      "Epoch  26 / iter   4, loss = 0.1090\n",
      "Epoch  27 / iter   0, loss = 0.1393\n",
      "Epoch  27 / iter   1, loss = 0.1775\n",
      "Epoch  27 / iter   2, loss = 0.1564\n",
      "Epoch  27 / iter   3, loss = 0.1245\n",
      "Epoch  27 / iter   4, loss = 0.7611\n",
      "Epoch  28 / iter   0, loss = 0.1470\n",
      "Epoch  28 / iter   1, loss = 0.1211\n",
      "Epoch  28 / iter   2, loss = 0.1285\n",
      "Epoch  28 / iter   3, loss = 0.1854\n",
      "Epoch  28 / iter   4, loss = 0.5240\n",
      "Epoch  29 / iter   0, loss = 0.1740\n",
      "Epoch  29 / iter   1, loss = 0.0898\n",
      "Epoch  29 / iter   2, loss = 0.1392\n",
      "Epoch  29 / iter   3, loss = 0.1842\n",
      "Epoch  29 / iter   4, loss = 0.0251\n",
      "Epoch  30 / iter   0, loss = 0.0978\n",
      "Epoch  30 / iter   1, loss = 0.1529\n",
      "Epoch  30 / iter   2, loss = 0.1640\n",
      "Epoch  30 / iter   3, loss = 0.1503\n",
      "Epoch  30 / iter   4, loss = 0.0975\n",
      "Epoch  31 / iter   0, loss = 0.1399\n",
      "Epoch  31 / iter   1, loss = 0.1595\n",
      "Epoch  31 / iter   2, loss = 0.1209\n",
      "Epoch  31 / iter   3, loss = 0.1203\n",
      "Epoch  31 / iter   4, loss = 0.2008\n",
      "Epoch  32 / iter   0, loss = 0.1501\n",
      "Epoch  32 / iter   1, loss = 0.1310\n",
      "Epoch  32 / iter   2, loss = 0.1065\n",
      "Epoch  32 / iter   3, loss = 0.1489\n",
      "Epoch  32 / iter   4, loss = 0.0818\n",
      "Epoch  33 / iter   0, loss = 0.1401\n",
      "Epoch  33 / iter   1, loss = 0.1367\n",
      "Epoch  33 / iter   2, loss = 0.0970\n",
      "Epoch  33 / iter   3, loss = 0.1481\n",
      "Epoch  33 / iter   4, loss = 0.0711\n",
      "Epoch  34 / iter   0, loss = 0.1157\n",
      "Epoch  34 / iter   1, loss = 0.1050\n",
      "Epoch  34 / iter   2, loss = 0.1378\n",
      "Epoch  34 / iter   3, loss = 0.1505\n",
      "Epoch  34 / iter   4, loss = 0.0429\n",
      "Epoch  35 / iter   0, loss = 0.1096\n",
      "Epoch  35 / iter   1, loss = 0.1279\n",
      "Epoch  35 / iter   2, loss = 0.1715\n",
      "Epoch  35 / iter   3, loss = 0.0888\n",
      "Epoch  35 / iter   4, loss = 0.0473\n",
      "Epoch  36 / iter   0, loss = 0.1350\n",
      "Epoch  36 / iter   1, loss = 0.0781\n",
      "Epoch  36 / iter   2, loss = 0.1458\n",
      "Epoch  36 / iter   3, loss = 0.1288\n",
      "Epoch  36 / iter   4, loss = 0.0421\n",
      "Epoch  37 / iter   0, loss = 0.1083\n",
      "Epoch  37 / iter   1, loss = 0.0972\n",
      "Epoch  37 / iter   2, loss = 0.1513\n",
      "Epoch  37 / iter   3, loss = 0.1236\n",
      "Epoch  37 / iter   4, loss = 0.0366\n",
      "Epoch  38 / iter   0, loss = 0.1204\n",
      "Epoch  38 / iter   1, loss = 0.1341\n",
      "Epoch  38 / iter   2, loss = 0.1109\n",
      "Epoch  38 / iter   3, loss = 0.0905\n",
      "Epoch  38 / iter   4, loss = 0.3906\n",
      "Epoch  39 / iter   0, loss = 0.0923\n",
      "Epoch  39 / iter   1, loss = 0.1094\n",
      "Epoch  39 / iter   2, loss = 0.1295\n",
      "Epoch  39 / iter   3, loss = 0.1239\n",
      "Epoch  39 / iter   4, loss = 0.0684\n",
      "Epoch  40 / iter   0, loss = 0.1188\n",
      "Epoch  40 / iter   1, loss = 0.0984\n",
      "Epoch  40 / iter   2, loss = 0.1067\n",
      "Epoch  40 / iter   3, loss = 0.1057\n",
      "Epoch  40 / iter   4, loss = 0.4602\n",
      "Epoch  41 / iter   0, loss = 0.1478\n",
      "Epoch  41 / iter   1, loss = 0.0980\n",
      "Epoch  41 / iter   2, loss = 0.0921\n",
      "Epoch  41 / iter   3, loss = 0.1020\n",
      "Epoch  41 / iter   4, loss = 0.0430\n",
      "Epoch  42 / iter   0, loss = 0.0991\n",
      "Epoch  42 / iter   1, loss = 0.0994\n",
      "Epoch  42 / iter   2, loss = 0.1270\n",
      "Epoch  42 / iter   3, loss = 0.0988\n",
      "Epoch  42 / iter   4, loss = 0.1176\n",
      "Epoch  43 / iter   0, loss = 0.1286\n",
      "Epoch  43 / iter   1, loss = 0.1013\n",
      "Epoch  43 / iter   2, loss = 0.1066\n",
      "Epoch  43 / iter   3, loss = 0.0779\n",
      "Epoch  43 / iter   4, loss = 0.1481\n",
      "Epoch  44 / iter   0, loss = 0.0840\n",
      "Epoch  44 / iter   1, loss = 0.0858\n",
      "Epoch  44 / iter   2, loss = 0.1388\n",
      "Epoch  44 / iter   3, loss = 0.1000\n",
      "Epoch  44 / iter   4, loss = 0.0313\n",
      "Epoch  45 / iter   0, loss = 0.0896\n",
      "Epoch  45 / iter   1, loss = 0.1173\n",
      "Epoch  45 / iter   2, loss = 0.0916\n",
      "Epoch  45 / iter   3, loss = 0.1043\n",
      "Epoch  45 / iter   4, loss = 0.0074\n",
      "Epoch  46 / iter   0, loss = 0.1008\n",
      "Epoch  46 / iter   1, loss = 0.0915\n",
      "Epoch  46 / iter   2, loss = 0.0877\n",
      "Epoch  46 / iter   3, loss = 0.1139\n",
      "Epoch  46 / iter   4, loss = 0.0292\n",
      "Epoch  47 / iter   0, loss = 0.0679\n",
      "Epoch  47 / iter   1, loss = 0.0987\n",
      "Epoch  47 / iter   2, loss = 0.0929\n",
      "Epoch  47 / iter   3, loss = 0.1098\n",
      "Epoch  47 / iter   4, loss = 0.4838\n",
      "Epoch  48 / iter   0, loss = 0.0693\n",
      "Epoch  48 / iter   1, loss = 0.1095\n",
      "Epoch  48 / iter   2, loss = 0.1128\n",
      "Epoch  48 / iter   3, loss = 0.0890\n",
      "Epoch  48 / iter   4, loss = 0.1008\n",
      "Epoch  49 / iter   0, loss = 0.0724\n",
      "Epoch  49 / iter   1, loss = 0.0804\n",
      "Epoch  49 / iter   2, loss = 0.0919\n",
      "Epoch  49 / iter   3, loss = 0.1233\n",
      "Epoch  49 / iter   4, loss = 0.1849\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_137/1533222288.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     68\u001B[0m \u001B[0mplot_x\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlosses\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[0mplot_y\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlosses\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 70\u001B[0;31m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mplot_x\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mplot_y\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     71\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        #np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        num_samples = error.shape[0]\n",
    "        cost = error * error\n",
    "        cost = np.sum(cost) / num_samples\n",
    "        return cost\n",
    "    \n",
    "    def gradient(self, x, y):\n",
    "        z = self.forward(x)\n",
    "        N = x.shape[0]\n",
    "        gradient_w = 1. / N * np.sum((z-y) * x, axis=0)\n",
    "        gradient_w = gradient_w[:, np.newaxis]\n",
    "        gradient_b = 1. / N * np.sum(z-y)\n",
    "        return gradient_w, gradient_b\n",
    "    \n",
    "    def update(self, gradient_w, gradient_b, eta = 0.01):\n",
    "        self.w = self.w - eta * gradient_w\n",
    "        self.b = self.b - eta * gradient_b\n",
    "            \n",
    "                \n",
    "    def train(self, training_data, num_epochs, batch_size=10, eta=0.01):\n",
    "        n = len(training_data)\n",
    "        losses = []\n",
    "        for epoch_id in range(num_epochs):\n",
    "            # 在每轮迭代开始之前，将训练数据的顺序随机打乱\n",
    "            # 然后再按每次取batch_size条数据的方式取出\n",
    "            np.random.shuffle(training_data)\n",
    "            # 将训练数据进行拆分，每个mini_batch包含batch_size条的数据\n",
    "            mini_batches = [training_data[k:k+batch_size] for k in range(0, n, batch_size)]\n",
    "            for iter_id, mini_batch in enumerate(mini_batches):\n",
    "                #print(self.w.shape)\n",
    "                #print(self.b)\n",
    "                x = mini_batch[:, :-1]\n",
    "                y = mini_batch[:, -1:]\n",
    "                a = self.forward(x)\n",
    "                loss = self.loss(a, y)\n",
    "                gradient_w, gradient_b = self.gradient(x, y)\n",
    "                self.update(gradient_w, gradient_b, eta)\n",
    "                losses.append(loss)\n",
    "                print('Epoch {:3d} / iter {:3d}, loss = {:.4f}'.\n",
    "                                 format(epoch_id, iter_id, loss))\n",
    "        \n",
    "        return losses\n",
    "\n",
    "# 获取数据\n",
    "train_data, test_data = load_data()\n",
    "\n",
    "# 创建网络\n",
    "net = Network(13)\n",
    "# 启动训练\n",
    "losses = net.train(train_data, num_epochs=50, batch_size=100, eta=0.1)\n",
    "\n",
    "# 画出损失函数的变化趋势\n",
    "plot_x = np.arange(len(losses))\n",
    "plot_y = np.array(losses)\n",
    "plt.plot(plot_x, plot_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "观察上述Loss的变化，随机梯度下降加快了训练过程，但由于每次仅基于少量样本更新参数和计算损失，所以损失下降曲线会出现震荡。\n",
    "\n",
    "------\n",
    "**说明：**\n",
    "\n",
    "由于房价预测的数据量过少，所以难以感受到随机梯度下降带来的性能提升。\n",
    "\n",
    "------\n",
    "\n",
    "### 2.5 模型保存\n",
    "Numpy提供了save接口，可直接将模型权重数组保存为.npy格式的文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.save('w.npy', net.w)\n",
    "np.save('b.npy', net.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 14)\n",
      "(102, 14)\n",
      "(404, 13) (404, 1)\n"
     ]
    }
   ],
   "source": [
    "# 自己实现一个\n",
    "train_data,eval_data=load_data()\n",
    "print(train_data.shape)\n",
    "print(eval_data.shape)\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Network_self(object):\n",
    "    def __init__(self,num_of_weights,lr):\n",
    "        self.num_of_weights=num_of_weights\n",
    "        np.random.seed(1000)\n",
    "        self.w=np.random.randn(num_of_weights,1)\n",
    "        self.b=0\n",
    "        self.lr=lr\n",
    "\n",
    "    def forward(self,input):        \n",
    "        z=np.dot(input,self.w)+self.b\n",
    "        #print(f'x.shape:{input.shape} z.shape {z.shape}')\n",
    "        return z\n",
    "\n",
    "    def loss(self,z,y):        \n",
    "        #print(f'y.shape:{y.shape}')\n",
    "        return np.mean(np.square(z-y))\n",
    "\n",
    "    def comp_gradient(self,input,y):\n",
    "        z=self.forward(input)\n",
    "        dj=(z-y)\n",
    "        #print('dj.shape',dj.shape)\n",
    "        dw=np.dot(input.T,dj)   \n",
    "        dz=np.mean(dj)\n",
    "        #print('dw.shape',dw.shape,dw,'dz.shape',dz.shape,dz)\n",
    "        return dw,dz\n",
    "\n",
    "    def update(self,dw,db):\n",
    "        self.w=self.w-self.lr*dw\n",
    "        self.b=self.b-self.lr*db    \n",
    "\n",
    "    def train(self,num_epochs,train_data,batch_size):\n",
    "        mini_batches=[train_data[k:k+batch_size,:] for k in range(0,len(train_data),batch_size)]\n",
    "        for i in range(num_epochs):\n",
    "            for j,mini_batch in enumerate(mini_batches):\n",
    "                x=mini_batch[:,:-1]\n",
    "                y=mini_batch[:,-1:]\n",
    "                z=self.forward(x)\n",
    "                loss=self.loss(z,y)\n",
    "                dw,dz=self.comp_gradient(x,y)\n",
    "                self.update(dw,dz)\n",
    "                print(f'{i} {j} {loss}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 13)\n",
      "0.07415720792396742\n",
      "0.027987155644590426\n",
      "0.011278356051494916\n",
      "0.005171745360818756\n",
      "0.0028960747771155475\n",
      "0.0020078143919125316\n",
      "0.0016246851255775606\n",
      "0.0014279442119772571\n",
      "0.001302276822401635\n",
      "0.0012058131749698572\n"
     ]
    }
   ],
   "source": [
    "net=Network_self(13,0.005)\n",
    "part_x=train_data[:3,:13]\n",
    "print(part_x.shape)\n",
    "y=train_data[:3,-1:]\n",
    "for i in range(1000):\n",
    "  z=net.forward(part_x)\n",
    "  loss=net.loss(z,y)\n",
    "  dw,dz=net.comp_gradient(part_x,y)\n",
    "  net.update(dw,dz)\n",
    "  #print(loss.shape)\n",
    "  if i%100==0: print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0.08550999478705966\n",
      "0 1 0.11436003135794558\n",
      "0 2 0.054458329283411364\n",
      "0 3 0.08457347391601003\n",
      "0 4 0.1672038790434872\n",
      "1 0 0.07093809784284878\n",
      "1 1 0.09528647228923179\n",
      "1 2 0.04554444625711046\n",
      "1 3 0.07247660818177697\n",
      "1 4 0.13654250502712212\n",
      "2 0 0.06011824404072657\n",
      "2 1 0.08098982140440436\n",
      "2 2 0.039239666907809057\n",
      "2 3 0.06344229016957935\n",
      "2 4 0.11287754120743182\n",
      "3 0 0.05170683024388294\n",
      "3 1 0.06985569367055872\n",
      "3 2 0.03450155406723409\n",
      "3 3 0.0564983944688094\n",
      "3 4 0.0943289742703826\n",
      "4 0 0.04506679093403896\n",
      "4 1 0.06102358294047068\n",
      "4 2 0.030834656613693102\n",
      "4 3 0.051074186334999674\n",
      "4 4 0.07962979315252128\n",
      "5 0 0.03978698737112118\n",
      "5 1 0.05393492846486475\n",
      "5 2 0.027948554107857456\n",
      "5 3 0.046780730592824546\n",
      "5 4 0.06787943181197556\n",
      "6 0 0.03556345931402533\n",
      "6 1 0.04819114020861057\n",
      "6 2 0.025649581479096087\n",
      "6 3 0.04333641578624234\n",
      "6 4 0.05841544198501854\n",
      "7 0 0.03216288070232089\n",
      "7 1 0.04349613563800506\n",
      "7 2 0.023799685445366748\n",
      "7 3 0.040532982987414534\n",
      "7 4 0.05073977060398072\n",
      "8 0 0.02940506539456089\n",
      "8 1 0.03962588874662619\n",
      "8 2 0.022297072482358665\n",
      "8 3 0.03821534356412895\n",
      "8 4 0.044472628216734096\n",
      "9 0 0.027150960447273532\n",
      "9 1 0.03640909318071605\n",
      "9 2 0.021065278890909665\n",
      "9 3 0.03626773219958895\n",
      "9 4 0.03932161252982386\n",
      "10 0 0.025293323172366936\n",
      "10 1 0.03371374061629817\n",
      "10 2 0.020046199217952476\n",
      "10 3 0.03460365052034606\n",
      "10 4 0.035060014864373754\n",
      "11 0 0.023749381767177113\n",
      "11 1 0.0314374291915495\n",
      "11 2 0.019195314691689183\n",
      "11 3 0.0331584233196972\n",
      "11 4 0.03151107102773408\n",
      "12 0 0.022455119088729505\n",
      "12 1 0.0295002309267242\n",
      "12 2 0.018478294562333856\n",
      "12 3 0.031883656547657735\n",
      "12 4 0.02853628174384857\n",
      "13 0 0.021360870014912554\n",
      "13 1 0.027839384256975613\n",
      "13 2 0.0178685174597051\n",
      "13 3 0.030743104266230458\n",
      "13 4 0.026026631439806578\n",
      "14 0 0.020427955401577616\n",
      "14 1 0.026405314012188024\n",
      "14 2 0.017345236199748496\n",
      "14 3 0.02970958472124281\n",
      "14 4 0.02389592628784646\n",
      "15 0 0.019626118524973566\n",
      "15 1 0.02515862826062803\n",
      "15 2 0.016892204679464097\n",
      "15 3 0.02876267858993975\n",
      "15 4 0.022075708402863665\n",
      "16 0 0.01893157537229688\n",
      "16 1 0.024067839964998693\n",
      "16 2 0.016496642419805026\n",
      "16 3 0.027887010620274287\n",
      "16 4 0.020511355210394662\n",
      "17 0 0.018325531349276555\n",
      "17 1 0.02310762991875974\n",
      "17 2 0.01614844883214527\n",
      "17 3 0.027070966547160652\n",
      "17 4 0.019159076496920883\n",
      "18 0 0.017793051250660427\n",
      "18 1 0.02225751611007678\n",
      "18 2 0.015839603849822973\n",
      "18 3 0.02630573492329944\n",
      "18 4 0.017983594862003675\n",
      "19 0 0.017322196537454922\n",
      "19 1 0.021500829734555072\n",
      "19 2 0.015563708631680568\n",
      "19 3 0.02558459161675795\n",
      "19 4 0.01695634844498816\n",
      "20 0 0.01690336497896712\n",
      "20 1 0.020823923606758343\n",
      "20 2 0.015315632164393123\n",
      "20 3 0.024902365650884755\n",
      "20 4 0.01605409405032095\n",
      "21 0 0.01652878371170707\n",
      "21 1 0.02021555744702741\n",
      "21 2 0.015091238334088096\n",
      "21 3 0.024255040630367278\n",
      "21 4 0.01525781809752745\n",
      "22 0 0.016192118840081603\n",
      "22 1 0.019666418342209113\n",
      "22 2 0.014887174419387295\n",
      "22 3 0.023639457584101478\n",
      "22 4 0.014551884852107625\n",
      "23 0 0.015888173778824167\n",
      "23 1 0.019168744933466385\n",
      "23 2 0.014700706657636276\n",
      "23 3 0.023053093684947856\n",
      "23 4 0.013923368036665447\n",
      "24 0 0.015612655347373227\n",
      "24 1 0.01871603152580336\n",
      "24 2 0.014529592022324494\n",
      "24 3 0.022493897739268354\n",
      "24 4 0.013361524538830542\n",
      "25 0 0.01536199173677704\n",
      "25 1 0.018302794030735637\n",
      "25 2 0.014371977951825498\n",
      "25 3 0.021960168139781938\n",
      "25 4 0.01285737852398871\n",
      "26 0 0.015133190308106049\n",
      "26 1 0.01792438394617856\n",
      "26 2 0.014226323722371734\n",
      "26 3 0.021450462562106747\n",
      "26 4 0.012403391569080748\n",
      "27 0 0.014923726068994635\n",
      "27 1 0.017576839812257602\n",
      "27 2 0.01409133863086207\n",
      "27 3 0.020963531368495457\n",
      "27 4 0.011993200014008455\n",
      "28 0 0.014731453851699083\n",
      "28 1 0.017256768027548747\n",
      "28 2 0.013965933268745153\n",
      "28 3 0.020498268691741383\n",
      "28 4 0.01162140499663221\n",
      "29 0 0.014554538860704196\n",
      "29 1 0.01696124676595167\n",
      "29 2 0.01384918101695429\n",
      "29 3 0.020053676678814608\n",
      "29 4 0.011283403910363188\n",
      "30 0 0.014391401503569823\n",
      "30 1 0.016687748147213712\n",
      "30 2 0.013740287540099563\n",
      "30 3 0.01962883950443422\n",
      "30 4 0.010975254537518169\n",
      "31 0 0.014240673364587514\n",
      "31 1 0.016434074893496495\n",
      "31 2 0.013638566555054816\n",
      "31 3 0.01922290461400152\n",
      "31 4 0.010693565046806054\n",
      "32 0 0.01410116190094117\n",
      "32 1 0.016198308531905323\n",
      "32 2 0.013543420531348962\n",
      "32 3 0.018835069293571597\n",
      "32 4 0.010435404536014036\n",
      "33 0 0.013971821990764023\n",
      "33 1 0.015978766839605994\n",
      "33 2 0.013454325275795608\n",
      "33 3 0.018464571144442374\n",
      "33 4 0.010198229954845395\n",
      "34 0 0.013851732883202348\n",
      "34 1 0.015773968719800534\n",
      "34 2 0.013370817582173669\n",
      "34 3 0.018110681400843816\n",
      "34 4 0.009979826136851281\n",
      "35 0 0.013740079423448402\n",
      "35 1 0.015582605077852345\n",
      "35 2 0.013292485304054786\n",
      "35 3 0.017772700300593355\n",
      "35 4 0.009778256363656349\n",
      "36 0 0.01363613667410763\n",
      "36 1 0.015403514563218564\n",
      "36 2 0.013218959346847947\n",
      "36 3 0.017449953922562173\n",
      "36 4 0.009591821425143263\n",
      "37 0 0.013539257245885072\n",
      "37 1 0.015235663274221408\n",
      "37 2 0.013149907182767297\n",
      "37 3 0.01714179205798637\n",
      "37 4 0.009419025561069149\n",
      "38 0 0.013448860798777112\n",
      "38 1 0.015078127703995574\n",
      "38 2 0.013085027576577847\n",
      "38 3 0.016847586797557445\n",
      "38 4 0.009258547999659084\n",
      "39 0 0.01336442528987102\n",
      "39 1 0.01493008034854786\n",
      "39 2 0.01302404627588967\n",
      "39 3 0.016566731602253634\n",
      "39 4 0.009109219067716884\n",
      "40 0 0.013285479633192064\n",
      "40 1 0.014790777510440747\n",
      "40 2 0.012966712471496016\n",
      "40 3 0.01629864069012864\n",
      "40 4 0.008970000050598191\n",
      "41 0 0.013211597506668345\n",
      "41 1 0.014659548920815581\n",
      "41 2 0.01291279587390506\n",
      "41 3 0.016042748619116535\n",
      "41 4 0.008839966141239823\n",
      "42 0 0.013142392095696896\n",
      "42 1 0.014535788873418871\n",
      "42 2 0.012862084284213338\n",
      "42 3 0.015798509981389364\n",
      "42 4 0.00871829194477694\n",
      "43 0 0.013077511605426233\n",
      "43 1 0.014418948620932485\n",
      "43 2 0.012814381562688224\n",
      "43 3 0.015565399150978489\n",
      "43 4 0.008604239106406756\n",
      "44 0 0.013016635407363435\n",
      "44 1 0.014308529829289008\n",
      "44 2 0.01276950591832544\n",
      "44 3 0.015342910045554485\n",
      "44 4 0.008497145710733596\n",
      "45 0 0.012959470712297099\n",
      "45 1 0.014204078922147766\n",
      "45 2 0.01272728845836565\n",
      "45 3 0.015130555877204212\n",
      "45 4 0.008396417165247427\n",
      "46 0 0.01290574968237064\n",
      "46 1 0.014105182177162785\n",
      "46 2 0.012687571949181799\n",
      "46 3 0.014927868877069267\n",
      "46 4 0.008301518332266737\n",
      "47 0 0.012855226911654458\n",
      "47 1 0.014011461459534249\n",
      "47 2 0.012650209749784124\n",
      "47 3 0.014734399985817557\n",
      "47 4 0.008211966715281956\n",
      "48 0 0.012807677217689462\n",
      "48 1 0.013922570497732862\n",
      "48 2 0.012615064886979969\n",
      "48 3 0.014549718506863511\n",
      "48 4 0.00812732653925432\n",
      "49 0 0.012762893696938138\n",
      "49 1 0.013838191622112306\n",
      "49 2 0.012582009247401134\n",
      "49 3 0.014373411722601015\n",
      "49 4 0.008047203591688687\n",
      "50 0 0.012720686005449787\n",
      "50 1 0.0137580329000817\n",
      "50 2 0.012550922866511828\n",
      "50 3 0.014205084476094875\n",
      "50 4 0.007971240713493049\n",
      "51 0 0.012680878832765894\n",
      "51 1 0.013681825612155639\n",
      "51 2 0.012521693298601063\n",
      "51 3 0.014044358722012986\n",
      "51 4 0.007899113846772072\n",
      "52 0 0.012643310542506423\n",
      "52 1 0.013609322021975154\n",
      "52 2 0.012494215054856489\n",
      "52 3 0.013890873051311915\n",
      "52 4 0.007830528561576577\n",
      "53 0 0.01260783195745744\n",
      "53 1 0.013540293400652936\n",
      "53 2 0.01246838909907766\n",
      "53 3 0.013744282194493751\n",
      "53 4 0.007765216995877315\n",
      "54 0 0.012574305270538787\n",
      "54 1 0.013474528271821481\n",
      "54 2 0.012444122392546638\n",
      "54 3 0.013604256508263652\n",
      "54 4 0.007702935153152835\n",
      "55 0 0.012542603065932427\n",
      "55 1 0.013411830848780473\n",
      "55 2 0.01242132748113761\n",
      "55 3 0.013470481450233763\n",
      "55 4 0.007643460510377752\n",
      "56 0 0.01251260743703118\n",
      "56 1 0.013352019639331778\n",
      "56 2 0.012399922118995878\n",
      "56 3 0.013342657046011619\n",
      "56 4 0.007586589896189067\n",
      "57 0 0.01248420918982567\n",
      "57 1 0.013294926197403426\n",
      "57 2 0.012379828924115616\n",
      "57 3 0.013220497352631633\n",
      "57 4 0.007532137604851323\n",
      "58 0 0.012457307121967824\n",
      "58 1 0.013240394003518015\n",
      "58 2 0.012360975061946897\n",
      "58 3 0.013103729921872849\n",
      "58 4 0.007479933716542228\n",
      "59 0 0.012431807369096588\n",
      "59 1 0.013188277458651903\n",
      "59 2 0.012343291953806106\n",
      "59 3 0.01299209526658182\n",
      "59 4 0.007429822598605253\n",
      "60 0 0.012407622811137642\n",
      "60 1 0.013138440978138542\n",
      "60 2 0.012326715007382265\n",
      "60 3 0.012885346332702744\n",
      "60 4 0.007381661565900066\n",
      "61 0 0.012384672532234933\n",
      "61 1 0.01309075817405744\n",
      "61 2 0.012311183367050978\n",
      "61 3 0.012783247979319774\n",
      "61 4 0.007335319681333637\n",
      "62 0 0.012362881328770734\n",
      "62 1 0.013045111116070856\n",
      "62 2 0.012296639682047401\n",
      "62 3 0.012685576468646315\n",
      "62 4 0.007290676680165344\n",
      "63 0 0.012342179260609066\n",
      "63 1 0.013001389661968682\n",
      "63 2 0.01228302989082652\n",
      "63 3 0.012592118967556127\n",
      "63 4 0.007247622003819502\n",
      "64 0 0.012322501241275976\n",
      "64 1 0.012959490850292325\n",
      "64 2 0.012270303020165282\n",
      "64 3 0.012502673061943678\n",
      "64 4 0.007206053930770211\n",
      "65 0 0.012303786663286097\n",
      "65 1 0.012919318348361391\n",
      "65 2 0.012258410997747389\n",
      "65 3 0.01241704628492623\n",
      "65 4 0.007165878793633597\n",
      "66 0 0.012285979055252617\n",
      "66 1 0.012880781949846645\n",
      "66 2 0.012247308477125341\n",
      "66 3 0.012335055659656632\n",
      "66 4 0.00712701027295412\n",
      "67 0 0.012269025767787635\n",
      "67 1 0.01284379711673988\n",
      "67 2 0.012236952674082592\n",
      "67 3 0.012256527257302207\n",
      "67 4 0.007089368759336677\n",
      "68 0 0.012252877685521664\n",
      "68 1 0.012808284561182376\n",
      "68 2 0.0122273032135259\n",
      "68 3 0.012181295770559293\n",
      "68 4 0.0070528807765841035\n",
      "69 0 0.012237488962852188\n",
      "69 1 0.012774169863143859\n",
      "69 2 0.012218321986128456\n",
      "69 3 0.012109204102912925\n",
      "69 4 0.0070174784593730576\n",
      "70 0 0.012222816781277273\n",
      "70 1 0.01274138312040411\n",
      "70 2 0.012209973014021547\n",
      "70 3 0.012040102973714115\n",
      "70 4 0.006983099079760169\n",
      "71 0 0.012208821126387265\n",
      "71 1 0.0127098586276904\n",
      "71 2 0.012202222324898275\n",
      "71 3 0.011973850539031132\n",
      "71 4 0.006949684617471236\n",
      "72 0 0.012195464582778844\n",
      "72 1 0.012679534582174063\n",
      "72 2 0.012195037833949977\n",
      "72 3 0.011910312028133987\n",
      "72 4 0.006917181369502678\n",
      "73 0 0.012182712145325538\n",
      "73 1 0.012650352812835653\n",
      "73 2 0.012188389233105503\n",
      "73 3 0.01184935939539028\n",
      "73 4 0.006885539595068895\n",
      "74 0 0.01217053104538931\n",
      "74 1 0.012622258531476645\n",
      "74 2 0.012182247887087017\n",
      "74 3 0.011790870987284667\n",
      "74 4 0.0068547131923706065\n",
      "75 0 0.012158890590692289\n",
      "75 1 0.012595200103391707\n",
      "75 2 0.012176586735834428\n",
      "75 3 0.011734731224220403\n",
      "75 4 0.006824659404047208\n",
      "76 0 0.01214776201768758\n",
      "76 1 0.012569128835922976\n",
      "76 2 0.012171380202884776\n",
      "76 3 0.011680830296719506\n",
      "76 4 0.0067953385485168185\n",
      "77 0 0.012137118355375646\n",
      "77 1 0.012543998783301263\n",
      "77 2 0.012166604109323546\n",
      "77 3 0.011629063875605089\n",
      "77 4 0.00676671377470822\n",
      "78 0 0.012126934299608854\n",
      "78 1 0.012519766566340946\n",
      "78 2 0.012162235592952413\n",
      "78 3 0.011579332835725195\n",
      "78 4 0.0067387508379535615\n",
      "79 0 0.01211718609701381\n",
      "79 1 0.012496391205698747\n",
      "79 2 0.012158253032343086\n",
      "79 3 0.011531542992760258\n",
      "79 4 0.00671141789504496\n",
      "80 0 0.012107851437738668\n",
      "80 1 0.012473833967533815\n",
      "80 2 0.012154635975469598\n",
      "80 3 0.011485604852644932\n",
      "80 4 0.006684685316665242\n",
      "81 0 0.012098909356303388\n",
      "81 1 0.012452058220519778\n",
      "81 2 0.012151365072632344\n",
      "81 3 0.011441433373129296\n",
      "81 4 0.006658525515586704\n",
      "82 0 0.01209034013989413\n",
      "82 1 0.012431029303259868\n",
      "82 2 0.012148422013406124\n",
      "82 3 0.011398947737002311\n",
      "82 4 0.006632912789194869\n",
      "83 0 0.01208212524350071\n",
      "83 1 0.01241071440124638\n",
      "83 2 0.012145789467362387\n",
      "83 3 0.011358071136502714\n",
      "83 4 0.006607823175038899\n",
      "84 0 0.012074247211347906\n",
      "84 1 0.012391082432585701\n",
      "84 2 0.012143451028331809\n",
      "84 3 0.011318730568446915\n",
      "84 4 0.006583234318239615\n",
      "85 0 0.01206668960411873\n",
      "85 1 0.012372103941782014\n",
      "85 2 0.012141391161988846\n",
      "85 3 0.011280856639611208\n",
      "85 4 0.006559125349700649\n",
      "86 0 0.012059436931510547\n",
      "86 1 0.012353751000937281\n",
      "86 2 0.012139595156553482\n",
      "86 3 0.01124438338191451\n",
      "86 4 0.00653547677417116\n",
      "87 0 0.012052474589703837\n",
      "87 1 0.01233599711778269\n",
      "87 2 0.012138049076418793\n",
      "87 3 0.011209248076958899\n",
      "87 4 0.006512270367299995\n",
      "88 0 0.01204578880335873\n",
      "88 1 0.012318817150008704\n",
      "88 2 0.012136739718524893\n",
      "88 3 0.011175391089497184\n",
      "88 4 0.006489489080903248\n",
      "89 0 0.012039366571786758\n",
      "89 1 0.012302187225407865\n",
      "89 2 0.01213565457131115\n",
      "89 3 0.011142755709409742\n",
      "89 4 0.006467116955740685\n",
      "90 0 0.012033195618974368\n",
      "90 1 0.012286084667386116\n",
      "90 2 0.012134781776089234\n",
      "90 3 0.011111288001786528\n",
      "90 4 0.006445139041162236\n",
      "91 0 0.012027264347161655\n",
      "91 1 0.01227048792543711\n",
      "91 2 0.01213411009068932\n",
      "91 3 0.011080936664724308\n",
      "91 4 0.006423541321044989\n",
      "92 0 0.012021561793704026\n",
      "92 1 0.01225537651020775\n",
      "92 2 0.012133628855241019\n",
      "92 3 0.011051652894463533\n",
      "92 4 0.006402310645494313\n",
      "93 0 0.012016077590966769\n",
      "93 1 0.012240730932814858\n",
      "93 2 0.01213332795995926\n",
      "93 3 0.011023390257503853\n",
      "93 4 0.00638143466783041\n",
      "94 0 0.012010801929022723\n",
      "94 1 0.012226532648100657\n",
      "94 2 0.012133197814813508\n",
      "94 3 0.010996104569351718\n",
      "94 4 0.006360901786424868\n",
      "95 0 0.012005725520942\n",
      "95 1 0.01221276400154066\n",
      "95 2 0.012133229320966017\n",
      "95 3 0.010969753779568164\n",
      "95 4 0.006340701090990474\n",
      "96 0 0.012000839570479357\n",
      "96 1 0.01219940817954061\n",
      "96 2 0.012133413843872192\n",
      "96 3 0.010944297862798855\n",
      "96 4 0.00632082231296271\n",
      "97 0 0.011996135741980617\n",
      "97 1 0.012186449162880465\n",
      "97 2 0.012133743187942603\n",
      "97 3 0.01091969871548273\n",
      "97 4 0.006301255779643104\n",
      "98 0 0.011991606132343601\n",
      "98 1 0.012173871683082385\n",
      "98 2 0.012134209572672335\n",
      "98 3 0.010895920057949085\n",
      "98 4 0.0062819923718030165\n",
      "99 0 0.011987243244881951\n",
      "99 1 0.01216166118149763\n",
      "99 2 0.012134805610149488\n",
      "99 3 0.010872927341626412\n",
      "99 4 0.0062630234844727375\n"
     ]
    }
   ],
   "source": [
    "net=Network_self(13,0.005)\n",
    "net.train(100,train_data,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 小结\n",
    "\n",
    "本节我们详细介绍了如何使用NumPy实现梯度下降算法，构建并训练了一个简单的线性模型实现波士顿房价预测，可以总结出，使用神经网络建模房价预测有三个要点：\n",
    "\n",
    "* 构建网络，初始化参数$w$和$b$，定义预测和损失函数的计算方法。\n",
    "* 随机选择初始点，建立梯度的计算方法和参数更新方式。\n",
    "* 从总的数据集中抽取部分数据作为一个mini_batch，计算梯度并更新参数，不断迭代直到损失函数几乎不再下降。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 作业1-2\n",
    "\n",
    "1. 样本归一化：预测时的样本数据同样也需要归一化，但使用训练样本的均值和极值计算，这是为什么？\n",
    "\n",
    "2. 当部分参数的梯度计算为0（接近0）时，可能是什么情况？是否意味着完成训练？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 作业 1-3\n",
    "\n",
    "1. 随机梯度下降的batchsize设置成多少合适？过小有什么问题？过大有什么问题？提示：过大以整个样本集合为例，过小以单个样本为例来思考。\n",
    "1. 一次训练使用的配置：5个epoch，1000个样本，batchsize=20，最内层循环执行多少轮？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 作业1-4\n",
    "\n",
    "#### 基本知识\n",
    "\n",
    "**1. 求导的链式法则**\n",
    "\n",
    "链式法则是微积分中的求导法则，用于求一个复合函数的导数，是在微积分的求导运算中一种常用的方法。复合函数的导数将是构成复合这有限个函数在相应点的导数的乘积，就像锁链一样一环套一环，故称链式法则。如 **图9** 所示，如果求最终输出对内层输入（第一层）的梯度，等于外层梯度（第二层）乘以本层函数的梯度。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/2beffa3f3d7c402685671b0825561a91c17216fe8b924f64b9f29a96f45cbc85\" width=\"200\" hegiht=\"\" ></center>\n",
    "<center><br>图9：求导的链式法则</br></center>\n",
    "<br></br>\n",
    "\n",
    "**2. 计算图的概念**\n",
    "\n",
    "（1）为何是反向计算梯度？即梯度是由网络后端向前端计算。当前层的梯度要依据处于网络中后一层的梯度来计算，所以只有先算后一层的梯度才能计算本层的梯度。     \n",
    "\n",
    "（2）案例：购买苹果产生消费的计算图。假设一家商店9折促销苹果，每个的单价100元。计算一个顾客总消费的结构如 **图10** 所示。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/46c43ead4fa942f5be87f25538a046ff9456516816274cbcb5f6df3768c0fd34\" width=\"400\" hegiht=\"40\" ></center>\n",
    "<center><br>图10：购买苹果所产生的消费计算图</br></center>\n",
    "<br></br>\n",
    "\n",
    "*  前向计算过程：以黑色箭头表示，顾客购买了2个苹果，再加上九折的折扣，一共消费100\\*2\\*0.9=180元。\n",
    "*  后向传播过程：以红色箭头表示，根据链式法则，本层的梯度计算 * 后一层传递过来的梯度，所以需从后向前计算。\n",
    " \n",
    "最后一层的输出对自身的求导为1。导数第二层根据 **图11** 所示的乘法求导的公式，分别为0.9\\*1和200\\*1。同样的，第三层为100 * 0.9=90，2 * 0.9=1.8。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/c251a2c290e946f99ce3a3381396c392b50e5a4243c346509bd91177b7f2da90\" width=\"200\"  ></center>\n",
    "<center><br>图11：乘法求导的公式</br></center>\n",
    "<br></br>\n",
    "\n",
    "#### 作业题\n",
    "\n",
    "1. 根据 **图12** 所示的乘法和加法的导数公式，完成 **图13** 购买苹果和橘子的梯度传播的题目。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/4ce8715f03f9477699707056544b1e6363f78aa09fda411d972878abb6d1d26f\" width=\"300\"  ></center>\n",
    "<center><br>图12：乘法和加法的导数公式</br></center>\n",
    "<br></br>\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/2fc6665e10f34f9e863172bb399862319f0914467d72457d9e7328616bdbe6df\" width=\"500\"  ></center>\n",
    "<center><br>图13：购买苹果和橘子产生消费的计算图</br></center>\n",
    "<br></br>  \n",
    "\n",
    "2. 挑战题：用代码实现两层的神经网络的梯度传播，中间层的尺寸为13【房价预测案例】（教案当前的版本为一层的神经网络），如 **图14** 所示。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/580f2553aa4643809006f5a8d3deb2aa8dd4e1aa69d94cf6a35ead5fe7cf469e\" width=\"300\"  ></center>\n",
    "<center><br>图14：两层的神经网络</br></center>\n",
    "<br></br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}